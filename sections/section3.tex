\section{为什么是神经网络}
% 从函数拟合的角度引入神经网络
\subsection{神经网络：一个大的函数}

相比于\textoverset{Neural Network}{神经网络}如何实现其功能，读者或许更想问的是：为什么要用神经网络？现有的神经网络为什么用了这些方法？对于这一类问题，一个现实的回答是：机器学习是高度以实用为导向的，实验显示这样做效果更好。在现实中，我们往往要解决各种各样的问题，人类开发者以手写每一行代码创造了各种各样的程序，自动化地解决了许多问题。但很多问题难以在有限的时间内找到确定性的解决方案，例如识别图片中的物体、识别语音、自然语言处理等等。它们有一个共同点：输入的信息量巨大、关系复杂，难以用确定的规则来描述。手动规定像素范围来判断物体类型，或用固定的规则来解析自然语言显然并不现实。因此人们自然要问有没有更加自动化、灵活、智能的方法来一劳永逸地解决这些问题。人工智能的概念就此提出，人们希望让机器自己学习知识来解决问题。

虽然目前人类仍然很难说摸到了\textoverset{Artificial General Intelligence}{通用人工智能}\footnote{通用人工智能：指能像人类一样解决各种通用的问题的人工智能。}的边界，但人工智能已然在许多问题上取得了巨大成就，走出了 20 世纪末 21 世纪初被大众认为是“伪科学”的寒冬。经过\uhref{https://arxiv.org/pdf/1512.03385}{深度残差网络}在图像识别的重大突破、\uhref{https://www.davidsilver.uk/wp-content/uploads/2020/03/unformatted_final_mastering_go.pdf}{AlphaGo}学会下围棋、\uhref{https://arxiv.org/pdf/1706.03762}{Transformer}在翻译比赛取得优异成绩并引来一波生成式模型的热潮等等，人工智能就这样走向了时代的焦点。但是如果要问：为什么它这么成功？最直接的回答仍是：It works.

除了一些基础的训练方法外，其它的结构构成、参数调整等等往往都是人们有一个想法，于是就这样展开了实验。部分实验成功了，就说明这个想法是对的，从而延伸出新的调节思路。如此循环往复，形成了现在的人工智能领域。因此就模型结构而言并没有非常完备的理论，有的只能说是经验法则。

不过我想可以对解决的方法做一个简单的分类。按照参数的数量，从参数复杂到参数简单可以画出一条轴。按照模型获取经验的方式，从模型完全编码了先验经验，到通过一些例子得到经验，再到持续在与环境的互动中获取经验，可以画出另一条轴。在这里我也试图并不严谨地画出了这样一个表格。
\begin{table}[H]
\centering
\begin{tabular}{c|cccc}
\toprule
\textbf{监督方式 $\backslash$ 参数量} & \textbf{超大参数量} & \textbf{大参数量} & \textbf{小参数量} & \textbf{经典模型} \\
\midrule
\textbf{持续互动} & PPO, A3C & DQN & Q-Learning & 经典控制\\
\textbf{输入/输出对} & ResNet, Transformer & 浅层CNN & 浅层MLP & SVM \\
\textbf{无监督} & GAN, SimCLR & -------- & K-Means, KNN & PCA, t-SNE \\
\bottomrule
\end{tabular}
\end{table}
读者看到的第一反应大抵是感到看不懂。不过我也并非想让读者先学完再来看这个表格，而是希望读者看到：解决问题的方法虽然多样，但仍可根据若干指标大致分类。表中的术语有的是模型结构，有的是算法，有的是思想，而右侧的一列甚至根本就不是机器学习，对机器学习有基本了解的读者或许会认为它们可比性存疑。诚然，模型之间并没有一个实际上的绝对界限，表中划分的位置也仅是凭借我的经验评价一个模型大多数时候处于什么位置，而非绝对的准则，但我认为这样的划分是有意义的，用一种更为建设性的话来说：意义就是在混乱的世界中建构起规律，用于解决问题。

大参数量的一侧——神经网络的领域，正是本书的主题。作为神经网络的引入，有必要从更高的角度来理解以神经网络为基础的模型目标是什么。小节标题已经足以表达内容核心：先不论内部结构如何，所谓的神经网络，无非也是一个函数。所谓函数，就必然要考虑到输入和输出，或者更准确地说，我们关心的就是怎么用计算机程序对给定的输入，得到我们想要的输出。无论是连续的数据，还是按照 0 或者 1 编码为向量的标签，输入和输出都可以变为向量。因此许多问题都可以归结为一个更加狭义的、数值拟合意义上的函数拟合问题。一个\textoverset{Encoder}{编码器}将原始输入变为向量这种易于处理的形式。而对于函数的原始输出，可以通过一个\textoverset{Decoder}{解码器}将数值构成的向量变为我们想要的输出。

而再向前看，在第一章中我们已经初步了解了以线性回归为代表的一类函数拟合问题。虽然这一问题从结构上相对简单，但是从这一情境中可以抽象出函数拟合的理念：有一些输入和输出的对应关系，我们要设计一个带参数的拟合模型，调整参数，让模型的输出尽可能接近我们预期的输出，接近程度则通过一个损失函数来衡量。

因此我会把模型抽象成五个要素：\textoverset{Input}{输入}、\textoverset{Output}{输出}、\textoverset{Architecture}{模型结构}、\textoverset{Loss Function}{损失函数}和\textoverset{Optimizer}{优化算法}。输入、模型架构和具体参数决定了输出如何计算，按照损失函数计算得到的损失指导模型调整具体参数，优化算法则决定了参数如何调整。当然这样的划分只是我自己的理解，而非理解神经网络的唯一方式。这里我不打算在概念之间玩文字游戏，把机器学习中的概念倒来倒去，变成一篇又臭又长，令人看完莫名其妙、不知所云、又对实践毫无益处的文章。因此我认为画一个图串起来是最直观的方式。
\begin{figure}[H]
\centering
\begin{tikzpicture}[>=Stealth]
    \fill [cyan, opacity=0.5] (0, 0) rectangle (1, 1);
    \node at (0.5, 0.5) {$x'$};
    \node [below] at (0.5, 0) {向量输入};
    \fill [green, opacity=0.5] (4, 0) rectangle (5, 1);
    \node at (4.5, 0.5) {$y'$};
    \node [below] at (4.5, 0) {向量输出};
    \draw [->] (1.2, 0.5) -- node [above] {模型} (3.8, 0.5);
    \fill [lightgray, opacity=0.5] (8, 0) rectangle (9, 1);
    \node at (8.5, 0.5) {$l$};
    \node [below] at (8.5, 0) {损失};
    \draw [->] (5.2, 0.5) -- node [above] {损失函数} (7.8, 0.5);
    \fill [yellow, opacity=0.5] (4, 2) rectangle (5, 3);
    \node at (4.5, 2.5) {$o$};
    \node [below] at (4.5, 2) {优化器};
    \draw [->] (8.5, 1.2) -- (8.5, 2.5) -- node [above] {优化信息} (5.2, 2.5);
    \draw [->] (3.8, 2.5) -- (2.5, 2.5) -- node [left] {参数更新} (2.5, 1.2);
    \draw [dashed] (-0.6, -0.8) rectangle (9.6, 3.4);
    \fill [cyan, opacity=0.5] (-4, 0) rectangle (-3, 1);
    \node at (-3.5, 0.5) {$x$};
    \node [below] at (-3.5, 0) {输入};
    \draw [->] (-2.8, 0.5) -- node [above] {编码器} (-0.2, 0.5);
    \fill [green, opacity=0.5] (4, -3) rectangle (5, -2);
    \node at (4.5, -2.5) {$y$};
    \node [below] at (4.5, -3) {输出};
    \draw [->] (4.5, -0.6) -- node [right] {解码器} (4.5, -1.8);
\end{tikzpicture}
\end{figure}

从输入到输出再到损失的过程通常称为\textoverset{Forward Propagation}{正向传播}，而从损失到参数的更新过程则称为\textoverset{Backward Propagation}{反向传播}。而这中间的模型结构常常由矩阵运算与一些\textoverset{Activation Function}{激活函数}构成的层组成。几乎可以说众多的神经网络中，只有这种传播的方式和网络的基本组成元素是相同的，如何从这些基本元素构建出好的模型则像是搭积木一样，各有各的搭法。

在这里我想简单讲讲使用矩阵运算的原因。在第一章中我们已经简单地学习了矩阵运算的基本知识，它本质上是正比例函数在向量空间中的推广，只是 $y=kx$ 中的斜率变成了一个个从输入 $x_j$ 连接到输出 $y_i$ 的权重 $w_{ij}$。从行看过去，它反映了输出的每个分量（或称为特征）是如何由输入的每个分量线性组合而成的。而从列看过去，它表明了输入的每个分量是如何影响输出的。就像一次函数有一个常数项一样，矩阵运算也有一个偏置项 $b$，运算的总体结构是 $y = wx + b$。从代数上看，它运算简单\footnote{简单：仅由简单的四则运算组成，现代 GPU 也常常提供高效的矩阵运算加速。}，而从分析上看，它的输出变化光滑，容易求导\footnote{容易求导：记住这一点，这对后续反向传播等算法的实现至关重要。如果在离散的空间中操作，例如使用阶跃函数或者逻辑门，便无法借助导数来进行参数更新。}。

下一节中我们会引入激活函数，暂且不论它们的具体形式如何，它们也是一些非常简单的运算。或许读者会有疑问，这样一些简单的运算，真的有能力让神经网络胜任复杂的任务吗？\uhref{https://www.zhihu.com/question/594296903/answer/2979485641}{万能逼近定理}\footnote{万能逼近定理：指出足够大的神经网络可以以任意精度在给定范围内拟合任意的复杂函数。}虽然在理论上告诉我们它可以，却需要假定足够多的神经元，并不令人安心。所幸无数的实验表明：可以，在人类能实现的范围内，量变也可以引起质变，将一系列简单的单元堆叠起来，便可以形成复杂的行为。Philip W. Anderson\footnote{Philip W. Anderson：美国物理学家，1977 年诺贝尔物理学奖获得者。}曾说过：“More is different”，他的原意指的是物理学中，微观的规律并不能简单地推导出宏观的规律，整个系统可以表现的与单个元素完全不同，因此微观和宏观需要不同的理论来描述。但这里我们不妨借用一下，同样地认识到大量简单的数学函数也可以产生复杂的行为。在神经网络的\textoverset{Emergent}{涌现}\footnote{涌现：即增大参数量带来性能突然提升的现象。}现象中，这一事实不断地被验证。就像婴儿可以通过教育称为适应社会的成年人一样，适当的算法和足够的训练数据的确可以让神经网络学会知识。

需要说明的是，现代的机器学习库 PyTorch 与 TensorFlow 都提供了完善的参数更新机制，使得用户不必自己实现优化算法。这可以说是非常简单易用，让用户可以聚焦模型的设计。不过我仍然会解读其中的原理，并试图说明设计网络结构与优化算法的人为什么要这么做。\footnote{其中的原理：实际上人类理解的神经网络工作原理与计算机实际运行的原理或许有很大的区别，人类对现在大部分网络的理解本质上都是经过实验后进行的归纳甚至是猜测，而非从数学上严格证明。神经网络的\textoverset{Interpretability}{可解释性}仍然是很大的问题，因此很多时候人们只知道怎么样做效果好，而不“真正地”理解为什么这么做效果好，有时也被人调侃为“新时代的炼金术”。虽然有许多相关的\uhref{https://www.zhihu.com/question/320688440}{解释}来帮助人们了解神经网络中发生什么，但学界内仍然没有形成一套系统的理论。}

\newpage

\subsection{激活函数与非线性}

将 $y=wx+b$ 作为一次函数的类比应该足以说明它是很简单的一类函数。但是正如一次函数的复合 $ y = w_2(w_1x+b_1)+b_2 = w_2w_1x + (w_2b_1+b_2) $
仍然是一次函数一样，如果仅仅沉浸在矩阵运算中，我们便永远无法表达那些复杂的函数。举个最简单的例子，我们甚至无法表示输入的绝对值 $y=|x|$。因此我们需要在模型的结构中加点“非线性”，让它不仅仅局限于简单的加减乘除，专业的说法称之为\textoverset{Activation Function}{激活函数}。激活函数直接作用在每个特征上，而且函数本身通常是固定的\footnote{通常是固定的：在一些模型，例如使用可变样条函数的\uhref{https://arxiv.org/pdf/2404.19756}{KAN}中，激活函数也是可学习的，而且各个元素上的效果可能不同，但是可变的激活函数总体来说并不常见。}，且总体通常呈现递增的趋势。

所谓逐元素作用，也就是说，与矩阵对特征进行组合不同，激活函数对各个分量的操作是独立的。其输入是一个向量，输出也是一个同样维数的向量。如果选定了激活函数 $f:\mathbb{R}\to\mathbb{R}$，输入为 $x = [x_1, x_2, \cdots, x_n]$，则输出为 $y = [f(x_1), f(x_2), \cdots, f(x_n)]$。

现在使用最多的激活函数是\textoverset{Rectified Linear Unit}{线性整流函数}(ReLU)，虽然相对于其它激活函数，诸如 Sigmoid、tanh 等等，ReLU 其实算是晚辈，但是在关于激活函数的讨论中，\uhref{https://proceedings.mlr.press/v15/glorot1a/glorot1a.pdf}{有研究}表明它的效果更好，而后\uhref{https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}{AlexNet} 的成功更让它成为了主流的激活函数。虽然失去了早期其它激活函数的仿生背景，但它好用，而且非常简单。它的定义是：
\[
    \text{ReLU}(x) = \max\{0, x\} = \begin{cases}
        x, & x \geq 0 \\
        0, & x < 0 
    \end{cases}
\]

图像是这样的：
\begin{figure}[H]
\centering
\begin{tikzpicture}[>=Stealth]
    \draw [->] (-2, 0) -- (2, 0) node [below] {$x$};
    \draw [->] (0, -1) -- (0, 2) node [left] {$y$};
    \draw [domain=-2:0, smooth, variable=\x, blue, thick] plot ({\x}, 0);
    \draw [domain=0:2, smooth, variable=\x, blue, thick] plot ({\x}, {\x});
    \end{tikzpicture}
\caption{ReLU 函数图像}
\end{figure}

举一个例子就可以看出逐元素作用的含义。例如有输入向量 $x = [1, -2, 3]$，那么它经过 ReLU 激活函数的输出为 $y = [1, 0, 3]$。正的部分被保留了，而负的部分被置为 0。正如电路中的半波\textoverset{Rectifier}{整流器}一样，把负值截断了。

而它的导数也非常简单：
\[
    \frac{\mathrm{d}}{\mathrm{d} x}\text{ReLU}(x) = \begin{cases}
        1, & x > 0 \\
        0, & x < 0 
    \end{cases}
\]

读者或许会关心，那 0 这一点不可导要怎么办？其实关系不大，因为一个小数几乎不可能\footnote{几乎不可能：在最常用的 32 位浮点数中，一个数恰好取到 0 的概率大概在 $10^{-9}$ 量级。虽然在 FP8 或者 FP16 量化中恰好取到 0 的概率更大，然而实践中这单个不可导点几乎不会对训练产生影响。}在训练中恰好落在 0 上。即使有，也可以任意地选择一个值，例如 0 或者 1\footnote{0 处的导数：PyTorch 通常选择 0}。有了这样的激活函数，函数的表达能力大大就增强了。以目标 $|x|$ 为例，假设有输入 $x$，只需两个 ReLU 函数值的和就可以表示它：
\[
    |x| = \text{ReLU}(x) + \text{ReLU}(-x) = \max\{0, x\} + \max\{0, -x\}
\]

初看可能会觉得这样的表达方式有点多此一举，像是为了 $|x|$ 这盘醋专门包的饺子。但是别急，让我们把它拆解成神经网络的结构，更加结构化地看待。

最初的输入是 $x$，它先经过一个线性的函数得到 $[x, -x]$，再经过 ReLU 函数得到中间的向量 $x^{(1)} = (\max\{0, x\}, \max\{0, -x\})$，而这使用一个线性函数就可以得到 $y = |x|$。

写成矩阵的形式就有
\[
w_1 = \begin{bmatrix}
    1 \\ -1
\end{bmatrix}, b_1 = \begin{bmatrix}
    0 \\ 0
\end{bmatrix}, w_2 = \begin{bmatrix}
    1 & 1
\end{bmatrix}, b_2 = 0
\]

遂可以写成 $y = w_2 \,\text{ReLU}(w_1x + b_1) + b_2$。我认为，把这件事作为一个 toy case\footnote{toy case：玩具案例，指的是一个简单的例子，用于说明某个概念或方法。}想明白多少可以帮助理解神经网络。把矩阵的每个权重都画出来就是这样了：
\begin{figure}[H]
\centering
\begin{tikzpicture}[>=Stealth, scale=1.5]
    \node [circle, fill=white, draw, minimum size=1.2cm] (x0) at (0, 0) {$x$};
    \node [circle, fill=white, draw, minimum size=1.2cm] (x11) at (2, 1) {$x_1^{(1)}$};
    \node [circle, fill=white, draw, minimum size=1.2cm] (x12) at (2, -1) {$x_2^{(1)}$};
    \node [circle, fill=white, draw, minimum size=1.2cm] (y) at (4, 0) {$|x|$};
    \draw [->, red] (x0) -- node [above left] {$1$} (x11);
    \draw [->, blue] (x0) -- node [below left] {$-1$} (x12);
    \draw [->, red] (x11) -- node [above right] {$1$} (y);
    \draw [->, red] (x12) -- node [below right] {$1$} (y);
    \draw [dashed, thin] (1.4, -1.8) rectangle (2.6, 1.8);
    \node [above] at (2, 1.8) {中间层};
    \node at (2, 0.4) {ReLU};
    \node[lightgray] at (2, 1.6) {$+0$};
    \node[lightgray] at (2, -0.4) {$+0$};
    \node[lightgray] at (4, 0.6) {$+0$};
    \node at (2, -1.6) {ReLU};
    \end{tikzpicture}
\caption{神经网络表示 $|x|$}
\end{figure}

这看起来很简单，读者可能想问：还能不能再给力一点，看看更复杂的情况呢？当然可以。不过在看之前先抛出两个思考题：
\begin{enumerate}
    \item 试着用线性函数和 ReLU 函数表示 $y = \max\{x_1, x_2\}$，并画出它的神经网络结构图。
    \item 线性函数和 ReLU 的组合\textbf{不能}表示什么函数呢？
\end{enumerate}

在思考这个问题时，读者可以先回顾 ReLU 的性质：它的作用是将负数截断为 0，而正数保持不变。那么，能否通过适当的线性变换和 ReLU 来分辨两个数的大小呢？实际上我们可以很容易地发现
\[
    \max\{x_1, x_2\} = x_1 + \text{ReLU}(x_2 - x_1) 
\]

但是这个答案并不够好，如果直接把它画成神经网络结构图，就会发现它的结构看起来像是这样：
\begin{figure}[H]
\centering
\begin{tikzpicture}[>=Stealth, scale=1.5]
    \node [circle, fill=white, draw, minimum size=1.2cm] (x01) at (0, 1) {$x_1$};
    \node [circle, fill=white, draw, minimum size=1.2cm] (x02) at (0, -1) {$x_2$};
    \node [circle, fill=white, draw, minimum size=1.2cm] (x11) at (2, -1) {$?$};
    \node [circle, fill=white, draw, minimum size=1.2cm] (y) at (4, 0) {$y$};
    \draw [->, blue] (x01) -- node [above right] {$-1$} (x11);
    \draw [->, red] (x02) -- node [below] {$1$} (x11);
    \draw [->, red] (x11) -- node [below right] {$1$} (y);
    \draw [->, red] (x01) -- node [above right] {$1$} (y);
    \node at (2, -1.6) {ReLU};
    \node[lightgray] at (2, -0.4) {$+0$};
    \node[lightgray] at (4, 0.6) {$+0$};
    \end{tikzpicture}
\caption{神经网络表示 $\max\{x_1, x_2\}$ 的一种方法}
\end{figure}

变量 $x_1$ 没有经过统一的隐藏层，而是跳过中间，直接连接到了输出层。显然就不能用一致的 $\text{ReLU}(wx+b)$ 的形式来表示了，而是要单独开一个通道来处理。而我们使用神经网络的目的本来就是用一致的方式来处理所有的输入，所以这样的表示方式并不优雅\footnote{并不优雅：与之对比，在深层神经网络网络中通常会引入看起来有些像这里的\textoverset{Shortcut Connection}{跳连接}结构，由此引出\textoverset{Residual Network}{残差网络}的概念。它看起来有些像这里的跳过中间层的结构，但那里是系统性地引入这样的连接，而不是这样对某个分量单独处理。}。

不过使用一点小小的技巧，可以把 $x_1$ 本身写成 $x_1 = \text{ReLU}(x_1) - \text{ReLU}(-x_1)$，这样一来就可以把它写成带有三个中间变量的一个网络结构了。把
\[
    \max\{x_1, x_2\} = \text{ReLU}(x_1) - \text{ReLU}(-x_1) + \text{ReLU}(x_2 - x_1)
\]

这一式子中的三个分量提出来，便可以得到
\[
\begin{aligned}
    x_1^{(1)} &= \text{ReLU}(x_1\,{\color{lightgray}+\,0x_2}) \\
    x_2^{(1)} &= \text{ReLU}(-x_1\,{\color{lightgray}+\,0x_2}) \\
    x_3^{(1)} &= \text{ReLU}(- x_1 + x_2) \\
    y &= x_1^{(1)} - x_2^{(1)} + x_3^{(1)} 
\end{aligned}
\]

偏置 $b$ 仍然为 $0$，读者可以自行试着写出对应的权重矩阵 $w$ ，按照新的写法重新绘制，这时结构图就会变成这样：
\begin{figure}[H]
\centering
\begin{tikzpicture}[>=Stealth, scale=1.5]
    \node [circle, fill=white, draw, minimum size=1.2cm] (x01) at (-1, 1.2) {$x_1$};
    \node [circle, fill=white, draw, minimum size=1.2cm] (x02) at (-1, -1.2) {$x_2$};
    \node [circle, fill=white, draw, minimum size=1.2cm] (x11) at (2, 2) {$x_1^{(1)}$};
    \node [circle, fill=white, draw, minimum size=1.2cm] (x12) at (2, 0) {$x_2^{(1)}$};
    \node [circle, fill=white, draw, minimum size=1.2cm] (x13) at (2, -2) {$x_3^{(1)}$};
    \node [circle, fill=white, draw, minimum size=1.2cm] (y) at (5, 0) {$y$};
    \draw [->, lightgray] (x02) -- node [below right] {$0$} (x11);
    \draw [->, lightgray] (x02) -- node [below right] {$0$} (x12); 
    \draw [->, red] (x01) -- node [above left] {$1$} (x11);
    \draw [->, blue] (x01) -- node [above right] {$-1$} (x12);
    \draw [->, blue] (x01) -- node [above right] {$-1$} (x13);
    \draw [->, red] (x02) -- node [below left] {$1$} (x13);
    \draw [->, red] (x11) -- node [above right] {$1$} (y);
    \draw [->, blue] (x12) -- node [above] {$-1$} (y);
    \draw [->, red] (x13) -- node [below right] {$1$} (y);
    \node [lightgray] at (2, 2.6) {$+0$};
    \node [lightgray] at (2, 0.6) {$+0$};
    \node [lightgray] at (2, -1.4) {$+0$};
    \node [below] at (2, 1.6) {ReLU};
    \node [below] at (2, -0.4) {ReLU};
    \node [below] at (2, -2.4) {ReLU};
    \draw [dashed, thin] (1.4, -2.8) rectangle (2.6, 2.8);
    \node [above] at (2, 2.8) {中间层};
    \end{tikzpicture}
\caption{神经网络表示 $\max\{x_1, x_2\}$ 的另一种方法}
\end{figure}

虽然中间的神经元多了一些，但是它的结构看起来就统一而且整齐得多了。或许有人会有疑问，这里连的线变多了，不是把事情复杂化了吗？实际上并没有，恰恰相反，把它整齐地写出来才有利于算法的数值优化。

一个有趣的事实是，如果把 True 和 False 分别视作 1 和 0，那么只需要最多这样的两层就可以表示任意的逻辑函数。例如 
\[
\begin{aligned}
    x_1 \; \text{and} \; x_2 &= \text{ReLU}(x_1 + x_2 - 1) \\
    x_1 \; \text{or} \; x_2 &= \text{ReLU}(x_1) + \text{ReLU}(x_2 - x_1) \\
    x_1 \; \text{xor} \; x_2 &= \text{ReLU}(x_1 - x_2) + \text{ReLU}(x_2 - x_1) 
\end{aligned}
\]

这至少表明逻辑可以在一定程度上编码进神经网络中，用一些可调的权重来模拟逻辑门\footnote{用权重模拟逻辑门：这里仅说明它可以，不过这么做太奢侈了，很浪费储存和计算资源。}，因此从这一特例来看，求特征的交集、并集的操作确实可以自然地以权重的方式编码到网络的运算中。

推而广之，不难发现 ReLU 本质上完成的是将函数分段的操作。调整权重就可以做到在不同的区域选择不同的段，从而给出不同的表达式。虽然它在每一根区域内仍然是线性的，但却可以通过一些点上的弯折来实现非线性，表达能力比单纯的线性函数大大提高。这样的函数在数学上称为\textoverset{Piecewise Linear Function}{分段线性函数}，如我们所见，ReLU 函数就提供了一种通用的方式来实现分段线性函数，从而将关于“分类”的信息编码到网络中。

那么它不能表示什么函数呢？由于其分段线性的特性，不难证明它无法完全精准地表示光滑的曲线，例如 $y = x^2$。而且可以证明，对于任何一个分段线性函数 $f(x)$，都可以找到一个常数 $c$ 使对于 $\|x\|$ 足够大的时候，$f(x) \leq c\|x\|$。从而增长速度有限，无法表示指数函数或者高次的多项式函数。

这确实体现出了它的局限性，但这必然是它的弱点吗？并不一定。一方面，虽然它本身无法\textbf{精准地表示}光滑的函数，但是只要给定一个自变量的区间，在这样的函数堆叠多层之后总是可以调整参数，做到\textbf{良好地近似}给定的函数。事实上只需要四段就可以在区间 $[-1, 1]$ 上用如下的分段线性函数来相当好地近似 $x^2$ 了，例如下面的分段线性函数 $f(x)$：
\[
    f(x) = 2\text{ReLU}(x-1) + 2\text{ReLU}(x) + 2\text{ReLU}(-x) + 2\text{ReLU}(-x-1) - 0.04 
\]

图像是这样的：
\begin{figure}[H]
\centering
\begin{tikzpicture}[>=Stealth]
    \begin{axis}[
        axis lines=middle,
        axis equal,
        xlabel={$x$}, ylabel={$y$},
        grid=major,
        legend pos=north west
    ]
    \addplot[blue, thick, domain=-1:1, smooth] {x^2};
    \addlegendentry{$x^2$}

    \addplot[red, thick] coordinates {(-1, 0.96) (-0.5, 0.21) (0, -0.04) (0.5, 0.21) (1, 0.96)};
    \addlegendentry{$f(x)$}
    \end{axis}
\end{tikzpicture}
\caption{分段线性函数近似光滑函数}
\end{figure}

另一方面，虽然它的输出会被输入大小的一个常数倍所控制，但在很大程度上，这也避免了在第一章中多项式拟合的数值爆炸问题。此外，这提醒我们应当将模型的输入输出控制在一个范围之内。遵循这些原则，ReLU 网络的表达能力已经足够强大，能解决大多数实际问题。尽管仍有一些细节需要注意，但这并不影响我们对其整体能力的理解。

另外再提一嘴其它的激活函数。Sigmoid 函数\footnote{Sigmoid 函数：Sigmoid 来源于拉丁语，得名于其类似小写字母 sigma 变体 $\varsigma$ 的形状。}是一个 S 型函数，定义为
\[
    \text{Sigmoid}(x) = \frac{1}{1 + e^{-x}}
\]

输出随输入变化的图像是这样的，可见它把输入压缩到了 $[0, 1]$ 的范围内：
\begin{figure}[H]
\centering
\begin{tikzpicture}[>=Stealth]
    \begin{axis}[
        axis lines=middle,
        axis equal,
        xlabel={$x$}, ylabel={$y$},
        grid=major,
        legend pos=north west
    ]
    \addplot[blue, thick, domain=-2:2, samples=100] {1/(1+exp(-x))};
    \addlegendentry{Sigmoid($x$)}
    \end{axis}
\end{tikzpicture}
\caption{Sigmoid 函数图像}
\end{figure}

tanh 函数是双曲正切函数，其定义为
\[
    \text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]

它的图像和 Sigmoid 函数很类似，只是经过了一个伸缩和平移，输出范围是 $[-1, 1]$：
\begin{figure}[H]
\centering
\begin{tikzpicture}[>=Stealth]
    \begin{axis}[
        axis lines=middle,
        axis equal,
        xlabel={$x$}, ylabel={$y$},
        grid=major,
        legend pos=north west
    ]
    \addplot[blue, thick, domain=-2:2, samples=100] {tanh(x)};
    \addlegendentry{tanh($x$)}
    \end{axis}
\end{tikzpicture}
\caption{tanh 函数图像}
\end{figure}

早期的研究中，它们出现在许多生物学的研究中，可以描述生物神经元的激活或者极化程度，于是人工神经网络出于仿生的考虑也使用了它们。然而它们在两端很小的导数也为优化带来了许多麻烦，导致了\textoverset{Vanishing Gradient}{梯度消失}\footnote{梯度消失：是指在深度神经网络中，由于输出随输入的变化过于小，导致信息无法有效地从输出传回输入，从而使得网络难以优化学习的现象。关于梯度的进一步介绍会在后文给出，此处可以简单理解为信息回传受阻。}的问题，后来逐渐被 ReLU 函数取代，仅在特定层要将输出限制在给定范围内时才使用。虽然近期有\uhref{https://arxiv.org/pdf/2503.10622}{研究}指出现在的优化器有能力克服这个问题，即使使用 tanh 仍然可以正常地优化，不过这也仅是一个理论上的结果，实际应用中通常认为它们仍然不如 ReLU 函数好用。从此也能看见人工智能的发展并非一帆风顺，仿生不是唯一的出路，人工的神经网络的发展和对其规律的认识必然要走过曲折的探索，才能形成一套独特而成熟的方法论。

不过 ReLU 在 $x<0$ 的区域也存在斜率为 $0$ 导致梯度消失的问题，为此人们还提出了一些变体，例如 Leaky ReLU 函数，它在 $x<0$ 的区域也有一个小的斜率，定义为
\[
    \text{Leaky ReLU}(x) = \begin{cases}
        x, & x \geq 0 \\
        \alpha x, & x < 0 
    \end{cases}
\]

上式中 $\alpha$ 是一个小的常数，通常取 $0.01$，它同样简单易于计算。还有一些较为复杂的变体，包括\textoverset{Gaussian Error Linear Unit}{高斯误差线性单元}(GELU)，\textoverset{Exponential Linear Unit}{指数线性单元}(ELU) 等，都在一定程度上克服了 ReLU 导数为 $0$ 导致信息传播不畅的问题。不过这些都属于工程上的细节问题，读者可以在需要的时候再去了解。

由此我们更加具体化地认识到了神经网络的工作原理：它的基本单元由线性函数与激活函数交替组成。每一层都可以看作是对输入进行线性组合，然后通过激活函数进行非线性变换以实现更复杂的表达能力。这让网络以一种统一的方式来处理输入数据，并有能力通过调整参数拟合复杂的输出。

\newpage

\subsection{神经网络的训练}

有了前面的模块，我们已经可以搭建起简单的神经网络了，理论上通过“合适的”参数就可以拟合任意的函数了，那么“合适的”参数从何而来呢？回顾第一章中，线性拟合问题可以完全通过解析方法求解得到最优的参数，然而神经网络网络却是一个复杂的非线性函数，根本不可能对各种情况分段讨论给出解析解。那么我们是否就无能为力了呢？当然不是。通过一些手段可以让信息从数据定向地“流向”模型的参数，通过一套优化算法来调整参数，使得模型的输出尽可能地接近目标，这一过程就叫做\textoverset{Training}{训练}，这一调整的过程使用的各种优化方法大多是基于\textoverset{Gradient Descent}{梯度下降法}\footnote{Gradient 词源说明：gradi 是一个拉丁语词根，意为行走，与 “步行”相关。与 progress, regress 等词中的 “gress” 是同一词根。因此从词根来看其实可以理解为在步行中下降。}的。

正如第一章中所见，拟合的好不好需要量化为损失的数值，来定量分析大小如何。而在机器学习上，我们就是借助这一可量化的评判标准——利用梯度下降算法给出了往更好的方向前进的一步。以\uhref{https://www.bilibili.com/video/BV1Ux411j7ri}{3Blue1Brown的视频}为代表的一系列科普教程中都有对这一算法的良好讲解。不过此处我仍然通过下山这个最经典的比喻来试图说明这一算法的原理。

前面我提到过，每层函数的函数需要可导，可导性就在此处显得尤为重要。即使不知道函数的总体行为，但是我们仍然可以通过导数对当前位置附近的情况形成大致的感知。就像在山上行走时，想快速下山的人并不需要知道整座山的形状，只要知道当前的坡度，就可以沿着向下的方向走。不过这一步不能走太大，在机器学习中，走太大了可能会走到山的另一边去，导数仅为优化提供了局部的信息。

在一元函数中，导数的正负可以直接指出当前的坡度是向上还是向下，局部的近似意味着对于很小的 $\Delta x$ 可以写出
\[
    \Delta y \approx f'(x_0)\Delta x
\]

看起来像是这样：
\begin{figure}[H]
\centering
\begin{tikzpicture}[>=Stealth, xscale=2]
    \draw [thin, ->] (-1, 0) -- (2, 0) node [below] {$x$};
    % \draw [thin, ->] (0, -0.5) -- (0, 3.5) node [left] {$f(x)$};
    \draw [domain=-0.7:1.5, smooth, variable=\x, blue, thick] plot ({\x}, {\x*\x+1}) node [above] {$f(x)$};
    \draw [domain=-0.3:1.3, smooth, variable=\x, red, thick] plot ({\x}, {\x-0.25+1});
    \draw [fill=lightgray] (0.5, 1.25) -- node [below] {$\Delta x$} (1.0, 1.25) -- node [right] {$f'(x_0) \Delta x$} (1.0, 1.75) -- cycle;
    \draw [dashed, thin] (0.5, 1.25) -- (0.5, 0) node [below] {$x_0$};
    \end{tikzpicture}
\caption{一元函数的导数近似}
\end{figure}

导数反映的是切线的斜率，也就是函数在局部的变化率信息。为了让 $f(x)$ 变小，我们只需要让 $\Delta x$ 的方向与 $f'(x)$ 相反就可以了。也就是说，可以令
\[
    \Delta x = -\eta f'(x)
\]

这里的 $\eta$ 决定了一步要走多大，称为\textoverset{Learning Rate}{学习率}。如果 $\eta$ 太小，可能会走得很慢；如果 $\eta$ 太大，可能会走过头。一步步地往下走，我们的位置就像是一个小球，从斜坡上滚下去，最终停留在一个极小值\footnote{极小值：不过这个极小值可能只是山谷中的一个盆地，而非全局的最低点。}。

不过我们的神经网络毕竟有很多的参数要在同时调整，你或许想问，我们能不能固定其它的参数，只调整一个参数，把它变成一元函数来处理呢？可以是确实可以，这可以把它变为了一元函数的情况，只是这种方法太低效了，更高效且优雅的做法是联合优化它们，同时调整所有的参数。听起来很合理，但是这初听起来似乎有点玄学，毕竟我们并不知道所有的参数之间如何作用的，庞大的网络让它们之间的关系变得复杂而难以捉摸。因此我认为需要在此处引入一些多元微积分的知识，来把求导操作推广到多维，得到所谓的梯度与\textoverset{Backpropagation}{反向传播}算法。

在一元情况下，导数是这样定义的
\[
    f'(x_0) = \lim_{\Delta x\to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x}
\]

但是在输入 $x$ 是向量的情况下，数学运算并不允许除以一个向量，那么应该如何求导呢？应回看一下最初的动机：求出导数的目的是为了让它在局部能用简单的线性函数来近似。如果使用 $\mathrm{d}x$ 表示一个输入的变化量，用 $\mathrm{d}y$ 表示我们对输出结果变化量的估计，那么在一元的情况下这个式子可以简单地写成
\[
    \mathrm{d}y = f'(x_0) \mathrm{d}x
\]

而在多元情况下，应当估计每个分量的变化量如何影响输出，再把这些线性的分量叠起来，从而有线性近似
\[
    \mathrm{d}y = a_1 \mathrm{d}x_1 + a_2 \mathrm{d}x_2 + \cdots + a_n \mathrm{d}x_n
\]

而线性近似意味着在 $\mathrm{d}x$ 足够小的情况下，这一近似给出的估计 $\mathrm{d}y$ 与真实的变化量 $\Delta y$ 之间的差距是可以忽略的，也就是说
\[
    \lim_{\mathrm{d}x\to 0} \frac{\Delta y - \mathrm{d}y}{\|\mathrm{d}x\|} = 0
\]

不过上面的极限在实际应用中可以假设成立，在足够好的可导的假设下，需要关心的只是如何算出这些系数 $a_i$。对于标量函数 $f$ 和向量输入 $x$，这样的一些系数所构成的向量就叫做\textoverset{Gradient}{梯度}，$f$ 对 $x$ 的梯度在 $x_0$ 处的值常记作 $\nabla_x f(x_0)$，而从根本上，它最主要的作用就是提供了线性近似 $\mathrm{d}y = \nabla_x f(x_0) \cdot \mathrm{d}x$ 的系数\footnote{说明：这里的 $\cdot$ 是向量点积，表明逐分量相乘并求和}。为了沿着网络层层回溯求出梯度，需要再引入链式法则。

设想如果 $y$ 本身并不是 $x$ 的函数，而是经过两步得到的 $x^{(1)} = f_1(x)$ 和 $y = f_2(x^{(1)})$，那么应该怎么求导呢？这一过程需要分步进行。如果从一个逐元素的视角看来，实际上是先使用 $\mathrm{d}x^{(1)}$ 来估计 $y$ 的变化量，然后再借助 $\mathrm{d}y$ 与 $\mathrm{d}x^{(1)}$ 之间的关系来估计 $\mathrm{d}x$ 对 $y$ 的影响。更为具体地说：如果能写出
\[
    \mathrm{d}y = a_1 \mathrm{d}x^{(1)}_1 + a_2 \mathrm{d}x^{(1)}_2 + \cdots + a_m \mathrm{d}x^{(1)}_m
\]

而 $\mathrm{d}x$ 对 $\mathrm{d}x^{(1)}$ 各项的影响分别是
\[
\begin{aligned}
    \mathrm{d}x^{(1)}_1 &= b_{11} \mathrm{d}x_1 + b_{12} \mathrm{d}x_2 + \cdots + b_{1n} \mathrm{d}x_n \\
    \mathrm{d}x^{(1)}_2 &= b_{21} \mathrm{d}x_1 + b_{22} \mathrm{d}x_2 + \cdots + b_{2n} \mathrm{d}x_n \\
    &\vdots \\
    \mathrm{d}x^{(1)}_m &= b_{m1} \mathrm{d}x_1 + b_{m2} \mathrm{d}x_2 + \cdots + b_{mn} \mathrm{d}x_n
\end{aligned}
\]

那么怎么把 $\mathrm{d}y$ 写成 $c_1 \mathrm{d}x_1 + c_2 \mathrm{d}x_2 + \cdots + c_n \mathrm{d}x_n$ 的形式呢？答案非常简单粗暴：把每个 $\mathrm{d}x^{(1)}_i$ 代入到 $\mathrm{d}y$ 的表达式中，就有了
\[
\begin{aligned}
    \mathrm{d}y =\, & a_1 (b_{11} \mathrm{d}x_1 + b_{12} \mathrm{d}x_2 + \cdots + b_{1n} \mathrm{d}x_n) \, + \\
    & a_2 (b_{21} \mathrm{d}x_1 + b_{22} \mathrm{d}x_2 + \cdots + b_{2n} \mathrm{d}x_n) \, + \\
    & \cdots \, +\\
    & a_n (b_{m1} \mathrm{d}x_1 + b_{m2} \mathrm{d}x_2 + \cdots + b_{mn} \mathrm{d}x_n)\\
    =\, & (a_1 b_{11} + a_2 b_{21} + \cdots + a_n b_{m1}) \mathrm{d}x_1 \,+ \\
    & (a_1 b_{12} + a_2 b_{22} + \cdots + a_n b_{m2}) \mathrm{d}x_2 \,+ \\
    & \cdots \, \,+\\
    & (a_1 b_{1n} + a_2 b_{2n} + \cdots + a_m b_{mn}) \mathrm{d}x_n
\end{aligned}
\]

由此可见，可以写出表达式
\[
\begin{aligned}
    c_j &= a_1 b_{1j} + a_2 b_{2j} + \cdots, a_m b_{mj}\\
    \mathrm{d}y &= c_1 \mathrm{d}x_1 + c_2 \mathrm{d}x_2 + \cdots + c_n \mathrm{d}x_n
\end{aligned}
\]

这表明，只要我们知道下一层计算出来的梯度 $a_i$ 和联系起两层的 $b_{ij}$，就可以将下一层的梯度“回溯”到上一层，计算出上一层的梯度 $c_j$，这种方法被称作反向传播。于是问题被进一步拆解，变成了更为细化的小问题：如何求出这里每层之间梯度传播的关系？

这里选取最简单的一层 $x^{(k)} = \text{ReLU}(w x^{(k-1)} + b)$ 来说明，其中 $w$ 是权重矩阵，$b$ 是偏置。我们需要理解 $\mathrm{d}x^{(k)}_i$ 应当如何表示。不过这里需要注意一点，在求导时不但要考虑 $x^{(k)}$ 的变化量，还要考虑 $w$ 和 $b$ 的变化量，因为 $w$ 和 $b$ 作为参数也会影响到 $x^{(k)}$ 的值，更是我们希望调整的对象。

首先写出 $x^{(k)}_i$ 的表达式
\[
    x^{(k)}_i = \text{ReLU}(w_{i1} x^{(k-1)}_1 + w_{i2} x^{(k-1)}_2 + \cdots + w_{in} x^{(k-1)}_n + b_i)
\]

接下来分类讨论，在 $\text{ReLU}$ 内部小于等于 0 的情况下，因为输入有微小变化时输出仍然为 0，所以
\[
    \mathrm{d}x^{(k)}_i = 0
\]

而当其输入大于 0 时，ReLU 可以去除掉，即变为
\[
\begin{aligned}
    \mathrm{d}x^{(k)}_i &= \mathrm{d}(w_{i1} x^{(k-1)}_1 + w_{i2} x^{(k-1)}_2 + \cdots + w_{in} x^{(k-1)}_n + b_i) \\
    &= w_{i1} \mathrm{d}x^{(k-1)}_1 + w_{i2} \mathrm{d}x^{(k-1)}_2 + \cdots + w_{in} \mathrm{d}x^{(k-1)}_n \\
    &\qquad +x^{(k-1)}_1 \mathrm{d}w_{i1} + x^{(k-1)}_2 \mathrm{d}w_{i2} + \cdots + x^{(k-1)}_n \mathrm{d}w_{in} + \mathrm{d}b_i \\
    &= w_{i:} \cdot \mathrm{d}x^{(k-1)} + x^{(k-1)} \cdot \mathrm{d}w_{i:} + \mathrm{d}b_i
\end{aligned}
\]

这里仍然沿用了向量的点积表示法，$w_{i:}$ 表示 $w$ 的第 $i$ 行。由此也可以看出 ReLU 的好处：使用它作为激活函数的梯度回溯非常方便。

这里我们会发现梯度信息发生了一个“分岔”，一部分信息传递给了 $x^{(k-1)}$，另一部分则传递给了 $w$ 和 $b$。传递给 $w$ 和 $b$ 的信息将在后续用于更新参数，而传递给 $x^{(k-1)}$ 的信息则会继续向后传递求出离结果更远、离输入更近的层的参数梯度。直到传递到输入层，最终得到所有参数的梯度，关于原始输入 $x$ 的输入则会被丢弃\footnote{丢弃：因为我们并不需要对输入进行调整，只需要对参数进行调整。不过一些研究表明，观察输入的梯度信息可以帮助我们理解模型认为哪些特征更加重要。}。这个过程看起来像是这样，如果说正向计算是把参数信息参与到计算中，一步步计算得到最终的结果（由于我们希望最小化损失，所以在模型的最终输出之后还有一个箭头表示损失函数）：
\begin{figure}[H]
\centering
\begin{tikzpicture}[>=Stealth, scale=1.2]
    \node (x) at (0, 0) {$x$};
    \node (x1) at (2, 0) {$x^{(1)}$};
    \node (x2) at (4, 0) {$\cdots$};
    \node (x3) at (6, 0) {$x^{(k-1)}$};
    \node (x4) at (8, 0) {$x^{(k)}$};
    \node (x5) at (10, 0) {$\cdots$};
    \node (y) at (12, 0) {$y$};
    \node (w1) at (1, -1) {$w^{(1)},b^{(1)}$};
    \node (w3) at (5, -1) {$w^{(k-1)},b^{(k-1)}$};
    \node (w4) at (7, -1) {$w^{(k)},b^{(k)}$};
    \node (w5) at (11, -1) {$w^{(\text{out})}, b^{(\text{out})}$};
    \node (loss) at (13, 0) {$l$};
    \draw [->] (x) -- (x1);
    \draw [->] (x1) -- (x2);
    \draw [->] (x2) -- (x3);
    \draw [->] (x3) -- (x4);
    \draw [->] (x4) -- (x5);
    \draw [->] (x5) -- (y);
    \draw [->] (w1) -- (x1);
    \draw [->] (w3) -- (x3);
    \draw [->] (w4) -- (x4);
    \draw [->] (w5) -- (y);
    \draw [->] (y) -- (loss);
\end{tikzpicture}
\caption{正向计算}
\end{figure}

那么反向传播就是考虑损失变化的估计量 $\mathrm{d}l$，并将它的梯度信息经由 $\mathrm{d}y$ 和各个中间计算结果 $\mathrm{d}x^{(k)}$ 逐层传递回去，直到输入层。
\begin{figure}[H]
\centering
\begin{tikzpicture}[>=Stealth, scale=1.2]
    \node (x) at (0, 0) {$\mathrm{d}x$};
    \node (x1) at (2, 0) {$\mathrm{d}x^{(1)}$};
    \node (x2) at (4, 0) {$\cdots$};
    \node (x3) at (6, 0) {$\mathrm{d}x^{(k-1)}$};
    \node (x4) at (8, 0) {$\mathrm{d}x^{(k)}$};
    \node (x5) at (10, 0) {$\cdots$};
    \node (y) at (12, 0) {$\mathrm{d}y$};
    \node (w1) at (1, -1) {$\mathrm{d}w^{(1)},\mathrm{d}b^{(1)}$};
    \node (w3) at (5, -1) {$\mathrm{d}w^{(k-1)},\mathrm{d}b^{(k-1)}$};
    \node (w4) at (7, -1) {$\mathrm{d}w^{(k)},\mathrm{d}b^{(k)}$};
    \node (w5) at (11, -1) {$\mathrm{d}w^{(\text{out})}, \mathrm{d}b^{(\text{out})}$};
    \node (loss) at (13, 0) {$\mathrm{d}l$};
    \draw [<-] (x) -- (x1);
    \draw [<-] (x1) -- (x2);
    \draw [<-] (x2) -- (x3);
    \draw [<-] (x3) -- (x4);
    \draw [<-] (x4) -- (x5);
    \draw [<-] (x5) -- (y);
    \draw [<-] (w1) -- (x1);
    \draw [<-] (w3) -- (x3);
    \draw [<-] (w4) -- (x4);
    \draw [<-] (w5) -- (y);
    \draw [<-] (y) -- (loss);
\end{tikzpicture}
\caption{反向传播}
\end{figure}

在图中看到从 $\mathrm{d}x^{(k)}$ 指向 $\mathrm{d}x^{(k-1)}$ 或者 $\mathrm{d}w^{(k)}, \mathrm{d}b^{(k)}$ 的箭头时，它指的是关于 $x^{(k)}$ 的梯度信息被传递到了 $x^{(k-1)}$ 和 $w^{(k)}, b^{(k)}$ 上。上面的图只是下面表达式的一个形象说明（这里用 $g_x^{(k)}$ 表示对 $x^{(k)}$ 的梯度，同理定义对 $w$ 和 $b$ 的梯度符号）
\[
\begin{aligned}
    \mathrm{d}l &= g_y \cdot \mathrm{d}y \\
    &= g_x^{(\text{out})} \cdot \mathrm{d}x^{(\text{out})} + g_w^{(\text{out})} \cdot \mathrm{d}w^{(\text{out})} + g_b^{(\text{out})} \cdot \mathrm{d}b^{(\text{out})} \\
    &= (\text{向前展开这一部分}) + g_w^{(\text{out})} \cdot \mathrm{d}w^{(\text{out})} + g_b^{(\text{out})} \cdot \mathrm{d}b^{(\text{out})} \\
    &= g_x^{(k)} \cdot \mathrm{d}x^{(k)} + g_w^{(k)} \cdot \mathrm{d}w^{(k)} + g_b^{(k)} \cdot \mathrm{d}b^{(k)} \\
    & \qquad+ \cdots + g_w^{(\text{out})} \cdot \mathrm{d}w^{(\text{out})} + g_b^{(\text{out})} \cdot \mathrm{d}b^{(\text{out})}\\
    &= g_x^{(k-1)} \cdot \mathrm{d}x^{(k-1)} + g_w^{(k-1)} \cdot \mathrm{d}w^{(k-1)} + g_b^{(k-1)} \cdot \mathrm{d}b^{(k-1)} \\
    & \qquad+ g_w^{(k)} \cdot \mathrm{d}w^{(k)} + g_b^{(k)} \cdot \mathrm{d}b^{(k)} + \cdots \\
    &\qquad + g_w^{(\text{out})} \cdot \mathrm{d}w^{(\text{out})} + g_b^{(\text{out})} \cdot \mathrm{d}b^{(\text{out})}\\
    & = \cdots \\
    & = g_x \cdot \mathrm{d}x \\
    & \qquad + g_w^{(1)} \cdot \mathrm{d}w^{(1)} + g_b^{(1)} \cdot \mathrm{d}b^{(1)} \\
    & \qquad + g_w^{(2)} \cdot \mathrm{d}w^{(2)} + g_b^{(2)} \cdot \mathrm{d}b^{(2)} \\
    & \qquad + \cdots \\
    & \qquad + g_w^{(\text{out})} \cdot \mathrm{d}w^{(\text{out})} + g_b^{(\text{out})} \cdot \mathrm{d}b^{(\text{out})}\\
\end{aligned}
\]

随着式子每次将第一项不断地展开，树状的计算图也从计算结果的终点不断向前回溯，最终在每个终止节点处得到对每个分量的梯度信息。理论上只需要手动维护这个图的状态就能完成反向传播的计算，不过实际应用中已经有了非常方便的自动求导工具，现代的机器学习库 PyTorch 和 TensorFlow 都有类似的功能，它们都会在内部维护一个计算图，自动地完成反向传播的计算。以 PyTorch 为例，用户不需要关心如何维护这个图，只需要设置 \texttt{requires\_grad} 属性为 \texttt{True}，在计算后对损失调用 \texttt{backward} 方法，就能自动地完成反向传播的计算。

在训练时数据点通常是固定的，所以可以直接令 $\mathrm{d}x = 0$，这样前面的梯度信息就仅留下与待优化的参数相关的部分
\[
\begin{aligned}
    \mathrm{d}l &= g_w^{(1)} \cdot \mathrm{d}w^{(1)} + g_b^{(1)} \cdot \mathrm{d}b^{(1)} \\
    &\qquad + g_w^{(2)} \cdot \mathrm{d}w^{(2)} + g_b^{(2)} \cdot \mathrm{d}b^{(2)} \\
    &\qquad + \cdots \\
    &\qquad + g_w^{(\text{out})} \cdot \mathrm{d}w^{(\text{out})} + g_b^{(\text{out})} \cdot \mathrm{d}b^{(\text{out})}\\
\end{aligned}
\]

而如果我们再做一层抽象，把这一堆参数塞到一个叫做 $\theta$ 的向量中，那么就可以把上面的式子写成
\[
    \mathrm{d}l = \nabla_\theta l \cdot \mathrm{d}\theta
\]

其中 $\nabla_\theta l$ 就是损失函数 $l$ 对参数 $\theta$ 的梯度，其各个分量表明了损失函数对每个参数的敏感程度。而仿照一元函数下山的做法，同样可以按照如下的方式沿着“下山最快的方向”来更新参数
\[
    \mathrm{d}\theta^* = -\eta \nabla_\theta l
\]

从图形上看，这个过程是垂直于等高线的方向在向下走，最终停留在一个极小值处。或许初看会觉得这一优化方法和等高线并不相关，但如果仔细想想就会发现，实际上等高线的形状就是损失函数的等值线，而沿着等值线的切线方向意味着
\[
    \mathrm{d}l = \nabla_\theta l \cdot \mathrm{d}\theta = 0
\]

前一个部分是梯度的方向，而令上式等于 $0$ 成立的方向 $\mathrm{d}\theta$ 就是等高线的切线方向。两个向量的内积为 $0$ 意味着它们是垂直的，用心观察的读者或许会发现，地图上的水系与等高线交汇之处，两条线往往是垂直的，水往低处走是一种自然的梯度下降。

在实际的神经网络中通常有数万乃至数亿个参数，在这样庞大的参数空间中梯度下降虽然有着下降最快的方向的理论支持，但想象一个高维空间已经足够困难，再去想它里面的等高线是什么样子就更是难上加难，所以为了便于理解，还是看一个低维空间中非常简单的样例来帮助理解。

以一个简单的函数 $f(\theta) = 2\theta_1^2 + \theta_2^2$ 为例，假设从 $(1, 1)$ 处开始迭代，取每次更新的 $\eta = 0.1$，因为 $\mathrm{d}f(\theta) = 4\theta_1 \mathrm{d}\theta_1 + 2\theta_2 \mathrm{d}\theta_2$，所以在第一步梯度为 $(2\theta_1, \theta_2) = (4, 2)$，取 $\mathrm{d}\theta^* = -\eta \nabla_\theta f$ 可以得到下一步的 $\theta$。进行 10 次迭代后，得到的点在等值线图上会按照如下图所示的方式移动，可以看到每次更新的 $\theta$ 都是垂直于等高线线的方向向下走的，逐渐逼近最优解 $(0, 0)$
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{img/gradient_descent.png}
\caption{梯度下降}
\end{figure}

不过同时我想也要给出一个错误样例说明学习率 $\eta$ 调太大会发生什么，假设刚才的例子中我们把 $\eta$ 调到 0.5, 就会发现它直接跳过极小值，跳到了空间中的对边，之后就在 $(-1, 0)$ 和 $(1, 0)$ 之间来回震荡了起来，无法收敛。
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{img/gradient_descent_0.5.png}
\caption{学习率设置不当的梯度下降}
\end{figure}

这个现象在高维空间中也会发生，如果学习率 $\eta$ 过大，常常会观察到 loss 在某个值附近反复震荡，无法收敛。

在实际有各种不同的梯度下降方法，最简单的是\textoverset{Stochastic Gradient Descent}{随机梯度下降法}(SGD)，它的基本思想是每次只随机抽取部分数据来计算梯度，来更新参数。这样做的好处是每次只需要计算一小部分数据，速度快；坏处是每次计算的梯度并不准确，可能会导致参数在最优解附近震荡。有的为了加速收敛速度，使用\textoverset{Momentum}{动量法}，将梯度信息不作为速度，而作为加速度来更新参数，让它能快速冲下坡。还有的使用\textoverset{Adaptive Momentum Method}{自适应动量法}(Adam)，在每次更新时考虑了历史梯度信息的影响，来调整学习率。这些方法都以各自的方式帮我们找到了更好的参数更新方式，来加速收敛速度。不过动量也不是越大越好，正如学习率不是越大越好一样，过大的动量会导致参数在最优解附近震荡，甚至无法收敛。

\newpage

\subsection*{动手实践：简单的网络实践}

为了让读者对训练神经网络形成一个大概的感知，这里我会用一个极其简单的例子来说明如何训练一个神经网络。而我们的目标就是：拟合一个一元函数 $f(x) = \sin 2x$，其中 $x \in [-5, 5]$。不过为了实现这个目标，需要先简单讲一下 Python 中的数组操作。

Python 中内置的数组类型是列表，它是动态的，可以在运行时改变长度，这不同于 C 语言中为数组分配一块内存空间并储存指定类型元素。而 Python 列表中的“元素”也实际上是对象的引用。虽然原则上通常推荐用列表存储相同类型的元素\footnote{通常推荐：写类型标注时通常把列表的类型标记为 \texttt{list[T]}，其中 \texttt{T} 是元素的类型，例如 \texttt{int}。}，但在运行中，这类强制的要求并不存在。与 C 等语言不同的是，Python 为了实现重载符，其中的四则运算是通过调用某些特定的函数实现的\footnote{四则运算：例如 $a + b$ 实际上是调用了 \texttt{a.\_\_add\_\_(b)} 函数来实现的，仅加法这一个操作就要经过多层函数调用的包装，比 C 语言的加法慢了大概一到两数量级。}，如果想在列表实现向量运算，在遍历列表的过程中就需要不停地检查对象的类型并调度对应的方法。打个比方，就像是一个工程师虽然知道生产的每一个环节，但拿到一个工件后也要先检查它的类型才能决定用什么工具来加工它。能处理的东西确实是灵活了，但是另一面是效率低下了。而与之对比的是流水线工人，他们面对的工件和工序是完全确定的，不需要检查工件的类型，只需要按照固定的流程来加工就可以了，省去了分析的时间，而且也方便同时上很多人来加工\footnote{多个人加工：指对于算术运算，CPU 中有指令可以并行地执行。}。

用 Python 原生的运算来做数值运算显然不够高效，但是 Python 的精髓就在于它的灵活性和可扩展性。可以说如果没有丰富的第三方库，Python 就不可能发展成今天这样一个强大的语言。在数值计算这一块的基石就是 NumPy，自其 2006 年发布 1.0 版本\footnote{1.0 版本：虽然对于 NumPy 来讲是 1.0 版本，不过它是在历史更久远的 Numeric 和 Numarray 两个已有的数值运算库基础上，为了统一数组运算，作为科学计算库 SciPy 的一个核心模块开发的。}开始，Python 社区内就开始广泛地使用，已经成为了 Python 进行数值计算的\textoverset{De Facto Standard}{事实标准}\footnote{事实标准：指虽然没有官方强制规定要用 NumPy，但 Python 社区内几乎所有教程、科研工作、乃至工业界都在使用它（或依赖它的其它模块），它成为了“数组运算”的通用接口标准。}，重塑了整个生态\footnote{生态：例如 SciPy 和 matplotlib 分别作为科学运算库和绘图库开发早于 NumPy，但是后来它们的底层和接口都改为使用 NumPy。在社区对矩阵运算的庞大需求的推动下，连一向谨慎添加新语法的 Python 官方都专门引入了\uhref{https://peps.python.org/pep-0465/}{PEP 465 提案}，早在 Python 3.5 就引入了额外的 \texttt{@} 运算符来表示矩阵乘法。}。不少人都\uhref{https://www.zhihu.com/question/645463253}{认为} NumPy 设计很好，在运算速度顶级的同时其语法和 Python 的语法风格保持了一致，同时还提供了清晰的接口和丰富的功能。有不少的优质教程可以帮助读者快速上手，\uhref{https://numpy.org/doc/stable/user/absolute_beginners.html}{NumPy 官方的初学者指南}已经是一份非常清晰的教程，其中引用的部分图片取自\uhref{https://jalammar.github.io/visual-numpy/}{NumPy 的图形化展示}，即使单独看这一篇图形化教程也很好。只要用心搜索，中文互联网上亦不乏相关的优质资料，\uhref{https://zhuanlan.zhihu.com/p/396444973}{一些知乎上文章}的清晰度与丰富度也已经可以媲美官方教程，让没有经验的读者也可以快速理解 NumPy 的数组接口设计。

虽有珠玉在前，众多的教程已清晰教会了我们如何使用基础的数组操作，但我还是决定简单地讲一下数组化运算的逻辑。第一条原则是数组中的元素类型是相同的，通常 NumPy 会自动推导\footnote{自动推导：比如整型数组会变成 int64，浮点数组变成 float64。但也可以手动指定 dtype，以防不符合预期，例如如果数据点是整数但是希望参与 64 位浮点运算，那么也可以手动指定数据为 float64 类型。}\textoverset{Data Type}{数据类型}（即 dtype 参数）。第二条则是对于加减乘除这样的四则运算和 NumPy 中提供的所有一元函数（例如 sin, cos, exp 等），运算都是逐个进行的。例如两个 64 位浮点数类型的数组四则运算的结果如下。
\begin{minted}[bgcolor=gray!15, frame=single, framesep=10pt, baselinestretch=1.2]{python}
import numpy as np
x = np.array([1, 2, 3], dtype=np.float64)
y = np.array([4, 5, 6], dtype=np.float64)

print(x + y) # [5. 7. 9.]
print(x - y) # [-3. -3. -3.]
print(x * y) # [ 4. 10. 18.]
print(x / y) # [0.25 0.4  0.5 ]
\end{minted}

与经典的向量操作对比，加减法都完全相同。但是读者或许会感到疑惑，向量点积应该返回一个标量，而向量间并未定义除法。乘除法为什么要这么定义呢？因为数组不仅仅是高维空间中的一个向量，它同样可以用于表示一个函数在每一点上的取值\footnote{函数取值：这里的函数是广义上的，例如图像也可以看成是一个关于空间位置 $(i, j)$ 的二元函数，其中 $i,j$ 分别表示一个像素点的行号、列号。乘 mask 就是一个典型的逐像素作用的例子。}。逐点的乘除法不过是 $f(x) = g(x) h(x)$ 和 $f(x) = g(x) / h(x)$ 在有限个点上的表示。按传统的写法，需要通过一个循环遍历所有的 $x$ 并写入每个点的运算结果，逐元素运算则省去了这样手动遍历的麻烦。不过 \texttt{*} 用于逐元素乘法不代表无法进行想要的的点积运算。在上面的例子中，如果想计算两个向量的点积，只需 \texttt{x @ y} 或者更为明确的 \texttt{x.dot(y)} 就可以了。

了解了 NumPy 的基本原理后，获得 $f(x) = \sin 2x$ 在 $[-5, 5]$ 的样本就非常简单了。\texttt{np.linspace} 可以在给定的闭区间上均匀地采样指定的点，例如如果我们希望在 $[-5, 5]$ 上均匀采样 200 个点，导入 numpy 后只需要两行代码就能得到预期的样本：
\begin{minted}[bgcolor=gray!15, frame=single, framesep=10pt, baselinestretch=1.2]{python}
import numpy as np
x = np.linspace(-5, 5, num=200)
y = np.sin(2 * x)
\end{minted}

数据已经准备好，再下一步则是搭建一个用来拟合的神经网络了。NumPy 的数组操作很好、很优雅，但是要搭建神经网络还少一层包装。只需要回忆下训练的过程就会发现，参数更新时需要求出各个环节的梯度，而计算第 $k$ 层的参数梯度需要用到上一层的输出 $x^{(k - 1)}$ 和下一层传回的梯度 $g_x^{(k)}$。也就是说为了计算梯度，数据在正向传播的过程中需要留下“痕迹”。计算进入下一层并不能直接将它们丢弃，而是还要储存直到到梯度计算与权重更新之后才能删除。诚然，其中的许多步骤可以手动推导：写出解析的表达式，自己维护一个表用于暂存这些状态，然后手动计算梯度。但是手动计算\textoverset{Gradient Function}{梯度函数}的时代已经过去了，从最快上手的角度来看，PyTorch 中\textoverset{Automatic Gradient}{自动微分}系统的包装已经很完备，让我们网络结构就能通过样本自动完成梯度计算与优化了。学会钻木取火也许对荒野求生有很大作用，但我们作为学习做饭的现代人类，没有理由不直接打开家里的天然气灶。

PyTorch 有着类似 NumPy 的数组接口，但多了许多对网络的包装。常见的用法只需要在自定义模块时继承 \texttt{torch.nn.Module}，并重写\textoverset{Initialization}{初始化}方法 \texttt{\_\_init\_\_} 和正向计算方法 \texttt{forward} 即可，将求导和参数更新的一切交给 PyTorch 的自动微分系统和优化器来处理。例如对于拟合 $f(x) = \sin 2x$ 的例子，只需要定义三层足矣：$1\to 10\to 10\to 1$\footnote{这样的中间层数量和规律并非是固定的，这里仅是随意的选择。}，每一层都是一个带偏置的线性层，在中间两层加上 Leaky ReLU 激活函数并取 $\alpha = 0.1$\footnote{$\alpha$：含义可以回顾本章第一节激活函数。}。最后一层的输出是一个标量，表示对 $f(x)$ 的预测值。定义网络的过程如下：
\begin{minted}[bgcolor=gray!15, frame=single, framesep=10pt, baselinestretch=1.2]{python}
import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()  # Initialize the parent class
        self.linear1 = nn.Linear(1, 10)
        self.linear2 = nn.Linear(10, 10)
        self.linear3 = nn.Linear(10, 1)
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.1)
    def forward(self, x):
        x = self.leaky_relu(self.linear1(x))
        x = self.leaky_relu(self.linear2(x))
        x = self.linear3(x)
        return x
\end{minted}

最上面的 \texttt{SimpleModel(nn.Module)} 表示我们定义的模型继承了 PyTorch 中的模块基类，\texttt{nn.Linear} 表示线性层。\texttt{forward} 作为模型调用的正向计算过程，通过 \texttt{model(x)} 调用时会自动触发 \texttt{forward} 的计算。

不过有一点重要的是调用 \texttt{model(x)} 时，因为一次训练时需要用到多个样本，所以输入的 $x$ 需要是一个二维数组，第一维表示样本数，第二维表示每个样本的特征数。对一元函数来说，特征数就是 $1$，样本数则是我们在前面准备的 $200$ 个点。为了让 PyTorch 能够正确地处理这个数据，我们需要将它转换为一个二维数组。可以使用 \texttt{x.reshape(-1, 1)} 来实现这个功能，其中 \texttt{-1} 表示自动推导这一维的大小，而 \texttt{1} 则表示第二维的大小为 $1$。

至于损失函数与优化器：使用均方误差作为损失函数，\texttt{torch.nn.MSELoss} 就是均方误差的函数，在下面的代码中写作 criterion，即准则。使用 \texttt{torch.optim.AdamW} 作为优化器\footnote{AdamW：是 Adam 优化器的一个变种，它是一个比较现代的优化器，通常比 SGD 更快收敛。}，下面的示例代码中随意地设置了学习率为 $0.001$。这个学习率一个常用的值。
\begin{minted}[bgcolor=gray!15, frame=single, framesep=10pt, baselinestretch=1.2]{python}
import torch.optim as optim

x = torch.from_numpy(x).float().reshape(-1, 1) # float64 -> float32
y = torch.from_numpy(y).float().reshape(-1, 1)
criterion = nn.MSELoss()
model = SimpleModel()
optimizer = optim.AdamW(model.parameters(), lr=0.001)
model.train()
for _ in range(2000):
    optimizer.zero_grad()
    output = model(x)
    loss = criterion(output, y)
    loss.backward()
    optimizer.step()
\end{minted}

迭代次数 2000 是简单实验得到的本任务在 0.001 学习率下较合理的训练轮数。其中 \texttt{optimizer.zero\_grad()} 一步用于清空上一步的梯度信息，\texttt{model(x)} 计算正向传播结果，\texttt{loss.backward()} 计算反向传播梯度信息，\texttt{optimizer.step()} 则是更新参数。

上面就是训练的核心过程。笔者将完整代码放在了 \texttt{examples/simple\_model.py} 中，并加入了进度条和实时输出损失的功能，方便观察训练过程及模型输出的变化。假设读者已经安装了 Python，在运行代码前，读者还需要安装 NumPy, PyTorch, matplotlib 和 tqdm 四个库。可以在命令行使用下面的命令来安装\footnote{提示：如果你在使用虚拟环境，一般不推荐在 base 环境中安装。此外，如果你计划使用 GPU 训练更复杂的模型，推荐安装支持 CUDA 的 PyTorch 版本，详情见前一章 GPU 相关部分。}：
\begin{minted}[bgcolor=gray!15, frame=single, framesep=10pt, baselinestretch=1.2]{bash}
pip install numpy torch matplotlib tqdm
\end{minted}

在笔者的电脑上运行代码后，训练过程中的模型输出会逐渐接近真实值，如图所示。图内，由浅到深的灰线表明从训练前期到后期的模型输出，蓝线表示真值。
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{img/simple_model_output.png}
\end{figure}

反映输出和预期 $f(x) = \sin 2x$ 差异的 loss 变化趋势则如下图所示
\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{img/simple_model_loss.png}
\end{figure}

\newpage

\subsection{如果目标并不明确呢？}