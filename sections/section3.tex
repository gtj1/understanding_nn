\section{为什么是神经网络}
% 从函数拟合的角度引入神经网络
\subsection{神经网络：一个大的函数}

相比于\textoverset{Neural Network}{神经网络}如何实现其功能，读者或许更想问的是：为什么要用神经网络？现有的神经网络为什么用了这些方法？对于这一类问题，一个统一但是现实的回答是：机器学习是高度以实用为导向的，实验显示这样做效果更好。在现实中，我们往往要解决各种各样的问题，人类开发者以手写每一行代码创造了各种各样的程序，自动化地解决了许多问题。但很多问题难以在有限的时间内找到确定性的解决方案，例如识别图片中的物体、识别语音、自然语言处理等等。它们有一个共同点：输入的信息量巨大、关系复杂，难以用确定的规则来描述。手动规定像素范围来判断物体类型，或用固定的规则来解析自然语言显然并不现实。因此人们自然要问有没有更加自动化、灵活、智能的方法来一劳永逸地解决这些问题。人工智能的概念就此提出，人们希望让机器自己学习知识来解决问题。

虽然目前人类仍然很难说摸到了\textoverset{Artificial General Intelligence}{通用人工智能}\footnote{通用人工智能：指能像人类一样解决各种通用的问题的人工智能。}的边界，但人工智能已然在许多问题上取得了巨大成就，走出了 20 世纪末 21 世纪初被大众认为是“伪科学”的寒冬。经过\uhref{https://arxiv.org/pdf/1512.03385}{深度残差网络}在图像识别的重大突破、\uhref{https://www.davidsilver.uk/wp-content/uploads/2020/03/unformatted_final_mastering_go.pdf}{AlphaGo}学会下围棋、\uhref{https://arxiv.org/pdf/1706.03762}{Transformer}在翻译比赛取得优异成绩并引来一波生成式模型的热潮等等，人工智能就这样走向了时代的焦点。但是如果要问：为什么它这么成功？最直接的回答仍是：It works.

除了一些基础的训练方法外，其它的结构构成、参数调整等等往往都是人们有一个想法，于是就这样展开了实验。部分实验成功了，就说明这个想法是对的，从而延伸出新的调节思路。如此循环往复，形成了现在的人工智能领域。因此就模型结构而言并没有非常统一的理论，有的只能说是经验法则。

不过我想可以对解决的方法做一个简单的分类。按照参数的数量，从参数复杂到参数简单可以画出一条轴。按照模型获取经验的方式，从模型完全编码了先验经验，到通过一些例子得到经验，再到持续在与环境的互动中获取经验，可以画出另一条轴。在这里我也试图并不严谨地画出了这样一个表格。
\begin{table}[H]
\centering
\begin{tabular}{c|cccc}
\toprule
\textbf{监督方式 $\backslash$ 参数量} & \textbf{超大参数量} & \textbf{大参数量} & \textbf{小参数量} & \textbf{经典模型} \\
\midrule
\textbf{持续互动} & PPO, A3C & DQN & Q-Learning & 经典控制\\
\textbf{输入/输出对} & ResNet, Transformer & 浅层CNN & 浅层MLP & SVM \\
\textbf{无监督} & GAN, SimCLR & -------- & K-Means, KNN & PCA, t-SNE \\
\bottomrule
\end{tabular}
\end{table}
读者看到的第一反应大抵是感到看不懂。不过我也并非想让读者先学完再来看这个表格，而是希望读者看到：解决问题的方法虽然多样，但仍可根据若干指标大致分类。表中的术语有的是模型结构，有的是算法，有的是思想，而右侧的一列甚至根本就不是机器学习，对机器学习有基本了解的读者或许会认为它们可比性存疑。诚然，模型之间并没有一个实际上的绝对界限，表中划分的位置也仅是凭借我的经验评价一个模型大多数时候处于什么位置，而非绝对的准则，但我认为这样的划分是有意义的，用一种更为建设性的话来说：意义就是在混乱的世界中建构起规律，用于解决问题。

大参数量的一侧——神经网络的领域，正是本书的主题。作为神经网络的引入，有必要从更高的角度来理解以神经网络为基础的模型目标是什么。小节标题已经足以表达内容核心：先不论内部结构如何，所谓的神经网络，无非也是一个函数。所谓函数，就必然要考虑到输入和输出，或者更准确地说，我们关心的就是怎么用计算机程序对给定的输入，得到我们想要的输出。无论是连续的数据，还是按照 0 或者 1 编码为向量的标签，输入和输出都可以变为向量。因此许多问题都可以归结为一个更加狭义的、数值拟合意义上的函数拟合问题。一个\textoverset{Encoder}{编码器}将原始输入变为向量这种易于处理的形式。而对于函数的原始输出，可以通过一个\textoverset{Decoder}{解码器}将数值构成的向量变为我们想要的输出。

而再向前看，在第一章中我们已经初步了解了以线性回归为代表的一类函数拟合问题。虽然这一问题从结构上相对简单，但是从这一情境中可以抽象出函数拟合的理念：有一些输入和输出的对应关系，我们要设计一个带参数的拟合模型，调整参数，让模型的输出尽可能接近我们预期的输出，接近程度则通过一个损失函数来衡量。

因此我会把模型抽象成五个要素：\textoverset{Input}{输入}、\textoverset{Output}{输出}、\textoverset{Architecture}{模型结构}、\textoverset{Loss Function}{损失函数}和\textoverset{Optimizer}{优化算法}。输入、模型架构和具体参数决定了输出如何计算，按照损失函数计算得到的损失指导模型调整具体参数，优化算法则决定了参数如何调整。当然这样的划分只是我自己的理解，而非理解神经网络的唯一方式。这里我不打算在概念之间玩文字游戏，把机器学习中的概念倒来倒去，变成一篇又臭又长，令人看完莫名其妙、不知所云、又对实践毫无益处的文章。因此我认为画一个图串起来是最直观的方式。
\begin{figure}[H]
\centering
\begin{tikzpicture}[>=Stealth]
    \fill [cyan, opacity=0.5] (0, 0) rectangle (1, 1);
    \node at (0.5, 0.5) {$x'$};
    \node [below] at (0.5, 0) {向量输入};
    \fill [green, opacity=0.5] (4, 0) rectangle (5, 1);
    \node at (4.5, 0.5) {$y'$};
    \node [below] at (4.5, 0) {向量输出};
    \draw [->] (1.2, 0.5) -- node [above] {模型} (3.8, 0.5);
    \fill [lightgray, opacity=0.5] (8, 0) rectangle (9, 1);
    \node at (8.5, 0.5) {$l$};
    \node [below] at (8.5, 0) {损失};
    \draw [->] (5.2, 0.5) -- node [above] {损失函数} (7.8, 0.5);
    \fill [yellow, opacity=0.5] (4, 2) rectangle (5, 3);
    \node at (4.5, 2.5) {$o$};
    \node [below] at (4.5, 2) {优化器};
    \draw [->] (8.5, 1.2) -- (8.5, 2.5) -- node [above] {优化信息} (5.2, 2.5);
    \draw [->] (3.8, 2.5) -- (2.5, 2.5) -- node [left] {参数更新} (2.5, 1.2);
    \draw [dashed] (-0.6, -0.8) rectangle (9.6, 3.4);
    \fill [cyan, opacity=0.5] (-4, 0) rectangle (-3, 1);
    \node at (-3.5, 0.5) {$x$};
    \node [below] at (-3.5, 0) {输入};
    \draw [->] (-2.8, 0.5) -- node [above] {编码器} (-0.2, 0.5);
    \fill [green, opacity=0.5] (4, -3) rectangle (5, -2);
    \node at (4.5, -2.5) {$y$};
    \node [below] at (4.5, -3) {输出};
    \draw [->] (4.5, -0.6) -- node [right] {解码器} (4.5, -1.8);
\end{tikzpicture}
\end{figure}

从输入到输出再到损失的过程通常称为\textoverset{Forward Propagation}{前向传播}，而从损失到参数的更新过程则称为\textoverset{Backward Propagation}{反向传播}。而这中间的模型结构常常由矩阵运算与一些\textoverset{Activation Function}{激活函数}构成的层组成。几乎可以说众多的神经网络中，只有这种传播的方式和网络的基本组成元素是相同的，如何从这些基本元素构建出好的模型则像是搭积木一样，各有各的搭法。

需要说明的是，现代的机器学习库 PyTorch 与 TensorFlow 都提供了完善的参数更新机制，使得用户不必自己实现优化算法。这可以说是非常简单易用，让用户可以聚焦模型的设计。不过我仍然会解读其中的原理，并试图说明设计网络结构与优化算法的人为什么要这么做。\footnote{其中的原理：实际上人类理解的神经网络工作原理与计算机实际运行的原理或许有很大的区别，人类对现在大部分网络的理解本质上都是经过实验后进行的归纳甚至是猜测，而非从数学上严格证明。神经网络的\textoverset{Interpretability}{可解释性}仍然是很大的问题，因此很多时候我们只知道这么做效果好，而不“真正地”理解为什么这么做效果好。}

\newpage

\subsection{激活函数与非线性}
\subsection{神经网络的训练}
\subsection{如果不知道目标，只知道回报呢？}