\section{为什么是神经网络}
% 从函数拟合的角度引入神经网络
\subsection{神经网络：一个大的函数}

相比于\textoverset{Neural Network}{神经网络}如何实现其功能，读者或许更想问的是：为什么要用神经网络？现有的神经网络为什么用了这些方法？对于这一类问题，一个现实的回答是：机器学习是高度以实用为导向的，实验显示这样做效果更好。在现实中，我们往往要解决各种各样的问题，人类开发者以手写每一行代码创造了各种各样的程序，自动化地解决了许多问题。但很多问题难以在有限的时间内找到确定性的解决方案，例如识别图片中的物体、识别语音、自然语言处理等等。它们有一个共同点：输入的信息量巨大、关系复杂，难以用确定的规则来描述。手动规定像素范围来判断物体类型，或用固定的规则来解析自然语言显然并不现实。因此人们自然要问有没有更加自动化、灵活、智能的方法来一劳永逸地解决这些问题。人工智能的概念就此提出，人们希望让机器自己学习知识来解决问题。

虽然目前人类仍然很难说摸到了\textoverset{Artificial General Intelligence}{通用人工智能}\footnote{通用人工智能：指能像人类一样解决各种通用的问题的人工智能。}的边界，但人工智能已然在许多问题上取得了巨大成就，走出了 20 世纪末 21 世纪初被大众认为是“伪科学”的寒冬。经过\uhref{https://arxiv.org/pdf/1512.03385}{深度残差网络}在图像识别的重大突破、\uhref{https://www.davidsilver.uk/wp-content/uploads/2020/03/unformatted_final_mastering_go.pdf}{AlphaGo}学会下围棋、\uhref{https://arxiv.org/pdf/1706.03762}{Transformer}在翻译比赛取得优异成绩并引来一波生成式模型的热潮等等，人工智能就这样走向了时代的焦点。但是如果要问：为什么它这么成功？最直接的回答仍是：It works.

除了一些基础的训练方法外，其它的结构构成、参数调整等等往往都是人们有一个想法，于是就这样展开了实验。部分实验成功了，就说明这个想法是对的，从而延伸出新的调节思路。如此循环往复，形成了现在的人工智能领域。因此就模型结构而言并没有非常完备的理论，有的只能说是经验法则。

不过我想可以对解决的方法做一个简单的分类。按照参数的数量，从参数复杂到参数简单可以画出一条轴。按照模型获取经验的方式，从模型完全编码了先验经验，到通过一些例子得到经验，再到持续在与环境的互动中获取经验，可以画出另一条轴。在这里我也试图并不严谨地画出了这样一个表格。
\begin{table}[H]
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        \textbf{监督方式 $\backslash$ 参数量} & \textbf{超大参数量}      & \textbf{大参数量} & \textbf{小参数量} & \textbf{经典模型} \\
        \midrule
        \textbf{持续互动}                  & PPO, A3C            & DQN           & Q-Learning    & 经典控制          \\
        \textbf{输入/输出对}                & ResNet, Transformer & 浅层CNN         & 浅层MLP         & SVM           \\
        \textbf{无监督}                   & GAN, SimCLR         & --------      & K-Means, KNN  & PCA, t-SNE    \\
        \bottomrule
    \end{tabular}
\end{table}
读者看到的第一反应大抵是感到看不懂。不过我也并非想让读者先学完再来看这个表格，而是希望读者看到：解决问题的方法虽然多样，但仍可根据若干指标大致分类。表中的术语有的是模型结构，有的是算法，有的是思想，而右侧的一列甚至根本就不是机器学习，对机器学习有基本了解的读者或许会认为它们可比性存疑。诚然，模型之间并没有一个实际上的绝对界限，表中划分的位置也仅是凭借我的经验评价一个模型大多数时候处于什么位置，而非绝对的准则，但我认为这样的划分是有意义的，用一种更为建设性的话来说：意义就是在混乱的世界中建构起规律，用于解决问题。

大参数量的一侧——神经网络的领域，正是本书的主题。作为神经网络的引入，有必要从更高的角度来理解以神经网络为基础的模型目标是什么。小节标题已经足以表达内容核心：先不论内部结构如何，所谓的神经网络，无非也是一个函数。所谓函数，就必然要考虑到输入和输出，或者更准确地说，我们关心的就是怎么用计算机程序对给定的输入，得到我们想要的输出。无论是连续的数据，还是按照 0 或者 1 编码为向量的标签，输入和输出都可以变为向量。因此许多问题都可以归结为一个更加狭义的、数值拟合意义上的函数拟合问题。一个\textoverset{Encoder}{编码器}将原始输入变为向量这种易于处理的形式。而对于函数的原始输出，可以通过一个\textoverset{Decoder}{解码器}将数值构成的向量变为我们想要的输出。

而再向前看，在第一章中我们已经初步了解了以线性回归为代表的一类函数拟合问题。虽然这一问题从结构上相对简单，但是从这一情境中可以抽象出函数拟合的理念：有一些输入和输出的对应关系，我们要设计一个带参数的拟合模型，调整参数，让模型的输出尽可能接近我们预期的输出，接近程度则通过一个损失函数来衡量。

因此我会把模型抽象成五个要素：\textoverset{Input}{输入}、\textoverset{Output}{输出}、\textoverset{Architecture}{模型结构}、\textoverset{Loss Function}{损失函数}和\textoverset{Optimizer}{优化算法}。输入、模型架构和具体参数决定了输出如何计算，按照损失函数计算得到的损失指导模型调整具体参数，优化算法则决定了参数如何调整。当然这样的划分只是我自己的理解，而非理解神经网络的唯一方式。这里我不打算在概念之间玩文字游戏，把机器学习中的概念倒来倒去，变成一篇又臭又长，令人看完莫名其妙、不知所云、又对实践毫无益处的文章。因此我认为画一个图串起来是最直观的方式。
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[>=Stealth]
        \fill [cyan, opacity=0.5] (0, 0) rectangle (1, 1);
        \node at (0.5, 0.5) {$x'$};
        \node [below] at (0.5, 0) {向量输入};
        \fill [green, opacity=0.5] (4, 0) rectangle (5, 1);
        \node at (4.5, 0.5) {$y'$};
        \node [below] at (4.5, 0) {向量输出};
        \draw [->] (1.2, 0.5) -- node [above] {模型} (3.8, 0.5);
        \fill [lightgray, opacity=0.5] (8, 0) rectangle (9, 1);
        \node at (8.5, 0.5) {$l$};
        \node [below] at (8.5, 0) {损失};
        \draw [->] (5.2, 0.5) -- node [above] {损失函数} (7.8, 0.5);
        \fill [yellow, opacity=0.5] (4, 2) rectangle (5, 3);
        \node at (4.5, 2.5) {$o$};
        \node [below] at (4.5, 2) {优化器};
        \draw [->] (8.5, 1.2) -- (8.5, 2.5) -- node [above] {优化信息} (5.2, 2.5);
        \draw [->] (3.8, 2.5) -- (2.5, 2.5) -- node [left] {参数更新} (2.5, 1.2);
        \draw [dashed] (-0.6, -0.8) rectangle (9.6, 3.4);
        \fill [cyan, opacity=0.5] (-4, 0) rectangle (-3, 1);
        \node at (-3.5, 0.5) {$x$};
        \node [below] at (-3.5, 0) {输入};
        \draw [->] (-2.8, 0.5) -- node [above] {编码器} (-0.2, 0.5);
        \fill [green, opacity=0.5] (4, -3) rectangle (5, -2);
        \node at (4.5, -2.5) {$y$};
        \node [below] at (4.5, -3) {输出};
        \draw [->] (4.5, -0.6) -- node [right] {解码器} (4.5, -1.8);
    \end{tikzpicture}
\end{figure}

从输入到输出再到损失的过程通常称为\textoverset{Forward Propagation}{正向传播}，而从损失到参数的更新过程则称为\textoverset{Backward Propagation}{反向传播}。而这中间的模型结构常常由矩阵运算与一些\textoverset{Activation Function}{激活函数}构成的层组成。几乎可以说众多的神经网络中，只有这种传播的方式和网络的基本组成元素是相同的，如何从这些基本元素构建出好的模型则像是搭积木一样，各有各的搭法。

在这里我想简单讲讲使用矩阵运算的原因。在第一章中我们已经简单地学习了矩阵运算的基本知识，它本质上是正比例函数在向量空间中的推广，只是 $y=kx$ 中的斜率变成了一个个从输入 $x_j$ 连接到输出 $y_i$ 的权重 $w_{ij}$。从行看过去，它反映了输出的每个分量（或称为特征）是如何由输入的每个分量线性组合而成的。而从列看过去，它表明了输入的每个分量是如何影响输出的。就像一次函数有一个常数项一样，矩阵运算也有一个偏置项 $b$，运算的总体结构是 $y = wx + b$。从代数上看，它运算简单\footnote{简单：仅由简单的四则运算组成，现代 GPU 也常常提供高效的矩阵运算加速。}，而从分析上看，它的输出变化光滑，容易求导\footnote{容易求导：记住这一点，这对后续反向传播等算法的实现至关重要。如果在离散的空间中操作，例如使用阶跃函数或者逻辑门，便无法借助导数来进行参数更新。}。

下一节中我们会引入激活函数，暂且不论它们的具体形式如何，它们也是一些非常简单的运算。或许读者会有疑问，这样一些简单的运算，真的有能力让神经网络胜任复杂的任务吗？\uhref{https://www.zhihu.com/question/594296903/answer/2979485641}{万能逼近定理}\footnote{万能逼近定理：指出足够大的神经网络可以以任意精度在给定范围内拟合任意的复杂函数。}虽然在理论上告诉我们它可以，却需要假定足够多的神经元，并不令人安心。所幸无数的实验表明：可以，在人类能实现的范围内，量变也可以引起质变，将一系列简单的单元堆叠起来，便可以形成复杂的行为。Philip W. Anderson\footnote{Philip W. Anderson：美国物理学家，1977 年诺贝尔物理学奖获得者。}曾说过：“More is different”，他的原意指的是物理学中，微观的规律并不能简单地推导出宏观的规律，整个系统可以表现的与单个元素完全不同，因此微观和宏观需要不同的理论来描述。但这里我们不妨借用一下，同样地认识到大量简单的数学函数也可以产生复杂的行为。在神经网络的\textoverset{Emergent}{涌现}\footnote{涌现：即增大参数量带来性能突然提升的现象。}现象中，这一事实不断地被验证。就像婴儿可以通过教育称为适应社会的成年人一样，适当的算法和足够的训练数据的确可以让神经网络学会知识。

需要说明的是，现代的机器学习库 PyTorch 与 TensorFlow 都提供了完善的参数更新机制，使得用户不必自己实现优化算法。这可以说是非常简单易用，让用户可以聚焦模型的设计。不过我仍然会解读其中的原理，并试图说明设计网络结构与优化算法的人为什么要这么做。\footnote{其中的原理：实际上人类理解的神经网络工作原理与计算机实际运行的原理或许有很大的区别，人类对现在大部分网络的理解本质上都是经过实验后进行的归纳甚至是猜测，而非从数学上严格证明。神经网络的\textoverset{Interpretability}{可解释性}仍然是很大的问题，因此很多时候人们只知道怎么样做效果好，而不“真正地”理解为什么这么做效果好，有时也被人调侃为“新时代的炼金术”。虽然有许多相关的\uhref{https://www.zhihu.com/question/320688440}{解释}来帮助人们了解神经网络中发生什么，但学界内仍然没有形成一套系统的理论。}


\begin{tcolorbox}[myrecommendbox, title=推荐阅读, breakable=false]
    \begin{itemize}
        \item 这篇文章讲述了神经网络的起源：\\
              \textit{如何简单形象又有趣地讲解神经网络是什么？ - 佳人李大花的回答 - 知乎}\\
              \url{https://www.zhihu.com/question/22553761/answer/3359939138}
        \item 读者或许会好奇所谓的万能逼近定理需要是如何能逼近给定函数的，这篇回答的解释不错：\\
              \textit{神经网络的万能逼近定理已经发展到什么地步了？ - 牛油果博士的回答 - 知乎}\\
              \url{https://www.zhihu.com/question/347654789/answer/1534866932}
        \item 这一问题下有关于神经网络“涌现”出新的现象的讨论，对其机理感兴趣的读者也可以想想背后的原因：\\
              \textit{如果神经网络规模足够大，会产生智能吗？ - 知乎}\\
              \url{https://www.zhihu.com/question/408690594}
    \end{itemize}
\end{tcolorbox}

\newpage

\subsection{激活函数与非线性}

将 $y=wx+b$ 作为一次函数的类比应该足以说明它是很简单的一类函数。但是正如一次函数的复合 $ y = w_2(w_1x+b_1)+b_2 = w_2w_1x + (w_2b_1+b_2) $
仍然是一次函数一样，如果仅仅沉浸在矩阵运算中，我们便永远无法表达那些复杂的函数。举个最简单的例子，我们甚至无法表示输入的绝对值 $y=|x|$。因此我们需要在模型的结构中加点“非线性”，让它不仅仅局限于简单的加减乘除，专业的说法称之为\textoverset{Activation Function}{激活函数}。激活函数直接作用在每个特征上，而且函数本身通常是固定的\footnote{通常是固定的：在一些模型，例如使用可变样条函数的\uhref{https://arxiv.org/pdf/2404.19756}{KAN}中，激活函数也是可学习的，而且各个元素上的效果可能不同，但是可变的激活函数总体来说并不常见。}，且总体通常呈现递增的趋势。

所谓逐元素作用，也就是说，与矩阵对特征进行组合不同，激活函数对各个分量的操作是独立的。其输入是一个向量，输出也是一个同样维数的向量。如果选定了激活函数 $f:\mathbb{R}\to\mathbb{R}$，输入为 $x = [x_1, x_2, \cdots, x_n]$，则输出为 $y = [f(x_1), f(x_2), \cdots, f(x_n)]$。

现在使用最多的激活函数是\textoverset{Rectified Linear Unit}{线性整流函数}(ReLU)，虽然相对于其它激活函数，诸如 Sigmoid、tanh 等等，ReLU 其实算是晚辈，但是在关于激活函数的讨论中，\uhref{https://proceedings.mlr.press/v15/glorot1a/glorot1a.pdf}{有研究}表明它的效果更好，而后\uhref{https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}{AlexNet} 的成功更让它成为了主流的激活函数。虽然失去了早期其它激活函数的仿生背景，但它好用，而且非常简单。它的定义是：
\[
    \text{ReLU}(x) = \max\{0, x\} = \begin{cases}
        x, & x \geq 0 \\
        0, & x < 0
    \end{cases}
\]

图像是这样的：
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[>=Stealth]
        \draw [->] (-2, 0) -- (2, 0) node [below] {$x$};
        \draw [->] (0, -1) -- (0, 2) node [left] {$y$};
        \draw [domain=-2:0, smooth, variable=\x, blue, thick] plot ({\x}, 0);
        \draw [domain=0:2, smooth, variable=\x, blue, thick] plot ({\x}, {\x});
    \end{tikzpicture}
    \caption{ReLU 函数图像}
\end{figure}

举一个例子就可以看出逐元素作用的含义。例如有输入向量 $x = [1, -2, 3]$，那么它经过 ReLU 激活函数的输出为 $y = [1, 0, 3]$。正的部分被保留了，而负的部分被置为 0。正如电路中的半波\textoverset{Rectifier}{整流器}一样，把负值截断了。

而它的导数也非常简单：
\[
    \frac{\mathrm{d}}{\mathrm{d} x}\text{ReLU}(x) = \begin{cases}
        1, & x > 0 \\
        0, & x < 0
    \end{cases}
\]

读者或许会关心，那 0 这一点不可导要怎么办？其实关系不大，因为一个小数几乎不可能\footnote{几乎不可能：在最常用的 32 位浮点数中，一个数恰好取到 0 的概率大概在 $10^{-9}$ 量级。虽然在 FP8 或者 FP16 量化中恰好取到 0 的概率更大，然而实践中这单个不可导点几乎不会对训练产生影响。}在训练中恰好落在 0 上。即使有，也可以任意地选择一个值，例如 0 或者 1\footnote{0 处的导数：PyTorch 通常选择 0}。有了这样的激活函数，函数的表达能力大大就增强了。以目标 $|x|$ 为例，假设有输入 $x$，只需两个 ReLU 函数值的和就可以表示它：
\[
    |x| = \text{ReLU}(x) + \text{ReLU}(-x) = \max\{0, x\} + \max\{0, -x\}
\]

初看可能会觉得这样的表达方式有点多此一举，像是为了 $|x|$ 这盘醋专门包的饺子。但是别急，让我们把它拆解成神经网络的结构，更加结构化地看待。

最初的输入是 $x$，它先经过一个线性的函数得到 $[x, -x]$，再经过 ReLU 函数得到中间的向量 $x^{(1)} = (\max\{0, x\}, \max\{0, -x\})$，而这使用一个线性函数就可以得到 $y = |x|$。

写成矩阵的形式就有
\[
    w_1 = \begin{bmatrix}
        1 \\ -1
    \end{bmatrix}, b_1 = \begin{bmatrix}
        0 \\ 0
    \end{bmatrix}, w_2 = \begin{bmatrix}
        1 & 1
    \end{bmatrix}, b_2 = 0
\]

遂可以写成 $y = w_2 \,\text{ReLU}(w_1x + b_1) + b_2$。我认为，把这件事作为一个 toy case\footnote{toy case：玩具案例，指的是一个简单的例子，用于说明某个概念或方法。}想明白多少可以帮助理解神经网络。把矩阵的每个权重都画出来就是这样了：
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[>=Stealth, scale=1.5]
        \node [circle, fill=white, draw, minimum size=1.2cm] (x0) at (0, 0) {$x$};
        \node [circle, fill=white, draw, minimum size=1.2cm] (x11) at (2, 1) {$x_1^{(1)}$};
        \node [circle, fill=white, draw, minimum size=1.2cm] (x12) at (2, -1) {$x_2^{(1)}$};
        \node [circle, fill=white, draw, minimum size=1.2cm] (y) at (4, 0) {$|x|$};
        \draw [->, red] (x0) -- node [above left] {$1$} (x11);
        \draw [->, blue] (x0) -- node [below left] {$-1$} (x12);
        \draw [->, red] (x11) -- node [above right] {$1$} (y);
        \draw [->, red] (x12) -- node [below right] {$1$} (y);
        \draw [dashed, thin] (1.4, -1.8) rectangle (2.6, 1.8);
        \node [above] at (2, 1.8) {中间层};
        \node at (2, 0.4) {ReLU};
        \node[lightgray] at (2, 1.6) {$+0$};
        \node[lightgray] at (2, -0.4) {$+0$};
        \node[lightgray] at (4, 0.6) {$+0$};
        \node at (2, -1.6) {ReLU};
    \end{tikzpicture}
    \caption{神经网络表示 $|x|$}
\end{figure}

这看起来很简单，读者可能想问：还能不能再给力一点，看看更复杂的情况呢？当然可以。不过在看之前先抛出两个思考题：
\begin{enumerate}
    \item 试着用线性函数和 ReLU 函数表示 $y = \max\{x_1, x_2\}$，并画出它的神经网络结构图。
    \item 线性函数和 ReLU 的组合\textbf{不能}表示什么函数呢？
\end{enumerate}

在思考这个问题时，读者可以先回顾 ReLU 的性质：它的作用是将负数截断为 0，而正数保持不变。那么，能否通过适当的线性变换和 ReLU 来分辨两个数的大小呢？实际上我们可以很容易地发现
\[
    \max\{x_1, x_2\} = x_1 + \text{ReLU}(x_2 - x_1)
\]

但是这个答案并不够好，如果直接把它画成神经网络结构图，就会发现它的结构看起来像是这样：
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[>=Stealth, scale=1.5]
        \node [circle, fill=white, draw, minimum size=1.2cm] (x01) at (0, 1) {$x_1$};
        \node [circle, fill=white, draw, minimum size=1.2cm] (x02) at (0, -1) {$x_2$};
        \node [circle, fill=white, draw, minimum size=1.2cm] (x11) at (2, -1) {$?$};
        \node [circle, fill=white, draw, minimum size=1.2cm] (y) at (4, 0) {$y$};
        \draw [->, blue] (x01) -- node [above right] {$-1$} (x11);
        \draw [->, red] (x02) -- node [below] {$1$} (x11);
        \draw [->, red] (x11) -- node [below right] {$1$} (y);
        \draw [->, red] (x01) -- node [above right] {$1$} (y);
        \node at (2, -1.6) {ReLU};
        \node[lightgray] at (2, -0.4) {$+0$};
        \node[lightgray] at (4, 0.6) {$+0$};
    \end{tikzpicture}
    \caption{神经网络表示 $\max\{x_1, x_2\}$ 的一种方法}
\end{figure}

变量 $x_1$ 没有经过统一的隐藏层，而是跳过中间，直接连接到了输出层。显然就不能用一致的 $\text{ReLU}(wx+b)$ 的形式来表示了，而是要单独开一个通道来处理。而我们使用神经网络的目的本来就是用一致的方式来处理所有的输入，所以这样的表示方式并不优雅\footnote{并不优雅：与之对比，在深层神经网络网络中通常会引入看起来有些像这里的\textoverset{Shortcut Connection}{跳连接}结构，由此引出\textoverset{Residual Network}{残差网络}的概念。它看起来有些像这里的跳过中间层的结构，但那里是系统性地引入这样的连接，而不是这样对某个分量单独处理。}。

不过使用一点小小的技巧，可以把 $x_1$ 本身写成 $x_1 = \text{ReLU}(x_1) - \text{ReLU}(-x_1)$，这样一来就可以把它写成带有三个中间变量的一个网络结构了。把
\[
    \max\{x_1, x_2\} = \text{ReLU}(x_1) - \text{ReLU}(-x_1) + \text{ReLU}(x_2 - x_1)
\]

这一式子中的三个分量提出来，便可以得到
\[
    \begin{aligned}
        x_1^{(1)} & = \text{ReLU}(x_1\,{\color{lightgray}+\,0x_2})  \\
        x_2^{(1)} & = \text{ReLU}(-x_1\,{\color{lightgray}+\,0x_2}) \\
        x_3^{(1)} & = \text{ReLU}(- x_1 + x_2)                      \\
        y         & = x_1^{(1)} - x_2^{(1)} + x_3^{(1)}
    \end{aligned}
\]

偏置 $b$ 仍然为 $0$，读者可以自行试着写出对应的权重矩阵 $w$ ，按照新的写法重新绘制，这时结构图就会变成这样：
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[>=Stealth, scale=1.5]
        \node [circle, fill=white, draw, minimum size=1.2cm] (x01) at (-1, 1.2) {$x_1$};
        \node [circle, fill=white, draw, minimum size=1.2cm] (x02) at (-1, -1.2) {$x_2$};
        \node [circle, fill=white, draw, minimum size=1.2cm] (x11) at (2, 2) {$x_1^{(1)}$};
        \node [circle, fill=white, draw, minimum size=1.2cm] (x12) at (2, 0) {$x_2^{(1)}$};
        \node [circle, fill=white, draw, minimum size=1.2cm] (x13) at (2, -2) {$x_3^{(1)}$};
        \node [circle, fill=white, draw, minimum size=1.2cm] (y) at (5, 0) {$y$};
        \draw [->, lightgray] (x02) -- node [below right] {$0$} (x11);
        \draw [->, lightgray] (x02) -- node [below right] {$0$} (x12);
        \draw [->, red] (x01) -- node [above left] {$1$} (x11);
        \draw [->, blue] (x01) -- node [above right] {$-1$} (x12);
        \draw [->, blue] (x01) -- node [above right] {$-1$} (x13);
        \draw [->, red] (x02) -- node [below left] {$1$} (x13);
        \draw [->, red] (x11) -- node [above right] {$1$} (y);
        \draw [->, blue] (x12) -- node [above] {$-1$} (y);
        \draw [->, red] (x13) -- node [below right] {$1$} (y);
        \node [lightgray] at (2, 2.6) {$+0$};
        \node [lightgray] at (2, 0.6) {$+0$};
        \node [lightgray] at (2, -1.4) {$+0$};
        \node [below] at (2, 1.6) {ReLU};
        \node [below] at (2, -0.4) {ReLU};
        \node [below] at (2, -2.4) {ReLU};
        \draw [dashed, thin] (1.4, -2.8) rectangle (2.6, 2.8);
        \node [above] at (2, 2.8) {中间层};
    \end{tikzpicture}
    \caption{神经网络表示 $\max\{x_1, x_2\}$ 的另一种方法}
\end{figure}

虽然中间的神经元多了一些，但是它的结构看起来就统一而且整齐得多了。或许有人会有疑问，这里连的线变多了，不是把事情复杂化了吗？实际上并没有，恰恰相反，把它整齐地写出来才有利于算法的数值优化。

一个有趣的事实是，如果把 True 和 False 分别视作 1 和 0，那么最多两层的网络就可以表示任意的逻辑函数。例如
\[
    \begin{aligned}
        x_1 \; \text{and} \; x_2 & = \text{ReLU}(x_1 + x_2 - 1)                      \\
        x_1 \; \text{or} \; x_2  & = \text{ReLU}(x_1) + \text{ReLU}(x_2 - x_1)       \\
        x_1 \; \text{xor} \; x_2 & = \text{ReLU}(x_1 - x_2) + \text{ReLU}(x_2 - x_1)
    \end{aligned}
\]

这至少表明逻辑可以在一定程度上编码进神经网络中，用一些可调的权重来模拟逻辑门\footnote{用权重模拟逻辑门：这里仅说明它可以，不过这么做太奢侈了，很浪费储存和计算资源。}，因此从这一特例来看，求特征的交集、并集的操作确实可以自然地以权重的方式编码到网络的运算中。

推而广之，不难发现 ReLU 本质上完成的是将函数分段的操作。调整权重就可以做到在不同的区域选择不同的段，从而给出不同的表达式。虽然它在每一根区域内仍然是线性的，但却可以通过一些点上的弯折来实现非线性，表达能力比单纯的线性函数大大提高。这样的函数在数学上称为\textoverset{Piecewise Linear Function}{分段线性函数}，如我们所见，ReLU 函数就提供了一种通用的方式来实现分段线性函数，从而将关于“分类”的信息编码到网络中。

那么它不能表示什么函数呢？由于其分段线性的特性，不难证明它无法完全精准地表示光滑的曲线，例如 $y = x^2$。而且可以证明，对于任何一个分段线性函数 $f(x)$，都可以找到一个常数 $c$ 使对于 $\|x\|$ 足够大的时候，$f(x) \leq c\|x\|$。从而增长速度有限，无法表示指数函数或者高次的多项式函数。

这确实体现出了它的局限性，但这必然是它的弱点吗？并不一定。一方面，虽然它本身无法\textbf{精准地表示}光滑的函数，但是只要给定一个自变量的区间，在这样的函数堆叠多层之后总是可以调整参数，做到\textbf{良好地近似}给定的函数。事实上，只需四段就可以在区间 $[-1, 1]$ 上用如下的分段线性函数来相当好地近似 $x^2$ 了，例如下面的分段线性函数 $f(x)$：
\[
    f(x) = 2\text{ReLU}(x-1) + 2\text{ReLU}(x) + 2\text{ReLU}(-x) + 2\text{ReLU}(-x-1) - 0.04
\]

图像是这样的：
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[>=Stealth]
        \begin{axis}[
                axis lines=middle,
                axis equal,
                xlabel={$x$}, ylabel={$y$},
                grid=major,
                legend pos=north west
            ]
            \addplot[blue, thick, domain=-1:1, smooth] {x^2};
            \addlegendentry{$x^2$}

            \addplot[red, thick] coordinates {(-1, 0.96) (-0.5, 0.21) (0, -0.04) (0.5, 0.21) (1, 0.96)};
            \addlegendentry{$f(x)$}
        \end{axis}
    \end{tikzpicture}
    \caption{分段线性函数近似光滑函数}
\end{figure}

另一方面，虽然它的输出会被输入大小的一个常数倍所控制，但在很大程度上，这也避免了在第一章中多项式拟合的数值爆炸问题。此外，这提醒我们应当将模型的输入输出控制在一个范围之内。遵循这些原则，ReLU 网络的表达能力已经足够强大，能解决大多数实际问题。尽管仍有一些细节需要注意，但这并不影响我们对其整体能力的理解。

另外再提一嘴其它的激活函数。Sigmoid 函数\footnote{Sigmoid 函数：Sigmoid 来源于拉丁语，得名于其类似小写字母 sigma 变体 $\varsigma$ 的形状。}是一个 S 型函数，定义为
\[
    \text{Sigmoid}(x) = \frac{1}{1 + e^{-x}}
\]

输出随输入变化的图像是这样的，可见它把输入压缩到了 $[0, 1]$ 的范围内：
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[>=Stealth]
        \begin{axis}[
                axis lines=middle,
                axis equal,
                xlabel={$x$}, ylabel={$y$},
                grid=major,
                legend pos=north west
            ]
            \addplot[blue, thick, domain=-2:2, samples=100] {1/(1+exp(-x))};
            \addlegendentry{Sigmoid($x$)}
        \end{axis}
    \end{tikzpicture}
    \caption{Sigmoid 函数图像}
\end{figure}

tanh 函数是双曲正切函数，其定义为
\[
    \text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]

它的图像和 Sigmoid 函数很类似，只是经过了一个伸缩和平移，输出范围是 $[-1, 1]$：
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[>=Stealth]
        \begin{axis}[
                axis lines=middle,
                axis equal,
                xlabel={$x$}, ylabel={$y$},
                grid=major,
                legend pos=north west
            ]
            \addplot[blue, thick, domain=-2:2, samples=100] {tanh(x)};
            \addlegendentry{tanh($x$)}
        \end{axis}
    \end{tikzpicture}
    \caption{tanh 函数图像}
\end{figure}

早期的研究中，它们出现在许多生物学的研究中，可以描述生物神经元的激活或者极化程度，于是人工神经网络出于仿生的考虑也使用了它们。然而它们在两端很小的导数也为优化带来了许多麻烦，导致了\textoverset{Vanishing Gradient}{梯度消失}\footnote{梯度消失：是指在深度神经网络中，由于输出随输入的变化过于小，导致信息无法有效地从输出传回输入，从而使得网络难以优化学习的现象。关于梯度的进一步介绍会在后文给出，此处可以简单理解为信息回传受阻。}的问题，后来逐渐被 ReLU 函数取代，仅在特定层要将输出限制在给定范围内时才使用。虽然近期有\uhref{https://arxiv.org/pdf/2503.10622}{研究}指出现在的优化器有能力克服这个问题，即使使用 tanh 仍然可以正常地优化，不过这也仅是一个理论上的结果，实际应用中通常认为它们仍然不如 ReLU 函数好用。从此也能看见人工智能的发展并非一帆风顺，仿生不是唯一的出路，人工的神经网络的发展和对其规律的认识必然要走过曲折的探索，才能形成一套独特而成熟的方法论。

不过 ReLU 在 $x<0$ 的区域也存在斜率为 $0$ 导致梯度消失的问题，为此人们还提出了一些变体，例如 Leaky ReLU 函数，它在 $x<0$ 的区域也有一个小的斜率，定义为
\[
    \text{Leaky ReLU}(x) = \begin{cases}
        x,        & x \geq 0 \\
        \alpha x, & x < 0
    \end{cases}
\]

上式中 $\alpha$ 是一个小的常数，通常取 $0.01$，它同样简单易于计算。还有一些较为复杂的变体，包括\textoverset{Gaussian Error Linear Unit}{高斯误差线性单元}(GELU)，\textoverset{Exponential Linear Unit}{指数线性单元}(ELU) 等，都在一定程度上克服了 ReLU 导数为 $0$ 导致信息传播不畅的问题。不过这些都属于工程上的细节问题，读者可以在需要的时候再去了解。

由此我们更加具体化地认识到了神经网络的工作原理：它的基本单元由线性函数与激活函数交替组成。每一层都可以看作是对输入进行线性组合，然后通过激活函数进行非线性变换以实现更复杂的表达能力。这让网络以一种统一的方式来处理输入数据，并有能力通过调整参数拟合复杂的输出。

\begin{tcolorbox}[myrecommendbox, title=推荐阅读, breakable=false]
    \begin{itemize}
        \item 文中为了简单起见，只是简单介绍了 ReLU 激活函数，关于更多激活函数的定义与性质可以看这篇笔记：\\
              \textit{深度学习随笔——激活函数(Sigmoid、Tanh、ReLU、Leaky ReLU、PReLU、RReLU、ELU、SELU、Maxout、Softmax、Swish、Softplus) - Lu1zero9的文章 - 知乎}\\
              \url{https://zhuanlan.zhihu.com/p/585276457}
        \item 形成图形化的直觉许多时候相当重要，这篇文章就给出了一个图形解释：\\
              \textit{形象的解释神经网络激活函数的作用是什么？ - 忆臻的文章 - 知乎}\\
              \url{https://zhuanlan.zhihu.com/p/25279356}
    \end{itemize}
\end{tcolorbox}

\newpage

\subsection{神经网络的训练}

有了前面的模块，我们已经可以搭建起简单的神经网络了，理论上通过“合适的”参数就可以拟合任意的函数了，“合适的”参数从何而来呢？回顾第一章中，线性拟合问题可以完全通过解析方法求解得到最优的参数，然而神经网络网络却是一个复杂的非线性函数，根本不可能对各种情况分段讨论给出解析解。这是否意味着我们就无能为力了呢？当然不是。通过一些手段可以让信息从数据定向地“流向”模型的参数，通过一套优化算法来调整参数，使得模型的输出尽可能地接近目标，这一过程就叫做\textoverset{Training}{训练}，这一调整的过程使用的各种优化方法大多是基于\textoverset{Gradient Descent}{梯度下降法}\footnote{Gradient 词源说明：gradi 是一个拉丁语词根，意为行走，与 “步行”相关。与 progress, regress 等词中的 “gress” 是同一词根。因此从词根来看其实可以理解为在步行中下降。}的。

正如第一章中所见，拟合的好不好需要量化为损失的数值，来定量分析大小如何。而在机器学习上，我们就是借助这一可量化的评判标准——利用梯度下降算法给出了往更好的方向前进的一步。以\uhref{https://www.bilibili.com/video/BV1Ux411j7ri}{3Blue1Brown的视频}为代表的一系列科普教程中都有对这一算法的良好讲解。不过此处我仍然通过下山这个最经典的比喻来试图说明这一算法的原理。

前面我提到过，每层函数的函数需要可导，可导性就在此处显得尤为重要。即使不知道函数的总体行为，但是我们仍然可以通过导数对当前位置附近的情况形成大致的感知。就像在山上行走时，想快速下山的人并不需要知道整座山的形状，只要知道当前的坡度，就可以沿着向下的方向走。不过这一步不能走太大，在机器学习中，走太大了可能会走到山的另一边去，导数仅为优化提供了局部的信息。

在一元函数中，导数的正负可以直接指出当前的坡度是向上还是向下，局部的近似意味着对于很小的 $\Delta x$ 可以写出
\[
    \Delta y \approx f'(x_0)\Delta x
\]

看起来像是这样：
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[>=Stealth, xscale=2]
        \draw [thin, ->] (-1, 0) -- (2, 0) node [below] {$x$};
        % \draw [thin, ->] (0, -0.5) -- (0, 3.5) node [left] {$f(x)$};
        \draw [domain=-0.7:1.5, smooth, variable=\x, blue, thick] plot ({\x}, {\x*\x+1}) node [above] {$f(x)$};
        \draw [domain=-0.3:1.3, smooth, variable=\x, red, thick] plot ({\x}, {\x-0.25+1});
        \draw [fill=lightgray] (0.5, 1.25) -- node [below] {$\Delta x$} (1.0, 1.25) -- node [right] {$f'(x_0) \Delta x$} (1.0, 1.75) -- cycle;
        \draw [dashed, thin] (0.5, 1.25) -- (0.5, 0) node [below] {$x_0$};
    \end{tikzpicture}
    \caption{一元函数的导数近似}
\end{figure}

导数反映的是切线的斜率，也就是函数在局部的变化率信息。为了让 $f(x)$ 变小，令 $\Delta x$ 的方向与 $f'(x)$ 相反就可以了。也就是说，可以令
\[
    \Delta x = -\eta f'(x)
\]

这里的 $\eta$ 决定了一步要走多大，称为\textoverset{Learning Rate}{学习率}。如果 $\eta$ 太小，可能会走得很慢；如果 $\eta$ 太大，可能会走过头。一步步地往下走，我们的位置就像是一个小球，从斜坡上滚下去，最终停留在一个极小值\footnote{极小值：不过这个极小值可能只是山谷中的一个盆地，而非全局的最低点。}。

不过我们的神经网络毕竟有很多的参数要在同时调整，你或许想问，我们能不能固定其它的参数，只调整一个参数，把它变成一元函数来处理呢？可以是确实可以，这可以把它变为了一元函数的情况，只是这种方法太低效了，更高效且优雅的做法是联合优化它们，同时调整所有的参数。听起来很合理，但是这初听起来似乎有点玄学，毕竟我们并不知道所有的参数之间如何作用的，庞大的网络让它们之间的关系变得复杂而难以捉摸。因此我认为需要在此处引入一些多元微积分的知识，来把求导操作推广到多维，得到所谓的梯度与\textoverset{Backpropagation}{反向传播}算法。

在一元情况下，导数是这样定义的
\[
    f'(x_0) = \lim_{\Delta x\to 0} \frac{f(x_0 + \Delta x) - f(x_0)}{\Delta x}
\]

但是在输入 $x$ 是向量的情况下，数学运算并不允许除以一个向量，那还能如何求导呢？应回看一下最初的动机：求出导数的目的是为了让它在局部能用简单的线性函数来近似。如果使用 $\mathrm{d}x$ 表示一个输入的变化量，用 $\mathrm{d}y$ 表示我们对输出结果变化量的估计。在一元的情况下这个式子可以简单地写成
\[
    \mathrm{d}y = f'(x_0) \mathrm{d}x
\]

而在多元情况下，应当估计每个分量的变化量如何影响输出，再把这些线性的分量叠起来，从而有线性近似
\[
    \mathrm{d}y = a_1 \mathrm{d}x_1 + a_2 \mathrm{d}x_2 + \cdots + a_n \mathrm{d}x_n
\]

而线性近似意味着在 $\mathrm{d}x$ 足够小的情况下，这一近似给出的估计 $\mathrm{d}y$ 与真实的变化量 $\Delta y$ 之间的差距是可以忽略的，也就是说
\[
    \lim_{\mathrm{d}x\to 0} \frac{\Delta y - \mathrm{d}y}{\|\mathrm{d}x\|} = 0
\]

不过上面的极限在实际应用中可以假设成立，在足够好的可导的假设下，需要关心的只是如何算出这些系数 $a_i$。对于标量函数 $f$ 和向量输入 $x$，这样的一些系数所构成的向量就叫做\textoverset{Gradient}{梯度}，$f$ 对 $x$ 的梯度在 $x_0$ 处的值常记作 $\nabla_x f(x_0)$，而从根本上，它最主要的作用就是提供了线性近似 $\mathrm{d}y = \nabla_x f(x_0) \cdot \mathrm{d}x$ 的系数\footnote{说明：这里的 $\cdot$ 是向量点积，表明逐分量相乘并求和}。为了沿着网络层层回溯求出梯度，需要再引入链式法则。

设想如果 $y$ 本身并不是 $x$ 的函数，而是经过两步得到的 $x^{(1)} = f_1(x)$ 和 $y = f_2(x^{(1)})$，那么应该怎么求导呢？这一过程需要分步进行。如果从一个逐元素的视角看来，实际上是先使用 $\mathrm{d}x^{(1)}$ 来估计 $y$ 的变化量，然后再借助 $\mathrm{d}y$ 与 $\mathrm{d}x^{(1)}$ 之间的关系来估计 $\mathrm{d}x$ 对 $y$ 的影响。更为具体地说：如果能写出
\[
    \mathrm{d}y = a_1 \mathrm{d}x^{(1)}_1 + a_2 \mathrm{d}x^{(1)}_2 + \cdots + a_m \mathrm{d}x^{(1)}_m
\]

而 $\mathrm{d}x$ 对 $\mathrm{d}x^{(1)}$ 各项的影响分别是
\[
    \begin{aligned}
        \mathrm{d}x^{(1)}_1 & = b_{11} \mathrm{d}x_1 + b_{12} \mathrm{d}x_2 + \cdots + b_{1n} \mathrm{d}x_n \\
        \mathrm{d}x^{(1)}_2 & = b_{21} \mathrm{d}x_1 + b_{22} \mathrm{d}x_2 + \cdots + b_{2n} \mathrm{d}x_n \\
                            & \vdots                                                                        \\
        \mathrm{d}x^{(1)}_m & = b_{m1} \mathrm{d}x_1 + b_{m2} \mathrm{d}x_2 + \cdots + b_{mn} \mathrm{d}x_n
    \end{aligned}
\]

接下来，怎么把 $\mathrm{d}y$ 写成 $c_1 \mathrm{d}x_1 + c_2 \mathrm{d}x_2 + \cdots + c_n \mathrm{d}x_n$ 的形式呢？答案非常简单粗暴：把每个 $\mathrm{d}x^{(1)}_i$ 代入到 $\mathrm{d}y$ 的表达式中，就有了
\[
    \begin{aligned}
        \mathrm{d}y =\, & a_1 (b_{11} \mathrm{d}x_1 + b_{12} \mathrm{d}x_2 + \cdots + b_{1n} \mathrm{d}x_n) \, + \\
                        & a_2 (b_{21} \mathrm{d}x_1 + b_{22} \mathrm{d}x_2 + \cdots + b_{2n} \mathrm{d}x_n) \, + \\
                        & \cdots \, +                                                                            \\
                        & a_n (b_{m1} \mathrm{d}x_1 + b_{m2} \mathrm{d}x_2 + \cdots + b_{mn} \mathrm{d}x_n)      \\
        =\,             & (a_1 b_{11} + a_2 b_{21} + \cdots + a_n b_{m1}) \mathrm{d}x_1 \,+                      \\
                        & (a_1 b_{12} + a_2 b_{22} + \cdots + a_n b_{m2}) \mathrm{d}x_2 \,+                      \\
                        & \cdots \, \,+                                                                          \\
                        & (a_1 b_{1n} + a_2 b_{2n} + \cdots + a_m b_{mn}) \mathrm{d}x_n
    \end{aligned}
\]

由此可见，可以写出表达式
\[
    \begin{aligned}
        c_j         & = a_1 b_{1j} + a_2 b_{2j} + \cdots, a_m b_{mj}                       \\
        \mathrm{d}y & = c_1 \mathrm{d}x_1 + c_2 \mathrm{d}x_2 + \cdots + c_n \mathrm{d}x_n
    \end{aligned}
\]

这表明，只要我们知道下一层计算出来的梯度 $a_i$ 和联系起两层的 $b_{ij}$，就可以将下一层的梯度“回溯”到上一层，计算出上一层的梯度 $c_j$，这种方法被称作反向传播。于是问题被进一步拆解，变成了更为细化的小问题：如何求出这里每层之间梯度传播的关系？

这里选取最简单的一层 $x^{(k)} = \text{ReLU}(w x^{(k-1)} + b)$ 来说明，其中 $w$ 是权重矩阵，$b$ 是偏置。我们需要理解 $\mathrm{d}x^{(k)}_i$ 应当如何表示。不过这里需要注意一点，在求导时不但要考虑 $x^{(k)}$ 的变化量，还要考虑 $w$ 和 $b$ 的变化量，因为 $w$ 和 $b$ 作为参数也会影响到 $x^{(k)}$ 的值，更是我们希望调整的对象。

首先写出 $x^{(k)}_i$ 的表达式
\[
    x^{(k)}_i = \text{ReLU}(w_{i1} x^{(k-1)}_1 + w_{i2} x^{(k-1)}_2 + \cdots + w_{in} x^{(k-1)}_n + b_i)
\]

接下来分类讨论，在 $\text{ReLU}$ 内部小于等于 0 的情况下，因为输入有微小变化时输出仍然为 0，所以
\[
    \mathrm{d}x^{(k)}_i = 0
\]

而当其输入大于 0 时，ReLU 可以去除掉，即变为
\[
    \begin{aligned}
        \mathrm{d}x^{(k)}_i & = \mathrm{d}(w_{i1} x^{(k-1)}_1 + w_{i2} x^{(k-1)}_2 + \cdots + w_{in} x^{(k-1)}_n + b_i)                                   \\
                            & = w_{i1} \mathrm{d}x^{(k-1)}_1 + w_{i2} \mathrm{d}x^{(k-1)}_2 + \cdots + w_{in} \mathrm{d}x^{(k-1)}_n                       \\
                            & \qquad +x^{(k-1)}_1 \mathrm{d}w_{i1} + x^{(k-1)}_2 \mathrm{d}w_{i2} + \cdots + x^{(k-1)}_n \mathrm{d}w_{in} + \mathrm{d}b_i \\
                            & = w_{i:} \cdot \mathrm{d}x^{(k-1)} + x^{(k-1)} \cdot \mathrm{d}w_{i:} + \mathrm{d}b_i
    \end{aligned}
\]

这里仍然沿用了向量的点积表示法，$w_{i:}$ 表示 $w$ 的第 $i$ 行。由此也可以看出 ReLU 的好处：使用它作为激活函数的梯度回溯非常方便。

这里我们会发现梯度信息发生了一个“分岔”，一部分信息传递给了 $x^{(k-1)}$，另一部分则传递给了 $w$ 和 $b$。传递给 $w$ 和 $b$ 的信息将在后续用于更新参数，而传递给 $x^{(k-1)}$ 的信息则会继续向后传递求出离结果更远、离输入更近的层的参数梯度。直到传递到输入层，最终得到所有参数的梯度，关于原始输入 $x$ 的输入则会被丢弃\footnote{丢弃：因为我们并不需要对输入进行调整，只需要对参数进行调整。不过一些研究表明，观察输入的梯度信息可以帮助我们理解模型认为哪些特征更加重要。}。这个过程看起来像是这样，如果说正向计算是把参数信息参与到计算中，一步步计算得到最终的结果（由于我们希望最小化损失，所以在模型的最终输出之后还有一个箭头表示损失函数）：
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[>=Stealth, scale=1.2]
        \node (x) at (0, 0) {$x$};
        \node (x1) at (2, 0) {$x^{(1)}$};
        \node (x2) at (4, 0) {$\cdots$};
        \node (x3) at (6, 0) {$x^{(k-1)}$};
        \node (x4) at (8, 0) {$x^{(k)}$};
        \node (x5) at (10, 0) {$\cdots$};
        \node (y) at (12, 0) {$y$};
        \node (w1) at (1, -1) {$w^{(1)},b^{(1)}$};
        \node (w3) at (5, -1) {$w^{(k-1)},b^{(k-1)}$};
        \node (w4) at (7, -1) {$w^{(k)},b^{(k)}$};
        \node (w5) at (11, -1) {$w^{(\text{out})}, b^{(\text{out})}$};
        \node (loss) at (13, 0) {$l$};
        \draw [->] (x) -- (x1);
        \draw [->] (x1) -- (x2);
        \draw [->] (x2) -- (x3);
        \draw [->] (x3) -- (x4);
        \draw [->] (x4) -- (x5);
        \draw [->] (x5) -- (y);
        \draw [->] (w1) -- (x1);
        \draw [->] (w3) -- (x3);
        \draw [->] (w4) -- (x4);
        \draw [->] (w5) -- (y);
        \draw [->] (y) -- (loss);
    \end{tikzpicture}
    \caption{正向计算}
\end{figure}

那么反向传播就是考虑损失变化的估计量 $\mathrm{d}l$，并将它的梯度信息经由 $\mathrm{d}y$ 和各个中间计算结果 $\mathrm{d}x^{(k)}$ 逐层传递回去，直到输入层。
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[>=Stealth, scale=1.2]
        \node (x) at (0, 0) {$\mathrm{d}x$};
        \node (x1) at (2, 0) {$\mathrm{d}x^{(1)}$};
        \node (x2) at (4, 0) {$\cdots$};
        \node (x3) at (6, 0) {$\mathrm{d}x^{(k-1)}$};
        \node (x4) at (8, 0) {$\mathrm{d}x^{(k)}$};
        \node (x5) at (10, 0) {$\cdots$};
        \node (y) at (12, 0) {$\mathrm{d}y$};
        \node (w1) at (1, -1) {$\mathrm{d}w^{(1)},\mathrm{d}b^{(1)}$};
        \node (w3) at (5, -1) {$\mathrm{d}w^{(k-1)},\mathrm{d}b^{(k-1)}$};
        \node (w4) at (7, -1) {$\mathrm{d}w^{(k)},\mathrm{d}b^{(k)}$};
        \node (w5) at (11, -1) {$\mathrm{d}w^{(\text{out})}, \mathrm{d}b^{(\text{out})}$};
        \node (loss) at (13, 0) {$\mathrm{d}l$};
        \draw [<-] (x) -- (x1);
        \draw [<-] (x1) -- (x2);
        \draw [<-] (x2) -- (x3);
        \draw [<-] (x3) -- (x4);
        \draw [<-] (x4) -- (x5);
        \draw [<-] (x5) -- (y);
        \draw [<-] (w1) -- (x1);
        \draw [<-] (w3) -- (x3);
        \draw [<-] (w4) -- (x4);
        \draw [<-] (w5) -- (y);
        \draw [<-] (y) -- (loss);
    \end{tikzpicture}
    \caption{反向传播}
\end{figure}

在图中看到从 $\mathrm{d}x^{(k)}$ 指向 $\mathrm{d}x^{(k-1)}$ 或者 $\mathrm{d}w^{(k)}, \mathrm{d}b^{(k)}$ 的箭头时，它指的是关于 $x^{(k)}$ 的梯度信息被传递到了 $x^{(k-1)}$ 和 $w^{(k)}, b^{(k)}$ 上。上面的图只是下面表达式的一个形象说明（这里用 $g_x^{(k)}$ 表示对 $x^{(k)}$ 的梯度，同理定义对 $w$ 和 $b$ 的梯度符号）
\[
    \begin{aligned}
        \mathrm{d}l & = g_y \cdot \mathrm{d}y                                                                                                                                           \\
                    & = g_x^{(\text{out})} \cdot \mathrm{d}x^{(\text{out})} + g_w^{(\text{out})} \cdot \mathrm{d}w^{(\text{out})} + g_b^{(\text{out})} \cdot \mathrm{d}b^{(\text{out})} \\
                    & = (\text{向前展开这一部分}) + g_w^{(\text{out})} \cdot \mathrm{d}w^{(\text{out})} + g_b^{(\text{out})} \cdot \mathrm{d}b^{(\text{out})}                                   \\
                    & = g_x^{(k)} \cdot \mathrm{d}x^{(k)} + g_w^{(k)} \cdot \mathrm{d}w^{(k)} + g_b^{(k)} \cdot \mathrm{d}b^{(k)}                                                       \\
                    & \qquad+ \cdots + g_w^{(\text{out})} \cdot \mathrm{d}w^{(\text{out})} + g_b^{(\text{out})} \cdot \mathrm{d}b^{(\text{out})}                                        \\
                    & = g_x^{(k-1)} \cdot \mathrm{d}x^{(k-1)} + g_w^{(k-1)} \cdot \mathrm{d}w^{(k-1)} + g_b^{(k-1)} \cdot \mathrm{d}b^{(k-1)}                                           \\
                    & \qquad+ g_w^{(k)} \cdot \mathrm{d}w^{(k)} + g_b^{(k)} \cdot \mathrm{d}b^{(k)} + \cdots                                                                            \\
                    & \qquad + g_w^{(\text{out})} \cdot \mathrm{d}w^{(\text{out})} + g_b^{(\text{out})} \cdot \mathrm{d}b^{(\text{out})}                                                \\
                    & = \cdots                                                                                                                                                          \\
                    & = g_x \cdot \mathrm{d}x                                                                                                                                           \\
                    & \qquad + g_w^{(1)} \cdot \mathrm{d}w^{(1)} + g_b^{(1)} \cdot \mathrm{d}b^{(1)}                                                                                    \\
                    & \qquad + g_w^{(2)} \cdot \mathrm{d}w^{(2)} + g_b^{(2)} \cdot \mathrm{d}b^{(2)}                                                                                    \\
                    & \qquad + \cdots                                                                                                                                                   \\
                    & \qquad + g_w^{(\text{out})} \cdot \mathrm{d}w^{(\text{out})} + g_b^{(\text{out})} \cdot \mathrm{d}b^{(\text{out})}                                                \\
    \end{aligned}
\]

随着式子每次将第一项不断地展开，树状的计算图也从计算结果的终点不断向前回溯，最终在每个终止节点处得到对每个分量的梯度信息。理论上只需要手动维护这个图的状态就能完成反向传播的计算，不过实际应用中已经有了非常方便的自动求导工具，现代的机器学习库 PyTorch 和 TensorFlow 都有类似的功能，它们都会在内部维护一个计算图，自动地完成反向传播的计算。以 PyTorch 为例，用户不需要关心如何维护这个图，只需要设置 \texttt{requires\_grad} 属性为 \texttt{True}，在计算后对损失调用 \texttt{backward} 方法，就能自动地完成反向传播的计算。

在训练时数据点通常是固定的，所以可以直接令 $\mathrm{d}x = 0$，这样前面的梯度信息就仅留下与待优化的参数相关的部分
\[
    \begin{aligned}
        \mathrm{d}l & = g_w^{(1)} \cdot \mathrm{d}w^{(1)} + g_b^{(1)} \cdot \mathrm{d}b^{(1)}                                            \\
                    & \qquad + g_w^{(2)} \cdot \mathrm{d}w^{(2)} + g_b^{(2)} \cdot \mathrm{d}b^{(2)}                                     \\
                    & \qquad + \cdots                                                                                                    \\
                    & \qquad + g_w^{(\text{out})} \cdot \mathrm{d}w^{(\text{out})} + g_b^{(\text{out})} \cdot \mathrm{d}b^{(\text{out})} \\
    \end{aligned}
\]

而如果我们再做一层抽象，把这一堆参数塞到一个叫做 $\theta$ 的向量中，那么就可以把上面的式子写成
\[
    \mathrm{d}l = \nabla_\theta l \cdot \mathrm{d}\theta
\]

其中 $\nabla_\theta l$ 就是损失函数 $l$ 对参数 $\theta$ 的梯度，其各个分量表明了损失函数对每个参数的敏感程度。而仿照一元函数下山的做法，同样可以按照如下的方式沿着“下山最快的方向”来更新参数
\[
    \mathrm{d}\theta^* = -\eta \nabla_\theta l
\]

从图形上看，这个过程是垂直于等高线的方向在向下走，最终停留在一个极小值处。或许初看会觉得这一优化方法和等高线并不相关，但如果仔细想想就会发现，实际上等高线的形状就是损失函数的等值线，而沿着等值线的切线方向意味着
\[
    \mathrm{d}l = \nabla_\theta l \cdot \mathrm{d}\theta = 0
\]

前一个部分是梯度的方向，而令上式等于 $0$ 成立的方向 $\mathrm{d}\theta$ 就是等高线的切线方向。两个向量的内积为 $0$ 意味着它们是垂直的，用心观察的读者或许会发现，地图上的水系与等高线交汇之处，两条线往往是垂直的，水往低处走是一种自然的梯度下降。

在实际的神经网络中通常有数万乃至数亿个参数，在这样庞大的参数空间中梯度下降虽然有着下降最快的方向的理论支持，但想象一个高维空间已经足够困难，再去想它里面的等高线是什么样子就更是难上加难，所以为了便于理解，还是看一个低维空间中非常简单的样例来帮助理解。

以一个简单的函数 $f(\theta) = 2\theta_1^2 + \theta_2^2$ 为例，假设从 $(1, 1)$ 处开始迭代，取每次更新的 $\eta = 0.1$，因为 $\mathrm{d}f(\theta) = 4\theta_1 \mathrm{d}\theta_1 + 2\theta_2 \mathrm{d}\theta_2$，所以在第一步梯度为 $(2\theta_1, \theta_2) = (4, 2)$，取 $\mathrm{d}\theta^* = -\eta \nabla_\theta f$ 可以得到下一步的 $\theta$。进行 10 次迭代后，得到的点在等值线图上会按照如下图所示的方式移动，可以看到每次更新的 $\theta$ 都是垂直于等高线线的方向向下走的，逐渐逼近最优解 $(0, 0)$
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/gradient_descent.png}
    \caption{梯度下降}
\end{figure}

不过同时我想也要给出一个错误样例说明学习率 $\eta$ 调太大会发生什么，假设刚才的例子中我们把 $\eta$ 调到 0.5, 就会发现它直接跳过极小值，跳到了空间中的对边，之后就在 $(-1, 0)$ 和 $(1, 0)$ 之间来回震荡了起来，无法收敛。
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/gradient_descent_0.5.png}
    \caption{学习率设置不当的梯度下降}
\end{figure}

这个现象在高维空间中也会发生，如果学习率 $\eta$ 过大，常常会观察到 loss 在某个值附近反复震荡，无法收敛。

在实际有各种不同的梯度下降方法，最简单的是\textoverset{Stochastic Gradient Descent}{随机梯度下降法}(SGD)，它的基本思想是每次只随机抽取部分数据来计算梯度，来更新参数。这样做的好处是每次只需要计算一小部分数据，速度快；坏处是每次计算的梯度并不准确，可能会导致参数在最优解附近震荡。有的为了加速收敛速度，使用\textoverset{Momentum}{动量法}，将梯度信息不作为速度，而作为加速度来更新参数，让它能快速冲下坡。还有的使用\textoverset{Adaptive Momentum Method}{自适应动量法}(Adam)，在每次更新时考虑了历史梯度信息的影响，来调整学习率。这些方法都以各自的方式帮我们找到了更好的参数更新方式，来加速收敛速度。不过动量也不是越大越好，正如学习率不是越大越好一样，过大的动量会导致参数在最优解附近震荡，甚至无法收敛。

\begin{tcolorbox}[myrecommendbox, title=推荐阅读, breakable=false]
    \begin{itemize}
        \item 至此我们已经基本理解了深度学习的核心思想，它是如何拟合的，又是如何优化的，我想看看 3Blue1Brown 的视频系列很有启发性：\\
              \textit{深度学习 Deep Learning - 3Blue1Brown - Bilibili}\\
              \url{https://space.bilibili.com/88461692/lists/1528929?type=series}
        \item 可以说是梯度下降非常通俗的解释：\\
              \textit{用随机梯度下降来优化人生 - 李沐的文章 - 知乎}\\
              \url{https://zhuanlan.zhihu.com/p/414009313}
        \item 文中没细讲反向传播的过程是如何发生的，如果想详细地理解其中的细节可以阅读：\\
              \textit{反向传播算法详解(手算详解传播过程) - 望舒同学的文章 - 知乎}\\
              \url{https://zhuanlan.zhihu.com/p/464268270}
        \item 关于不同优化方法的选择与优劣比较，这篇文章有很好的总结：\\
              \textit{梯度下降法的神经网络容易收敛到局部最优，为什么应用广泛？ - Summer Clover的回答 - 知乎}\\
              \url{https://www.zhihu.com/question/68109802/answer/1677007564}
        \item 虽然本书中关于具体网络结构的介绍在后面，但是读者如果感兴趣也可以先了解一些很经典的网络架构，笔者觉得这一回答总结的很好：\\
              \textit{2024年，深度学习，你心目中的top10算法是什么？ - Russ的回答 - 知乎}\\
              \url{https://www.zhihu.com/question/638660013/answer/3401213045}
    \end{itemize}
\end{tcolorbox}

\newpage

\subsection*{动手实践：简单的网络实践}

为了让读者对训练神经网络形成一个大概的感知，这里我会用一个极其简单的例子来说明如何训练一个神经网络。而我们的目标就是：拟合一个一元函数 $f(x) = \sin 2x$，其中 $x \in [-5, 5]$。不过为了实现这个目标，需要先简单讲一下 Python 中的数组操作。

Python 中内置的数组类型是列表，它是动态的，可以在运行时改变长度，这不同于 C 语言中为数组分配一块内存空间并储存指定类型元素。而 Python 列表中的“元素”也实际上是对象的引用。虽然原则上通常推荐用列表存储相同类型的元素\footnote{通常推荐：写类型标注时通常把列表的类型标记为 \texttt{list[T]}，其中 \texttt{T} 是元素的类型，例如 \texttt{int}。}，但在运行中，这类强制的要求并不存在。与 C 等语言不同的是，Python 由于其运算符重载机制，其中的四则运算是通过调用某些特定的函数实现的\footnote{四则运算：例如 $a + b$ 实际上是调用了 \texttt{a.\_\_add\_\_(b)} 函数来实现的，仅加法这一个操作就要经过多层函数调用的包装，比 C 语言的加法慢了大概一到两数量级。}，如果想在列表实现向量运算，在遍历列表的过程中就需要不停地检查对象的类型并调度对应的方法。打个比方，就像是一个工程师虽然知道生产的每一个环节，但拿到一个工件后也要先检查它的类型才能决定用什么工具来加工它。能处理的东西确实是灵活了，但是另一面是效率低下了。而与之对比的是流水线工人，他们面对的工件和工序是完全确定的，不需要检查工件的类型，只要不停地按照固定的流程来加工就可以了，省去了分析的时间，而且也方便同时上很多人来加工\footnote{多个人加工：指对于算术运算，CPU 中有指令可以并行地执行。}。

用 Python 原生的运算来做数值运算显然不够高效，但是 Python 的精髓就在于它的灵活性和可扩展性。可以说如果没有丰富的第三方库，Python 就不可能发展成今天这样一个强大的语言。在数值计算这一块的基石就是 NumPy，自其 2006 年发布 1.0 版本\footnote{1.0 版本：虽然对于 NumPy 来讲是 1.0 版本，不过它是在历史更久远的 Numeric 和 Numarray 两个已有的数值运算库基础上，为了统一数组运算，作为科学计算库 SciPy 的一个核心模块开发的。}开始，Python 社区内就开始广泛地使用，已经成为了 Python 进行数值计算的\textoverset{De Facto Standard}{事实标准}\footnote{事实标准：指虽然没有官方强制规定要用 NumPy，但 Python 社区内几乎所有教程、科研工作、乃至工业界都在使用它（或依赖它的其它模块），它成为了“数组运算”的通用接口标准。}，重塑了整个生态\footnote{生态：例如 SciPy 和 matplotlib 分别作为科学运算库和绘图库开发早于 NumPy，但是后来它们的底层和接口都改为使用 NumPy。在社区对矩阵运算的庞大需求的推动下，连一向谨慎添加新语法的 Python 官方都专门引入了\uhref{https://peps.python.org/pep-0465/}{PEP 465 提案}，早在 Python 3.5 就引入了额外的 \texttt{@} 运算符来表示矩阵乘法。}。不少人都\uhref{https://www.zhihu.com/question/645463253}{认为} NumPy 设计很好，在运算速度顶级的同时其语法和 Python 的语法风格保持了一致，同时还提供了清晰的接口和丰富的功能。有不少的优质教程可以帮助读者快速上手，\uhref{https://numpy.org/doc/stable/user/absolute_beginners.html}{NumPy 官方的初学者指南}已经是一份非常清晰的教程，其中引用的部分图片取自\uhref{https://jalammar.github.io/visual-numpy/}{NumPy 的图形化展示}，即使单独看这一篇图形化教程也很好。只要用心搜索，中文互联网上亦不乏相关的优质资料，\uhref{https://zhuanlan.zhihu.com/p/396444973}{一些知乎上文章}的清晰度与丰富度也已经可以媲美官方教程，让没有经验的读者也可以快速理解 NumPy 的数组接口设计。

虽有珠玉在前，众多的教程已清晰教会了我们如何使用基础的数组操作，但我还是决定简单地讲一下数组化运算的逻辑。第一条原则是数组中的元素类型是相同的，通常 NumPy 会自动推导\footnote{自动推导：比如整型数组会变成 int64，浮点数组变成 float64。但也可以手动指定 dtype，以防不符合预期，例如如果数据点是整数但是希望参与 64 位浮点运算，我们也可以手动指定数据为 float64 类型。}\textoverset{Data Type}{数据类型}（即 dtype 参数）。第二条则是对于加减乘除这样的四则运算和 NumPy 中提供的所有一元函数（例如 sin, cos, exp 等），运算都是逐个进行的。例如两个 64 位浮点数类型的数组四则运算的结果如下。
\begin{minted}[bgcolor=gray!15, frame=single, framesep=10pt, baselinestretch=1.2]{python}
import numpy as np
x = np.array([1, 2, 3], dtype=np.float64)
y = np.array([4, 5, 6], dtype=np.float64)

print(x + y) # [5. 7. 9.]
print(x - y) # [-3. -3. -3.]
print(x * y) # [ 4. 10. 18.]
print(x / y) # [0.25 0.4  0.5 ]
\end{minted}

与经典的向量操作对比，加减法都完全相同。但是读者或许会感到疑惑，向量点积应该返回一个标量，而向量间并未定义除法。乘除法为什么要这么定义呢？因为数组不仅仅是高维空间中的一个向量，它同样可以用于表示一个函数在每一点上的取值\footnote{函数取值：这里的函数是广义上的，例如图像也可以看成是一个关于空间位置 $(i, j)$ 的二元函数，其中 $i,j$ 分别表示一个像素点的行号、列号。乘 mask 就是一个典型的逐像素作用的例子。}。逐点的乘除法不过是 $f(x) = g(x) h(x)$ 和 $f(x) = g(x) / h(x)$ 在有限个点上的表示。按传统的写法，需要通过一个循环遍历所有的 $x$ 并写入每个点的运算结果，逐元素运算则省去了这样手动遍历的麻烦。不过 \texttt{*} 用于逐元素乘法不代表无法进行想要的的点积运算。在上面的例子中，如果想计算两个向量的点积，只需 \texttt{x @ y} 或者更为明确的 \texttt{x.dot(y)} 就可以了。

了解了 NumPy 的基本原理后，获得 $f(x) = \sin 2x$ 在 $[-5, 5]$ 的样本就非常简单了。\texttt{np.linspace} 可以在给定的闭区间上均匀地采样指定的点，例如如果我们希望在 $[-5, 5]$ 上均匀采样 200 个点，导入 numpy 后通过两行代码就能得到预期的样本：
\begin{minted}[bgcolor=gray!15, frame=single, framesep=10pt, baselinestretch=1.2]{python}
import numpy as np
x = np.linspace(-5, 5, num=200)
y = np.sin(2 * x)
\end{minted}

数据已经准备好，再下一步则是搭建一个用来拟合的神经网络了。NumPy 的数组操作很优雅，但是要搭建神经网络还少一层包装。回忆训练的过程便会发现更新参数依赖于各个环节的梯度，其中第 $k$ 层的梯度计算需要用到上一层的输出 $x^{(k - 1)}$ 和下一层传回的梯度 $g_x^{(k)}$。为了计算梯度，数据在正向传播的过程中有必要留下“痕迹”：计算进入下一层并不能直接将它们丢弃，而是等到梯度计算与权重更新之后才能删除。诚然，其中的许多步骤可以手动推导：写出解析的表达式，自己维护一个表用于暂存这些状态，然后手动计算梯度。但是手动计算\textoverset{Gradient Function}{梯度函数}的时代已经过去了，从最便捷的角度来看，PyTorch 中\textoverset{Automatic Gradient}{自动微分}系统的包装已经很完备，只需定义网络结构就能自动计算梯度并优化参数了。学会钻木取火也许对荒野求生作用很大，但现代社会中的我们在学习做饭时已有了方便的工具，没有理由不直接打开家里的天然气灶。

PyTorch 有着类似 NumPy 的数组接口，但多了许多对网络的包装。常见的用法是在自定义模块时继承 \texttt{torch.nn.Module}，并重写\textoverset{Initialization}{初始化}方法 \texttt{\_\_init\_\_} 和正向计算方法 \texttt{forward} 即可，将求导和参数更新的一切交给 PyTorch 的自动微分系统和优化器来处理。例如对于拟合 $f(x) = \sin 2x$ 的例子，三层的网络已经足够：$1\to 10\to 10\to 1$\footnote{这样的中间层数量和规律并非是固定的，这里仅是随意的选择。}，每一层都是一个带偏置的线性层，在中间两层加上 Leaky ReLU 激活函数并取 $\alpha = 0.1$\footnote{$\alpha$：含义可以回顾本章第一节激活函数。}。最后一层的输出是一个标量，表示对 $f(x)$ 的预测值。定义网络的过程如下：
\begin{minted}[bgcolor=gray!15, frame=single, framesep=10pt, baselinestretch=1.2]{python}
import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()  # Initialize the parent class
        self.linear1 = nn.Linear(1, 10)
        self.linear2 = nn.Linear(10, 10)
        self.linear3 = nn.Linear(10, 1)
        self.leaky_relu = nn.LeakyReLU(negative_slope=0.1)
    def forward(self, x):
        x = self.leaky_relu(self.linear1(x))
        x = self.leaky_relu(self.linear2(x))
        x = self.linear3(x)
        return x
\end{minted}

最上面的 \texttt{SimpleModel(nn.Module)} 表示我们定义的模型继承了 PyTorch 中的模块基类，\texttt{nn.Linear} 表示线性层。\texttt{forward} 作为模型调用的正向计算过程，通过 \texttt{model(x)} 调用时会自动触发 \texttt{forward} 的计算。

不过有一点重要的是调用 \texttt{model(x)} 时，因为一次训练时需要用到多个样本，所以输入的 $x$ 需要是一个二维数组，第一维表示样本数，第二维表示每个样本的特征数。对一元函数来说，特征数就是 $1$，样本数则是我们在前面准备的 $200$ 个点。为了让 PyTorch 能够正确地处理这个数据，我们需要将它转换为一个二维数组。可以使用 \texttt{x.reshape(-1, 1)} 来实现这个功能，其中 \texttt{-1} 表示自动推导这一维的大小，而 \texttt{1} 则表示第二维的大小为 $1$。

至于损失函数与优化器：使用均方误差作为损失函数，\texttt{torch.nn.MSELoss} 就是均方误差的函数，在下面的代码中写作 criterion，即准则。使用 \texttt{torch.optim.AdamW} 作为优化器\footnote{AdamW：是 Adam 优化器的一个变种，它是一个比较现代的优化器，通常比 SGD 更快收敛。}，下面的示例代码中随意地设置了学习率为 $0.001$。这个学习率一个常用的值。
\begin{minted}[bgcolor=gray!15, frame=single, framesep=10pt, baselinestretch=1.2]{python}
import torch.optim as optim

x = torch.from_numpy(x).float().reshape(-1, 1) # float64 -> float32
y = torch.from_numpy(y).float().reshape(-1, 1)
criterion = nn.MSELoss()
model = SimpleModel()
optimizer = optim.AdamW(model.parameters(), lr=0.001)
model.train()
for _ in range(2000):
    optimizer.zero_grad()
    output = model(x)
    loss = criterion(output, y)
    loss.backward()
    optimizer.step()
\end{minted}

迭代次数 2000 是简单实验得到的本任务在 0.001 学习率下较合理的训练轮数。其中 \texttt{optimizer.zero\_grad()} 一步用于清空上一步的梯度信息，\texttt{model(x)} 计算正向传播结果，\texttt{loss.backward()} 计算反向传播梯度信息，\texttt{optimizer.step()} 则是更新参数。

上面就是训练的核心过程。笔者将完整代码放在了 \texttt{examples/simple\_model.py} 中，并加入了进度条和实时输出损失的功能，方便观察训练过程及模型输出的变化。假设读者已经安装了 Python，在运行代码前，读者还需要安装 NumPy, PyTorch, matplotlib 和 tqdm 四个库。可以在命令行使用下面的命令来安装\footnote{提示：如果你在使用虚拟环境，一般不推荐在 base 环境中安装。此外，如果你计划使用 GPU 训练更复杂的模型，推荐安装支持 CUDA 的 PyTorch 版本，详情见前一章 GPU 相关部分。}：
\begin{minted}[bgcolor=gray!15, frame=single, framesep=10pt, baselinestretch=1.2]{bash}
pip install numpy torch matplotlib tqdm
\end{minted}

在运行代码后，训练过程中的模型输出会逐渐接近真实值，一次运行的结果如图所示。图中，由蓝到红的线表明从训练前期到后期的模型输出，虚线表示真值。
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{img/simple_model_output.pdf}
\end{figure}

反映输出和预期 $f(x) = \sin 2x$ 差异的 loss 变化趋势则如下图所示
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{img/simple_model_loss.pdf}
\end{figure}

\newpage

\subsection{如果没人教你怎么做}

在前文中，我们已经学会了如何利用神经网络来拟合一个我们期望的函数。但本质上，这依然是在让机器去完成人类已经会做的任务。它只是沿着数据标记者留下的轨迹，一步步将已有的知识“抄写”进模型之中。这种拿来主义的优势显而易见：路径明确、学习高效，知识转化为模型参数后，能极大地提升生产力。

然而，摸着石头过河终究只能走一段路。我们不可能永远依赖于前人的经验去指引未来的方向。历史的车轮滚滚向前，祖辈的智慧无法解决所有的新问题。终有一日，我们必须走出自己的探索之路。人类的发展史，正是一个不断拓荒、积累知识、改造世界的过程。我们当然希望人工智能不仅能继承人类的经验，还能像人类一样，在探索中扩展认知边界，带来新的发现。所谓开拓精神，就是即使前方一片未知，也要义无反顾地前行。

当没有人教人工智能该怎么做时，我们就让它在试错中学会如何做得更好。正如人类不是天生会走路，而是在一次次跌倒中学会了平衡，我们也可以让\textoverset{Agent}{智能体}\footnote{智能体：即在环境中做出决策的人工智能。}通过探索和反馈，不断调整\textoverset{Policy}{策略}\footnote{策略：即智能体根据环境决定应该怎么做，它也有可能是按某个给定的概率选择动作。例如当拿到一副牌时根据牌面就知道现在可以打哪些牌，应该优先打什么牌，这样的想法就是策略。}、优化\textoverset{Action}{行动}\footnote{行动：智能体按照它的策略选择的实际动作。策略虽然指导了行动，但它并不等同于行动。仍然用打牌来比喻：如果你觉得两张单牌你都不想要，可以“随便”打一张，那么策略是一个概率分布，可能两张牌各 1/2 的概率。行动则是实际打出去的牌，它可以是“随便”选的，但是一次只会打出去一张。}。这，正是\textoverset{Reinforcement Learning}{强化学习}的起点。

在学习强化学习前，为了建立起一个清晰的印象，我想有必要先概括一下强化学习与前面讲的\textoverset{Deep Learning}{深度学习}\footnote{深度学习：前文的用语是神经网络，但是现在的神经网络通常堆叠很多层，因而常常也称为深度学习。}的关系。深度学习和强化学习是两个较为独立的概念。深度学习更多的是考虑如何构建一个模型来预测数据，强化学习则是一种思想，关注的是如何从环境中学习到新知识。因此二者既可以单独地使用，又可以双管齐下，正如我在文中一贯强调的：开发的第一步永远是明确需求。任务的情境决定了是否应当使用深度学习或者是否应当使用强化学习，从而划分出来四种情况：
\begin{itemize}
    \item 完全可以使用经典模型就可以解决的任务，例如静态情况下简单的线性回归，或者动态情况下的经典控制理论，它们既不需要深度学习也不需要强化学习。
    \item 在训练信息已知情况下的预测只需要单独使用深度学习。
    \item 在一些环境有限的较为简单的情况，列出表格就能完成强化学习的任务，这时只需要使用强化学习。
    \item 在一些复杂的环境中，强化学习让知识从环境中来，深度学习让知识到模型中去。在对环境认识更充分的同时也指导智能体更好地探索环境。
\end{itemize}

强化学习听起来就像是一个很复杂的概念，对于初学者而言光是看到“强化”二字就可能感到畏惧，各种各样的术语与学习方法更是让人眼花缭乱，摸不着头脑。数学上，最经典的方式是通过一个用五元组表示的\textoverset{Markov Decision Process}{马尔可夫决策过程}\footnote{马尔可夫决策过程：这解释起来又是一个复杂的概念，简单来说就是决策只用考虑当前\textoverset{State}{状态}，而不需要假设整个环境的背后还有什么“隐藏的”信息。}来描述的。

关于强化学习的场景描述，上来就是一大段关于马尔可夫链的定义显然会劝退读者，因此我想先打几个比方再真正地引入定义。有这样几个很经典的哲学问题：我是谁，从哪来，到哪去。此外我想还应该加上一个问题：一切的意义是什么。这几个问题总的来讲可以与强化学习的场景建模相对应。

首先应该问“我是谁”，换句话说，智能体它本质上是什么呢？我认为是它能对环境做出反应。从一个辩证的角度看来，“存在”就是运动、选择与实践。我的本质在于我的选择和实践，只有在行动中，我才真正体现出我是谁。或许有人会认为智能体当前处在的环境更接近对“存在”的描述。但如前文所述，我认为智能体的本质在于其选择行动的能力。策略是行动的一个概率分布，因此在决策前需要先知道能采取哪些行动。在状态 $s$ 下可以采取的行动通常称作行动空间 $A(s)$，智能体在状态 $s$ 下的输出实际上就是 $A(s)$ 上的一个分布。

那么“从哪来”的问题呢？这就是当前所处的状态 $s$，打牌时我们能观测到自己手上的牌和已经打出去的牌都是已知的信息，这些信息的总和就可以作为一个状态。\footnote{\textoverset{Observation}{观测}：在严格意义上和\textoverset{State}{状态}其实有一些区别，但马尔可夫假设中它们相同。许多优化方法都是在马尔可夫假设下运作的，因此初学者不妨假定它们相同}

至于“到哪去”，则是说一个状态在进行一个行动后下一个状态是什么。不过它并不一定是注定的结局。即使在相同的状态下，相同的行动也可能暗含随机的因素，两次实验投出的骰子也可能带来不同的结局。从数学上，“到哪去”要如何刻画呢？它其实是给定状态与行动下，下一刻状态的概率分布，即 $P(s'|s,a)$，其中 $s'$ 是下一个状态。

但如果把目光从结构转向目标，我们还需要问一个更根本的问题：一切的意义是什么。只有明确了评价指标，才能确定要怎么样优化行为，“明确目标”并不仅仅是开发原则，更是行为学习能否成功的前提。如果在状态 $s$ 下采取行动 $a$，下一个状态是 $s'$，这样的过程中会获得一个\textoverset{Reward}{奖励} $R(s,a,s')$，这个奖励可以是正的也可以是负的。正的奖励表示这个行动是好的，负的奖励则表示这个行动是不好的。当然也可以是零，表示这个行动是中性的。奖励的定义是非常重要的，它直接影响到智能体的行为。

下棋、打牌、或者其它积分制的游戏中有直接且明确的输赢指标，因此它们是最先被强化学习攻克的。机器人控制中，速度、平滑性、执行难度等因素都需要权衡，如何衡量一个指标已经需要细细思考，多个指标的共同优化便更难设计。近年来，随着大语言模型的发展，涌现出用长\textoverset{Chain of Thought}{思维链}先细致思考再给出回答的做法。但是要量化地评判回答好不好则又是一个很困难的问题：数学证明和代码易于形式化验证，它们的评判尚且有个客观的标准，成为了第一批拿来验证大语言模型强化学习的对象，但以艺术创作为代表的许多问题由于因人而异的审美与需求，难以确定什么是“好”，挑战仍然很艰巨。强化学习的有效性并不只取决于算法的聪明程度，更取决于奖励定义是否合理明确——这一点往往比想象中更难做到。

在上面提到的几个基础的问题之外，还有一个从直觉上不太容易直接想到的参数 $\gamma$，称作\textoverset{Discount Factor}{折扣因子}\footnote{折扣因子：在经济学中，也译作贴现因子。}。它的引入是为了保证回报的收敛性，在决策过程中也有着重要的意义——因为 $\gamma$ 决定了评价指标更注重当下还是更眼光长远。众所周知钱币会随着通货膨胀而贬值，明天才能拿到的钱比今天拿到的钱更不值钱，因此考虑累计奖励时需要给未来的奖励打折。$0 \le \gamma \le 1$ 控制着折扣的比例，下一次的奖励 $r$ 在这一次看起来就变为了 $\gamma r$。一般来讲它的值在 $0.9$ 到 $0.99$ 之间，如果设置太小则过于短视，只见到眼前的利益，可能留下长期的潜在隐患；而如果设置太接近 $1$，则可能因为对未来的错误判断，被误导走上错误的道路，却在路上自以为选择正确，沦为一种画饼充饥，只是在行进的路上不断地安慰自己这是一时的阵痛与必要的牺牲。

至此建模所需的五个要素已经概括完毕，表示为一个五元组：
\[
    M = (S, A, P, R, \gamma)
\]

其中
\begin{itemize}
    \item $S$: 状态空间，表示所有可能的状态集合。
    \item $A(s)$: 行动空间，表示状态 $s$ 下所有可能的行动集合。
    \item $P(s' | s, a)$: 状态转移概率，表示在状态 $s$ 下采取行动 $a$ 后转移到状态 $s'$ 的概率。
    \item $R(s, a, s')$: 奖励函数，表示在状态 $s$ 下采取行动 $a$ 后转移到状态 $s'$ 的奖励。
    \item $\gamma$: 折扣因子，表示未来奖励的折扣率。
\end{itemize}

这些要素不只是建模的参数，它们决定了智能体如何理解世界、如何行动，也最终决定它是否能成功学习并做出明智的决策。为了有效的学习，需要精心地设计这些元素。

我们希望把复杂的不确定性问题，转化为对单个动作平均效果的评估，这样能更直接地判断一个动作值不值得做。因为每次行动的结果可能不同，我们可以先不管具体结果，而是先看平均表现，这就需要计算\textoverset{Expectation}{(数学)期望}。特别地，我们注意到在五元组中，$P$ 和 $R$ 都依赖于三个参数：当前状态 $s$、当前行动 $a$ 和下一个状态 $s'$。由于状态转移是一个概率过程，$P(s' | s, a)$ 实际上是一个关于下一个状态 $s'$ 的概率分布。因此，我们可以通过这个概率分布计算期望奖励，得到每个状态-行动对 $(s, a)$ 的期望奖励值。

更为具体地说，仍然采用 $R$ 的符号来表示单步行动的奖励，不过这时定义的是一个二元函数，即
\[
    R(s, a) = \mathbb{E}_{s' \sim P(s' | s, a)}[R(s, a, s')] = \sum_{s'} \underset{\text{每一个结果的概率}}{\underbrace{P(s' | s, a)}} \cdot \underset{\text{结果对应的奖励}}{\underbrace{R(s, a, s')}}
\]

这样就把 $s'$ 从 $R$ 的参数中消去。我们可以把 $R(s, a)$ 理解为在状态 $s$ 下采取行动 $a$ 的期望奖励，这意味着我们不再等到下一个状态发生后再获得奖励，而是直接把这一项提到了结果 $s'$ 出来之前。有的问题中 $s'$ 随 $(s, a)$ 唯一确定\footnote{说明：$s'$ 由 $(s, a)$ 对完全确定时称为确定性环境，例如下围棋时下一步完全由当前局面和走子决定。}，那么 $P(s' | s, a)$ 仅在一个确定的 $s'$ 上取值 $1$，在其它地方取值 $0$，这时就省去了对随机变量 $s'$ 求期望的一步，$R(s, a, s')$ 直接退化为 $R(s, a)$。

如何衡量一个策略的好坏呢？仍然使用一个期望来描述，既然策略是一个分布
\[
    \pi(a | s), \quad \text{where} \; a \in A(s)
\]

因此我们可以先定义每个动作的“好坏”程度（质量），再以策略对这些动作加权平均，得到该状态的整体评价。对动作的评估称作\textoverset{Quality}{质量函数}\footnote{质量：也称作\textoverset{Action-Value Function}{动作值函数}，衡量一个状态-动作对的好坏程度。}，记作 $Q(s, a)$。如果暂且不论它的计算方式，对一个状态 $s$ 的好坏估计，即\textoverset{Value}{值函数}，表示为这一策略下所有可能的行动的加权平均\footnote{记号说明：$V^\pi$ 表示的是在策略 $\pi$ 下的值函数，不是 $V$ 的 $\pi$ 次方。这也意味着当策略改变时，这函数的值也随之改变，下面的 $Q^\pi (s, a)$ 含义同理}：
\[
    V^\pi (s) = \sum_{a \in A(s)} \pi(a | s) Q^\pi (s, a)
\]

读者或许想剥洋葱式地继续追问，动作的质量 $Q(s, a)$ 又是如何计算的呢？这就需要对下一步的状态 $s'$ 进行评估了。第一项是这一个动作带来的即时奖励，第二项是下一个状态的值函数 $V(s')$ 的期望，不过考虑到以后的奖励不如现在的奖励重要，这一步要乘上一个折扣因子 $\gamma$。也就是说：
\[
    \begin{aligned}
        Q^\pi (s, a) & =
        \sum_{s'} P(s' | s, a) \left[ R(s, a, s') + \gamma V^\pi (s') \right]                                                                                \\
                     & = \underset{\text{即时回报期望}}{\underbrace{R(s, a)}} + \gamma \underset{\text{下一个状态的值期望}}{\underbrace{\sum_{s'} P(s' | s, a) V^\pi (s')}}
    \end{aligned}
\]

当然我们可以把一个式子带入另一个式子中得到仅关于 $V^\pi (s)$ 或者仅包含 $Q^\pi (s, a)$ 的表达式。例如将 $V^\pi (s)$ 带入 $Q^\pi (s, a)$ 的表达式从而得到
\[
    Q^\pi (s, a) = R(s, a) + \gamma \sum_{s'} P(s' | s, a) \left[\sum_{a' \in A(s')} \pi(a' | s') Q^\pi (s', a')\right]
\]

而如果反过来，将 $Q^\pi (s, a)$ 带入 $V^\pi (s)$ 的表达式则会得到\footnote{说明：这个方程与上面关于 $Q^\pi (s, a)$ 的递归表达式都是 Bellman 期望方程的等价形式。前者是关于 $Q$ 的 Bellman 方程，后者是关于 $V$ 的 Bellman 方程。}：
\[
    V^\pi (s) = \sum_{a \in A(s)} \pi(a | s) \left[ R(s, a) + \gamma \sum_{s'} P(s' | s, a) V^\pi (s')\right]
\]

熟悉编程的读者或许要捏捏鼻子，笃定地说：$Q^\pi (s, a)$ 和 $V^\pi (s)$ 之间的定义互相依赖，这个式子放到任何一个编程语言里面都会因为一层层展开的函数调用而导致无限递归，或在实现中造成死循环似的矛盾，这样一个定义怎么可能成立呢？而我们之前给出的两种代入方式似乎也印证了这个问题：不论从哪个函数开始，最终都会引用回自身。

然而从数学角度看，这样的定义不仅合理，而且结构上非常优美，它妙就妙在 $\gamma$ 让后期的奖励经折扣变得越来越小：虽然包含无限项，但只要 $R$ 是有界的，它最终就确实能收敛。因为后面的 $V$ 展开为 $Q$ 再展开为 $V$ 时每展开一次都要乘上一个 $\gamma$，遥远的奖励会被越来越小的折扣因子压缩到接近 $0$。虽“一尺之锤，日取其半，万世不竭”，但越来越小的比例让余项逐渐无足轻重，从结果上是收敛的\footnote{收敛性说明：还记得级数收敛性的读者或许不难推出：只要 $0 \le \gamma < 1$ 这个级数就会收敛，给定 $|R|$ 的上界 $M$，与要求的精度 $\varepsilon$，完全可以计算出一个截断的 $N$，使得“只需要”展开 $N$ 次并丢弃后面的项就能保证估计值 $\hat Q^\pi (s, a)$ 与真实值 $Q^\pi (s, a)$ 之间的误差小于 $\varepsilon$。}。另外，从操作上并非通过暴力地展开来计算所有的结果，因为指数级增长的情况使得计算并不现实，通过两项之间的管理来递推导出与更新值（或质量）函数的估计值才是实际的做法。

另外一种情况是与环境的交互每次都能在有限时间内停止，有一个\textoverset{Terminal State}{终止状态}所组成的空间 $T$，当到达一个终止状态 $s_T \in T$ 时环境的演化便不再继续，而是直接给出一个值 $V(s_T)$，例如下棋到最终返回一个胜/负/平得到的 +1、0、-1 或其他环境约定的终值，因此也最终会收敛。值得一提的是，这样的值是不依赖于策略 $\pi$ 的，因为智能体的行动已经走到了终止，在这个状态下不会有新的动作与奖励发生，未来回报为零，因此策略无从作用。

现代的强化学习算法几乎无一例外地将任务分为了两个部分，一个部分是认识环境，一个部分是优化策略，这两个部分的灵活组合衍生出了多种不同的算法。强化学习的算法多种多样，在一节内讲完显然不可能。为了简单起见，先来看看最简单的 Q-Learning 的方法。传统的 Q-Learning 不需要神经网络，对环境的认识直接显式地存储在一个表格中，选择动作时直接根据查表的结果选择。它只能解决一些很简单的问题，不过作为一个经典算法，它确实也是很纯粹的强化学习。那么，Q-Learning 到底是怎么“学会”行动的呢？

Q-Learning 做的第一个假设就是：空间是有限的。也就是说，状态空间 $S$ 和行动空间 $A$ 都是有限的集合。认识环境的部分变得极为简单：只需要拿一个表\footnote{表：在 Python 中通常使用字典来实现。}来储存每个状态-行动对 $(s, a)$ 的质量（后文中称作 $Q$ 值）估计\footnote{估计：与真值相比可能会有误差。理想情况下随着与环境的持续交互，经验会逐步修正误差，对 $Q$ 值的估计会不断更新并趋近于真实值。但是真实情况下可能会有各种阻碍。} $\hat Q(s, a)$，我们无法一开始就知道每个 $(s, a)$ 的真实价值\footnote{说明：认识环境的关键就在于逐渐修正价值估计，如果一开始都知道真值就完全没必要学了。}，因此初始时通常全部初始化为 0，虽然一开始环境是未知的，但在每次行动交互后，算法可以通过经验更新这个表格来形成对环境更加精准的认识。

在这样的假设下策略也很好写出：基于对 $Q$ 值的估计 $\hat Q$ 直接选取 $Q$ 值最大的行动\footnote{选取方式：这种方法通常称为贪心选择。不过在探索环境时通常会有一些其它的处理，例如 $\varepsilon$-greedy 或者按照温度选择的策略，这一点会在后文提到。}。也就是说\footnote{记号说明：上标的星号表示最优，$\pi^*_{\hat Q}$ 表明智能体基于 $Q$ 值估计给出其认为的最优策略。}：
\[
    \pi^*_{\hat Q} (a | s) = \begin{cases}
        1, & \text{if } a = \argmax_{a' \in A(s)} \hat Q(s, a') \\
        0, & \text{otherwise}
    \end{cases}
\]

需要注意，这里的策略是确定性的，即在每个状态下始终选择当前估计价值最高的动作，但是这里为了保持前后文记号的一致性，仍然写作一个分布。不严格地说对于这种确定性的策略\footnote{记号说明：在其它地方你可能会看见这样的表示，这个式子中的 $\pi^*_{\hat Q}$ 不表示概率分布，而是从状态直接映射到最优的行动。不过本文中仅此处展示这样的写法，其它地方仍然表示概率分布。}也可以写成
\[
    \pi^*_{\hat Q} (s) = \argmax_{a' \in A(s)} Q(s, a')
\]

在贪心策略下，很容易由 $Q$ 的估计 $\hat Q$ 导出对值函数的估计 $\hat V$\footnote{记号说明：严格来讲应当写作 $V^{\pi^*_{\hat Q}}$，但从视觉上这样上标与下标的嵌套书写会有些混乱。美观起见，此处省略了 $\pi$ 并将其上下标直接标注在 $V$ 上。}：
\[
    \begin{aligned}
        \hat V^*_{\hat Q} (s) & = \sum_{a \in A(s)} \pi^*_{\hat Q} (a | s) \hat Q(s, a)                         \\
                              & = \hat Q(s, a^*), \quad \text{where } a^* = \argmax_{a' \in A(s)} \hat Q(s, a') \\
                              & = \max_{a' \in A(s)} \hat Q(s, a')
    \end{aligned}
\]

也就是说，在当前状态 $s$ 下，我们评估其价值时，重要的只是当前估计下最有价值的那个动作所带来的期望收益，这与策略的贪心性质保持一致。在理想状态下，对于贪心策略下真正的 $Q$ 值，对于每个 $(s, a)$ 对，它都应当满足前文导出的 Bellman 方程（这里将相同的式子重复写一遍）：
\[
    Q^\pi (s, a) = R(s, a) + \gamma \sum_{s'} P(s' | s, a) \left[\sum_{a' \in A(s')} \pi(a' | s') Q^\pi (s', a')\right]
\]

在贪心的策略下，作适当的化简便可以得到\footnote{说明：这一方程通常称为 Bellman 最优性方程。}：
\[
    \begin{aligned}
        Q^* (s, a) & = R(s, a) + \gamma \sum_{s'} P(s' | s, a) \left[
            \sum_{a' \in A(s')} \pi^* (a' | s') Q^* (s', a')
        \right]                                                                                               \\
                   & = R(s, a) + \gamma \sum_{s'} P(s' | s, a) Q^* (s', a^*) \quad \text{where} \; a \in A(s) \\
                   & = R(s, a) + \gamma \sum_{s'} P(s' | s, a) \max_{a' \in A(s')} Q^* (s', a')
    \end{aligned}
\]

然而对于估计值 $\hat Q$，上述等式通常并不成立，因为它尚未收敛于真正的 $Q^*$，存在一定误差。即实践中
\[
    \hat Q (s, a) \ne R(s, a) + \gamma \sum_{s'} P(s' | s, a) \max_{a' \in A(s')} \hat Q (s', a')
\]

总是会有一个误差\footnote{误差：通常称作 Bellman 误差、Q 误差、或者\textoverset{Temporal Difference Error}{时序差分误差}(TD Error)。}，记作 $\delta_{\hat Q} (s, a)$，即\footnote{记号说明：$\text{LHS}$ 表示\textoverset{Left-Hand Side}{等式左侧}，$\text{RHS}$ 表示\textoverset{Right-Hand Side}{等式右侧}。}：
\[
    \begin{aligned}
        \delta_{\hat Q} (s, a) & = \text{RHS} - \text{LHS}                                                                                                                                                                  \\
                               & = \underset{\text{经过单步奖励修正的 $Q$ 值估计}}{\underbrace{R(s, a) + \gamma \sum_{s'} P(s' | s, a) \max_{a' \in A(s')} \hat Q (s', a')}} - \underset{\text{$Q$ 值的直接估计}}{\underbrace{\hat Q (s, a)}}
    \end{aligned}
\]

而我们期待的是让 $\hat Q$ 逐渐收敛到 $Q^*$，因此这个不等号左右两侧的差值 $\delta_{\hat Q} (s, a)$ 也应该逐渐减小。强化学习的核心就是用这一误差 $\delta_{\hat Q} (s, a)$ 作为更新的信号，来修正对 $Q$ 值的估计。很显然地，下标的 $\hat Q$ 表明这一误差依赖于当前的 $Q$ 值估计，因此在估计量 $\hat Q$ 变化时，$\delta_{\hat Q} (s, a)$ 也会随之变化。

这已经很接近 $Q$ 值估计的更新了，然而仍然有一个问题：即使是将式子化为了关于 $\hat Q$ 的形式，$\delta_{\hat Q} (s, a)$ 的真值可能仍然未知，因为它依赖于 $P$ 和 $R$ 的真实值，而我们并不知道真值。除非我们拥有对环境的完整建模，否则状态的转移只能通过自己尝试来获得，所幸我们可以记录下每一步的当前状态 $s$、当前行动 $a$、下一个状态 $s'$ 和即时奖励 $r$，这给了我们一个退而求其次的方法，即考虑在 $s \overset{a}{\to} s'$ 时的误差：
\[
    \delta_{\hat Q} (s, a, s', r) = r + \gamma \max_{a' \in A(s')} \hat Q (s', a') - \hat Q (s, a)
\]

虽然它和 $\delta_{\hat Q} (s, a)$ 之间有着很明显的差别，但是在对 $s'$ 求期望的意义上，至少有
\[
    \mathbb{E}_{s' \sim P(s' | s, a)}[\delta_{\hat Q} (s, a, s', r)] = \delta_{\hat Q} (s, a)
\]

回忆一下我们学过的参数更新过程：在深度学习中，以负梯度作为更新信号时，学习率 $\eta$ 给出更新：
\[
    \theta^{\text{new}} = \theta^{\text{old}} - \eta \cdot \text{gradient}
\]

正如深度学习中使用学习率控制梯度下降过程一样，Q-Learning 中同样使用一个学习率 $\alpha$ 来控制更新的幅度\footnote{记号说明：Q-Learning 中更习惯使用 $\alpha$ 而非前文使用的 $\eta$ 来表示 $Q$ 表的学习率。}。假设我们已经有了一些 $(s, a, s', r)$ 的样本，我们就能以 $\delta_{\hat Q} (s, a, s', r)$ 为更新信号，修正对 $Q$ 值的估计：
\[
    \hat Q^{\text{new}} (s, a) = \hat Q^{\text{old}} (s, a) + \alpha \cdot \delta_{\hat Q} (s, a, s', r)
\]

从某种意义上，这颇有一种“自己拟合自己”的感觉：$Q$ 值估计的更新依赖于当前的 $Q$ 值估计。读者或许想问：如果原来的 $Q$ 值估计 $\hat Q^{\text{old}} (s, a)$ 就是不靠谱，那更新后不还是错的吗？这个话既不完全对，也不完全是错的。确实，这样的更新会将原来的误差带到新的估计中，因此原本的估计不好确实会影响到新的估计。但是一方面，这样的误差会随着 $\gamma$ 的引入而衰减，另外，$r$ 直接地引入了当前的奖励，虽然修正的估计值也不准确，但是总的来说会变得更准一些。

这个过程可以说充满了随机因素：在每次更新时，$\hat Q$ 的估计会依赖于当前的 $Q$ 值估计，这本身就带有一定的误差；而在每次更新时，$\delta_{\hat Q} (s, a, s', r)$ 不仅依赖于 $(s, a)$ 对，还依赖于采样探索时的下一个状态 $s'$ 和即时奖励 $r$，如果状态转移本身就带有随机性，因而 $s'$ 与 $r$ 本身也会有随机性，$V(s')$ 与 $r$ 本身就有可能波动，而非随着 $(s, a)$ 而唯一确定，这种波动称为\textoverset{Sampling Error}{采样误差}。因此 $\alpha$ 的选择非常重要，如果简单粗暴地令 $\alpha = 1$，那么把式子展开就会得到\footnote{说明：读者不难将 $\alpha$ 带入自行推导出这个式子。不过这么更新是一个错误示范。}
\[
    \hat Q^{\text{new}} (s, a) = r + \gamma \max_{a' \in A(s')} \hat Q^{\text{old}} (s', a')
\]

也就是直接拿修正的 $Q$ 值估计来替换原来的 $Q$ 值估计，但是一旦 $r$ 存在随机性或者探索的路径发生了一些变化，这样的做法就会导致 $Q$ 值的估计在每次更新时都发生剧烈的震荡，而无法收敛。反之如果 $\alpha$ 过小，更新的幅度就会过小，虽然不会发生剧烈的震荡，但是收敛速度会变得很慢。一般来讲，$\alpha$ 的值在 $0.1$ 到 $0.5$ 之间比较合适。这样它既能较快地保证收敛，又能起到一定的滤波作用，避免过大的震荡。这种做法的历史根源向上追溯到经典控制中的\textoverset{Kalman Filter}{卡尔曼滤波器}，通过对当前的估计值和新的观测值进行加权平均来修正对状态的估计，基于增益\footnote{增益：在卡尔曼滤波器中，增益是一个权重因子，用于平衡当前估计和新观测值之间的影响。}来控制更新的幅度。

如果要处理井字棋或者走迷宫这样的简单场景，需要的元素 $(S, A, P, R, \gamma)$ 都很容易定义出来，而 $S$ 与每个状态下的 $A(s)$ 都有限，参数更新的规则又有了，看起来已经可以直接上手了。不过如果真正试一下就会发现一个问题：一旦探索到某条路径上的 $\hat Q > 0$，它可能直接导致在后续的探索中，智能体总是选择这条路径，而忽略了其它可能更好的路径。就像是一个挖矿者，在走了一段路后发现了一个闪亮的宝箱，便急于停止挖掘，错过了藏在更深处的真正的钻石。就像下面这张经典的图一样：
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{img/digging_for_diamonds.png}
    \caption{挖掘钻石的人错过了更好的机会}
    {图源：\uhref{https://www.kapwing.com/explore/digging-for-diamonds-never-give-up-meme-template}{Kapwing}}
\end{figure}

这意味着守成不变的贪心策略并不适合于探索未知的环境，路径依赖带来的可能是不思进取。一味地效仿已经被验证过的路径，可能会陷入局部最优解而无法跳出，如果仅仅因为可能的风险而反对变革，便会堵死更好的道路，最终掉进局部最优解里打转的周期之中。为了避免这种情况，需要在探索与利用之间取得平衡，既要利用当前的知识来选择更好的行动，又要适时地探索新的路径。
这就称为\textoverset{Exploration}{探索}与\textoverset{Exploitation}{利用}之间的平衡：在每次行动时，智能体以一个小概率 $\varepsilon$ 随机选择一个动作，而不是总是选择当前估计值最大的动作。这样的扰动或许可以揭示一条更好的道路，避免陷入局部最优解。$\varepsilon$ 通常会随着时间的推移而逐渐减小，这样一开始应当多探索一些新的路径，增长对环境规律的认识。而后期作为一个成熟稳重的智能体，则更多地利用当前的知识来选择更好的行动。

经验利用与探索之间的平衡是强化学习中的一个重要问题，因为一方面我们希望获得最优的策略，另一方面我们又需要在有限的时间内探索足够多的状态和行动，以便获得足够的信息来更为全面地评估状态与行动的价值。像上面这样以 $\varepsilon$-greedy 的方式来平衡探索与利用的策略是一种常用方法，其它的方法还有基于\textoverset{Temperature}{温度}\footnote{温度机制：在每次行动时，智能体以一个温度参数 $\tau$ 来控制选择动作的概率分布，这种选择方式称为\textoverset{Boltzmann Exploration}{玻尔兹曼探索}，与粒子在不同能级上的分布有关。}、\textoverset{Upper Confidence Bound}{上置信界}（UCB）\footnote{上置信界：与置信区间的估计有关，它是多臂老虎机（一种赌博机）问题解出的最优结果，也有着良好的数学基础，许多基于树搜索的方法都依赖于对上置信界的估计。}等的方法。在 Q-Learning 中，因为 $\pi^*_{\hat Q}$ 是一个确定性的策略，因此需要人为地引入一个随机性来平衡探索与利用，采样时的实际策略与当前计算出的理想策略 $\pi^*_{\hat Q}$ 不同，这称为\textoverset{Off-Policy}{离线策略}。而在一些其它的强化学习算法\footnote{其它的强化学习算法：例如 Actor-Critic 等方法}中，策略本身就是一个概率分布，因此通常不需要引入额外的探索机制，而是直接从 $\pi$ 中采样动作，这称为\textoverset{On-Policy}{在线策略}。

至此，有了参数更新机制，又有了探索与利用的平衡，Q-Learning 算法就可以完整地写出来了。算法的核心思想是：在每次行动时，智能体根据当前的 $Q$ 值估计 $\hat Q$ 选择一个动作 $a$，然后与环境交互得到下一个状态 $s'$ 和即时奖励 $r$，并用 $\delta_{\hat Q} (s, a, s', r)$ 来修正对 $Q$ 值的估计。不过俗话说“温故而知新”，智能体的学习也是一样：$(s, a, s', r)$ 的样本并不是用过一次就丢掉的，智能体可以将它们存储在一个经验池（也称作\textoverset{Replay Buffer}{回放池}）中，更新参数时再从中读取。在探索多次后会反复使用这些样本来更新 $Q$ 值估计，就像学习时我们会回忆过往的尝试经理，并使用它们进一步巩固自己的认识，这一过程称作\textoverset{Experience Replay Mechanism}{经验回放机制}。需要注意的是，在更新的过程中，每次的更新信息 $\delta_{\hat Q} (s, a, s', r)$ 也会随着 $\hat Q$ 的变化而变化，因此每次更新时都需要重新计算。

不过正如在引入 Q-Learning 时所提到的，Q-Learning 只能解决一些简单的问题，面对复杂的环境时，一个表便完全无法存储所有的 $(s, a)$ 对了。以下围棋为例，棋盘上的格点是有限的，因此每个局面 $s$ 下的行动空间 $A(s)$ 也是有限的。但即便整个状态空间 $S$ 是有限的，其大小却令实际的储存不切实际。对于 $19 \times 19$ 的棋盘，每个格点上可以放置黑子、白子或者为空，状态空间大小的上界在 $3^{19 \times 19}$，也就是 $10^{172}$ 的数量级，任何的计算机都无法存储这样一个表格。对于这种情况，可以改用一个神经网络来近似地表示 $Q$ 值估计 $\hat Q$，而更新的过程也改为对 $\delta_{\hat Q} (s, a, s', r)$ 平方和的梯度下降，不过前提是从状态空间中确实能够抽离出某种“规律”，神经网络才有可能学习到较好的近似函数。这种使用神经网络近似 $Q(s, a)$ 的方法称为\textoverset{Deep Q-Learning}{深度 Q 学习}，它基本上是 Q-Learning 直接的神经网络变种。

如果说用神经网络近似 $Q$ 表还能在一定程度上弥补表格空间不够的问题，仍然能用最大化 $\hat Q(s, a)$ 选出策略 $a^*$。那么更为甚者，当状态空间 $S$ 与每个状态下的行动空间 $A(s)$ 都是无限、甚至是连续空间时\footnote{连续行动空间：例如控制机器人的关节角度。}，不但表格方法失效了，策略也不再能简单地通过查表来找到最大值。这时，在策略部分也需要引入深度学习了，输入一个状态 $s$，输出通常包含两个部分：
\begin{enumerate}
    \item 值函数：由于状态空间的无限性，$Q$ 的完整估计 $\hat Q$ 也无法写出，所以退而求其次，计算的是基于值函数的估计 $V(s)$。
    \item 策略函数：输出一个概率分布 $\pi(a | s)$，表示在当前状态下选择每个动作的概率。
\end{enumerate}

这两个部分通常称为\textoverset{Critic}{评论者}与\textoverset{Actor}{行动者}\footnote{评论者与行动者：评论者负责评估当前的状态与行动的价值，行动者负责选择当前的行动。不过笔者更喜欢的叫法是\textoverset{Value Head}{价值头}与\textoverset{Policy Head}{策略头}，这一描述从含义上或许更为清晰。}。

评论者的目标通常仍然是最小化 $\delta$ 的平方和，来修正对值函数的估计，不过这里既然不是基于 $\hat Q$ 的估计了，不能还是 $\delta_{\hat Q} (s, a, s', r)$ 了，这一误差的形式如何便有待商榷。当基于 $Q$ 值估计时，我们借助 $Q$ 值的 Bellman 方程来定义了误差。为了定义基于 $V$ 值的误差，我们需要借助 $V$ 的 Bellman 方程来定义，前文中的形式为：
\[
    V^\pi (s) = \sum_{a \in A(s)} \pi(a | s) \left[ R(s, a) + \gamma \sum_{s'} P(s' | s, a) V^\pi (s')\right]
\]

然而对于连续的情况，我们不得不把求和改为积分，形式上大约是这样：
\[
    V^\pi (s) = \int_{a \in A(s)} \pi(a | s) \left[ R(s, a) + \gamma \int_{s'} P(s' | s, a) V^\pi (s')\mathrm{d}s'\right] \mathrm{d}a
\]

因此对于对 $V$ 的估计 $\hat V$，我们仍然可以定义一个误差 $\delta_{\hat V} (s)$，即
\[
    \begin{aligned}
        \delta_{\hat V} (s) & = \text{RHS} - \text{LHS}                                                                                                            \\
                            & = \int_{a \in A(s)} \pi(a | s) \left[ R(s, a) + \gamma \int_{s'} P(s' | s, a) V^\pi (s')\mathrm{d}s'\right] \mathrm{d}a - \hat V (s) \\
    \end{aligned}
\]

与之前相同的是，我们并不知道 $P$ 和 $R$ 的真实值，因此真实的 $\delta_{\hat V} (s)$ 也无从计算，只能粗糙地用样本上的采样来近似真值，假设有样本 $(s, a, s', r)$，将 $a$ 与 $s'$ 都用单点分布代替，并在样本上化简为：
\[
    \delta_{\hat V} (s, a, s', r) = r + \gamma \hat V (s') - \hat V (s)
\]

总的来说，先用样本的单步回报信息修正，再估计估计中含有的误差，这一思想与前面计算 $Q$ 值的 Bellman 误差思路相同。注意到这里对 $a$ 和 $s'$ 都做了采样处理，这意味着这一过程中发生了两次近似，一次是用单点的 $a$ 近似策略下的期望，另一次是用单点的 $s'$ 近似 $P$ 和 $R$。好处是我们不需要知道环境的精确模型，仅通过经验采样即可进行学习\footnote{说明：这种方法称为\textoverset{Model-Free}{无模型}的方法，“模型”指的是对环境的精确建模。}。但是坏处也很显然：它引入的误差更大，因此同时优化策略与值函数的算法通常会比单独优化其中一个要复杂得多，许多算法就是为了保证这一过程的稳定性与收敛性而提出的。

虽然 Q-Learning 中的贪心策略可以直接用 $\hat Q$ 来选择最优的行动，但在连续的空间中，通常意义上的最大值并不现实，因此只能再退一步，试图最大化 $V^\pi (s)$。而行动者则是需要优化策略来提高 $V^\pi (s)$，回忆 $V^\pi$ 的计算公式
\[
    V^\pi (s) = \sum_{a \in A(s)} \pi(a | s) Q^\pi (s, a)
\]

在连续情况下变为了
\[
    V^\pi (s) = \int_{a \in A(s)} \pi(a | s) Q^\pi (s, a) \mathrm{d}a
\]

而对动作采样近似就得到了这一项的贡献为（贡献会随着概率，即重要性 $\pi(a | s)$ 而变化）：
\[
    \pi(a | s) [r + \gamma \hat V^\pi (s')]
\]

也就是说在优化值估计的同时，我们希望通过增大这一项的贡献来增大实际的期望 $V^\pi (s)$。而在主流强化学习算法中，$L$ 中的后一项从 $Q$ 的估计改为一种\textoverset{Advantage}{相对优势}估计，它考虑的是差值而非值本身，例如在 PPO 中\footnote{\textoverset{Proximal Policy Optimization}{近端策略优化}(PPO)：一种策略优化算法，由 OpenAI 的 John Schulman 等人在 2017 年提出。具体的优化方法在此处不再叙述，读者可以自行翻阅\uhref{https://arxiv.org/pdf/1707.06347}{原论文}或查找网上的其它相关解释。}，目标函数中用策略加权的的项就不是 $Q$ 值，而是基于优势函数 $A(s, a) = Q(s, a) - V(s)$，使用 Bellman 误差的加权和来给出相对优势的估计 $\hat A$ 用于评估当前策略相对旧策略的改进效果，实践证明这极大提高了稳定性。同时基于一种比较的思想，最大化的策略一部分还要除以 $\pi^{\text{old}} (a | s)$，来鼓励相对当前的策略进行优化。

从信息流动的角度来看，这种二元的结构引发的信息流差不多是这样的：为了减少对值函数的估计误差，评论者会根据下一步的值函数估计 $\hat V(s')$ 与回报，通过在值函数的 loss \footnote{值函数的 loss：即 $\delta$ 的平方平均值。}中设置一个误差的项来修正当前对 $V$ 值的估计 $\hat Q(s)$，信息便从环境流向评论者；而行动者则会根据当前的 $Q$ 值估计 $\hat Q$ 来优化 $\pi$，在策略函数的 loss \footnote{策略函数的 loss：通常是负的期望值函数或者期望优势。}中设置一个希望极大化的目标（为统一为最小化问题，通常将其负号加入 loss 中）。信息进一步从评论者流向行动者。智能体在修改策略的同时探索新的环境，新的信息则再次流向回报，形成一个闭环，推动着信息随着策略的优化不断从环境流向模型。

与通常的深度学习不同的是，这一过程中两个部分的的 loss 函数都并不是单向的信息接受器，而是“有进有出”的。策略引来的新探索将新计算出的偏差注入值的 loss，评论者的更新则从其中“取出”新的信息，并将计算出来的策略改善信息注入策略的 loss。这一过程中 loss 就像一个外卖柜，信息的传入和传出在训练过程中不断发生。loss 大时，可能表明无法收敛，但是有时又表明信息流正在快速运行；而 loss 小的时候，可能表明模型已经收敛，但是有时又表明信息流动缓慢。

放眼如今的强化学习研究界，除了 Q-Learning 之外，基于问题的各种条件与假设强化学习算法也有了各种各样的变种与改进。这一节之内无法一一列举，仅从理念上阐释了最基础的强化学习算法 Q-Learning 的基本思想。在此过程中，一些基础的概念已经基本上介绍完毕，其它的变种只是在某种对环境的假设下做文章，无非是优化方式或目标设置不同。真正掌握核心机制后，面对新方法就无需害怕，而能通过把握环境特征、面向的问题与信息流的方向快速理解其本质。不过我一向认为良好的直觉与理解是学习强化学习的基础，各个方法的细节反倒是可以在需要时再临时查阅的，只是希望读者在阅读完这一节后，能够理解核心概念，清晰地认识强化学习的基本思想。

回顾在本章中讲过的内容，信息非常多，涵盖了深度学习与强化学习中的关键思想与方法。那么在这一章的最后，我们照例来小结如下：

\begin{itemize}
    \item 最开始，我们从函数拟合引入。学习神经网络基本上需要牢牢记住一个点，它的本质就是为了拟合，从而找到数据的规律。
    \item 神经网络的基本模型可以概括为一个从输入到输出计算，再通过偏差矫正的循环，不过输入和输出还需要做一些编码与解码处理。
    \item 神经网络的基本模块是一些矩阵运算与非线性激活函数的组合，一层层叠加起来就可以做到简单的函数无法拟合的复杂函数。
    \item 激活函数通过引入非线性，给了神经网络强大的表达能力，基本的逻辑函数都能借助激活函数来表示就表明它至少能够完成逻辑运算能够解决的问题。
    \item 神经网络的训练过程就是通过反向传播算法来计算梯度，进而更新参数，来最小化损失函数，梯度下降算法是常用的优化方法。就像下山一样，梯度帮助函数找到极小值，不过也可能陷入局部极小，学习率与优化器的选取也很重要。
    \item 梯度下降中反向传播的计算和正向的计算刚好反过来，信息流原本是从输入与参数“汇聚”到输出，反向传播则是从输出“分散”到输入与每一个参数之中，描述了它们的贡献。
    \item 最后一节中，我们介绍了强化学习的基本思想，虽然强化学习不一定依赖于网络，但是也是机器学习的一个重要组成部分。与神经网络相比，它更注重从环境中获取样本从而得到信息的过程。
    \item 通常会通过五个要素来建模环境。“我是谁、从哪来、到哪去”可以解释动作空间、状态空间与转移函数，奖励函数是环境的反馈，折扣因子则是对未来奖励的折现，表明未来的奖励更不重要。
    \item 在环境的基础上，通过 $V$ 或者 $Q$ 可以列出状态或者动作的价值函数，来评估当前的状态或者动作的好坏。并通过 Bellman 方程来递归给出表达式。
    \item 总的来说强化学习会分为认识环境与策略优化两个部分，前者是通过奖励来学习环境的规律，后者则是通过对环境的认识来优化策略。
    \item 我们以 Q-Learning 为一个简单的例子，体会了强化学习的基本思想，它的策略非常简单直接，认识环境直接通过记录在 Q 表中，更新时通过对 Q 表的更新来优化策略。Bellman 误差 $\delta$ 描述了 $Q$ 表和预期的差异，并作为指导信息修正 Q 表。
    \item 除此之外，还需要探索机制与经验回放机制来平衡探索与利用，避免陷入局部最优解，并增加样本的利用率。
    \item 如果状态空间无限但是动作空间有限，仍然可以使用 Q-Learning 的思想，只不过需要用神经网络来近似 $Q$ 表，更新时通过对 $\delta$ 的平方和进行梯度下降来修正。
    \item 如果连动作空间也是无限的，那么价值和策略都需要用神经网络来近似，评论者与行动者分别负责对值函数与策略函数的估计与优化。评论者通过对值函数的误差来修正对值函数的估计，行动者则通过对值函数的估计来优化策略。
    \item 这种二元结构的运作方式是一个循环，获取到的信息会增加对环境的认识（即改进值函数的估计），而对环境的认识又会影响到策略的优化（即改进策略函数的估计），策略的优化进一步引发新的探索，带来新的信息，最终在理想状况下会越学越好。
    \item 强化学习的研究发展到今天，已经有了许多不同的变种与改进，只需要理解环境特征、它面向的问题和信息流的情况，就可以对比不同的算法来理解它们的本质。
\end{itemize}

\begin{tcolorbox}[myrecommendbox, title=推荐阅读, breakable=false]
    \begin{itemize}
        \item 强化学习其实就是一种控制：\\
              \textit{为什么自动化所很多做强化学习的课题组？ - 消融ball的回答 - 知乎}\\
              \url{https://www.zhihu.com/question/647238131/answer/3424550835}
        \item 于是你或许会有这样的疑问，感觉强化学习好像许多时候不如传统方法？这种想法是对的，但是强化学习本质上是在很弱的前提下（许多时候只能记录状态，行动和奖励）求解问题，详细解释可以看：\\
              \textit{为什么我总感觉强化学习不是真的人工智能？ - Lemon的回答 - 知乎}\\
              \url{https://www.zhihu.com/question/542991276/answer/2580779607}
        \item 不过强化学习很多时候看起来有用的方法实际上很难落地，因此许多时候还应用受限，一些关于它的吐槽可以看回答：\\
              \textit{为什么说强化学习在近年不会被广泛应用？ - 王源的回答 - 知乎}\\
              \url{https://www.zhihu.com/question/404471029/answer/1590652261}
        \item 前面讲了很多直觉上的理解，不过想要深入研究强化学习的读者仅仅从直觉上理解是不够的，更细的探讨可以看《强化学习的数学原理》这份教程：\\
              \textit{强化学习的数学原理】课程：从零开始到透彻理解（完结） - 西湖大学WindyLab - Bilibili}\\
              \url{https://www.bilibili.com/video/BV1sd4y167NS}
    \end{itemize}
\end{tcolorbox}

\newpage