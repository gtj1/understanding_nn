\section{泛化性：一个矛盾}

\subsection{网络的大小好像小于训练数据？哪来的泛化性}

先回顾泛化性的概念：泛化性是一种举一反三的能力，我们期待模型在原始的数据集做完题之后再给它新的题目也能做的很好，可现实往往事与愿违。有时候模型连例题都没学会，这种“还没搞懂”的状态就叫做欠拟合；而有时候，模型把训练数据“背得滚瓜烂熟”，反而在新题面前一筹莫展——这就是过拟合。

在许多任务\footnote{许多任务：例如图像处理领域奠基性的文章之一\uhref{http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf}{《基于梯度的学习在文档识别中的应用》}{(\emph{Gradient-Based Learning Applied to Document Recognition})}。论文 11 页图 5 显示，论文使用的 LeNet-5 网络在大小为 60000 的训练集（总像素量大约 4700 万）上错误率降到了不足 1\%，而这一网络大小也仅仅 60000 左右。}中，神经网络的参数数量远小于训练样本的数据量，可它却能比我们想象得更好地泛化，甚至学会了我们没有显式教它的规律\footnote{显式教它：指手动编码特征告诉它什么特征对应什么对象。}。因此在此处我想抛出两个问题：一是泛化到底从何而来？二是我们凭什么相信它能举一反三？又或者说，我们对“举一反三”这件事的理解，本身是否过于天真？在给出解答之前让我举几个例子来帮助我们理解问题，读者也不妨在看的过程中自行思考。

从出生开始，身边大人们的交谈与各种媒体就让我们沉浸在语言环境当中。我们无需先理解“主谓宾”的结构或者词法搭配规则，却完全不影响我们不知不觉地从一张白纸开始，逐渐学会说出完整的句子。同时很显然的是，我们作为母语者并未系统学过语法，但在潜移默化中学习的效果好过拿着辞典和语法书的外国人\footnote{学习效果更好：或许这也可以作为机器学习中人工特征不如自动学习特征的一个旁证。}。

进入学校后，我们在听课之外还要靠练习来掌握各学科内容。中学时代的题目大多围绕考点精心挑选与组织——即便是新题，也多是老知识的排列组合。到了大学，考试当前我们也只是跟着老师划重点，翻一翻历年考题，做几道作业题，考试也就差不多了。我们未必做过所有题，但那些做过的题目，往往足以覆盖考试所需的知识范围。从结果上看，这种学习方式确实达成了目标：通过考试\footnote{达成了目标：从功利角度出发确实如此，但此言并不表明笔者完全认同这种模式。}。

但是课程考试之外呢？除了一些可以称之为“常识”的知识外，我们学过的大部分知识不再适用，需要重新学习。过往是否有好成绩与我们是否会做出一桌好菜几无关系，也并不能确保我们能写出好的论文。面对人生道路上的重要抉择，我们更需要沉思良久来反复斟酌利弊。人类科学的发展历程同样如此，那些有着明确解决方案的问题往往会快速发展成成熟的理论，标志着人类已经学会了“做这类题”。但是已有的经历并不能保证我们能顺利地解决一切新的问题——当一个问题已经解决时，这个问题就成为经验的一部分，不再是新问题了，因此新问题总是在研究的边界上或者远在边界以外。

这些道理非常简单，看起来不言自明。但是就在其中我们可以注意到做出题目至少要求一个人见过相似的问题——毕竟我们不能要求一个理科学生写出一篇好的文艺评论，同样不能要求一个文科学生能对数学分析张口就来。一类问题的背后总是一种普遍的规律，而不是随机的字符组合。正是隐含其中的共通特征让我们有可能在新的题目中用普遍性的方法提取出有用的信息，并按习得的特定方式处理它们。所谓的“灵光乍现”其实也是在某个语境长期的浸染下将一种感觉式的理解转化为一种明确的认知。

用统计的话语来讲，无论是人类的语言又或者是图像和其它媒介，都可以看作采样自各自的概率分布。不同的样本出现的概率并不相同：“今天晚上吃什么”出现的概率显然高于“我认为意大利面就应该拌42号混凝土”；一副人脸的照片出现的概率也远高于一张随机噪声图像，一个特定领域的语言或图像更是其中的一个子集。只要能保证做题和考试的题目来自同一个分布，我们就有理由相信模型能学会从分布中抽取出有用的信息，系统地解决这一个分布内的问题，而不仅仅是那些训练集中的问题。

于是前面的两个问题就有了答案：泛化性来自于数据分布中隐藏的规律。我们之所以相信模型能够举一反三，是因为我们相信：只要数据足够规律，模型就有能力学到其中的结构，而且有能力比我们人工编码的特征做得更好\footnote{比人工编码做的更好：事实上甚至比人类自己做的更好，现代的 AI 求解验证码的速度与准确度均超过了人类，见 \uhref{https://zhuanlan.zhihu.com/p/652027321}{《一觉睡醒，AI 破解验证码的速度比我还快了》}，而当今的验证码\uhref{https://www.zhihu.com/question/519996527/answer/3398147109}{实际上是用来验蠢的}。}。

顺着这个思路继续思考，我们会发现许多有趣的结论。以图像为例：在真实世界中，相邻像素的颜色往往相近，一个物体的颜色也通常只在小范围内波动。而从理论上说，一张 $224 \times 224$ 的 RGB 图像有 $256^{224 \times 224 \times 3}$ 种可能——这是一个天文数字。然而，几乎所有这些可能的图像都是无意义的噪声，真正有意义的图像只占极小一部分\footnote{有意义的数据点只占一部分：这通常称为\textoverset{Manifold Hypothesis}{(低维)流形假设}，假设认为多维的数据点实际上处于一个维度低的多的子空间中。}。

更进一步地，即便在这有限的一部分图像中，也有大量冗余信息。例如在识别动物时，图像整体左右平移几个像素，或略微缩放、旋转，并不会改变其类别。因此，理想情况下，一个模型应当学习的是图像中有用的、稳定的特征，而不是所有微小的细节。

这就好比我们在做回归分析时只关注回归方程，而希望剔除掉每个数据点的随机噪声。幸运的是，当数据量足够大时，噪声的影响往往会被平均掉\footnote{写给熟悉概率论的读者：若噪声独立同分布，样本均值估计的偏差将以 $O(n^{-1/2})$ 的速度减小，可参考大数法则证明。}。神经网络虽然远比线性回归复杂，但在很多实际场景下，我们仍然相信模型会在足够数据下自动“忽略噪声”，聚焦于结构性信息——而这也是它能泛化的又一重要原因。

推荐算法运行的基础也是如此：\uhref{https://www.bilibili.com/video/BV1mUbyzbEqi}{大数据是如何猜测用户喜好的呢}？每个用户可能喜欢某个视频，也可能不喜欢，让每个用户把每个视频都看一遍并打分显然不现实。光是从储存来看，记录上亿的用户观看上亿个视频的结果就已经超出了大多数系统的承受范围。所幸用户的选择并不是独立的，如果两个用户常常看相同的视频，那么他们的喜好也很可能相似，所以：对它使用低秩假设\footnote{\textoverset{Low-Rank Hypothesis}{低秩假设}：指的是可以找到一个低秩的矩阵来近似给定的矩阵，写成一竖一横两个长条形矩阵的乘积。如果读者对线性代数中的秩并不熟悉，只需要理解它表达的含义与前文“有效数据只占极小一部分”相同。}吧。

简单来说，视频有很多，用户也有很多。但视频可以按照内容打上若干标签，用户也可以按照兴趣分成若干类。标签的数量相比视频或者用户数量来说则是极少的，从数学上这可以表示为两个矩阵的乘积，不过从直觉上我想读者应该能理解：通过给用户分类可以得知他们可能喜欢哪些视频，从而相应地推荐给他们。这样一来，即使每个用户并没有看过每个视频的情况下，仍然能给出不错的推荐，新用户或新内容也能在少量互动后迅速归类。早期的谷歌搜索和一些平台的推荐算法确实是基于这样简单原理的，只是现在多半升级换代，已经替换为神经网络了，能更好地匹配用户个性化的兴趣，这相比给人贴上几个固定的标签确实人性化了很多。

事实上，我们人类本身也在做类似的事情。语言就是信息精简最典型的例子。当我说“我吃了一碗牛肉汤粉”，我并不需要告诉你碗的大小、汤粉的粗细、牛肉是煮的还是炖的，但对方依然能迅速理解，并还原出一个清晰的场景\footnote{还原出清晰的场景：这也是图片/视频生成 AI 能从少量的提示词生成占用储存空间更大的视频的基础。}。这种沟通的效率正是基于我们共享的生活经验和语言规则——用极少的文字精准传达出极其丰富的含义。

社会大生产也遵循类似的原则。现代工业之所以能以惊人的效率生产出复杂产品，靠的不是每个工人都具备全局视角和专业知识，而是把一个复杂任务分解成大量重复的小步骤。每位工人只需专注于几个标准化动作，经过极少的培训，也能熟练完成自己的部分。这种“复杂结构由低复杂度模块构成”的机制，本质上也是一种结构性的体现：我们不需要掌握系统的全貌，却能通过掌握其中规律性最强、重复性最高的部分，就能以少量的知识有效地参与到系统运行之中。

结构不止是自然存在于语言、图像、社会系统中的客观规律，它也可以被人为地“设计”和“构造”出来，而这往往源于一种根深蒂固的人类本能：我们总希望用更少的精力解决更多的问题。换句话说，“懒人”推动了世界的进步。

编程中的模板正是如此：在笔者看来，C++ 相比于 C 最伟大的发明是标准库和模板可以让程序员写出可以重复使用的代码，从而减轻人们的心智负担。作为对比，在 C 语言中，我们并非出于兴趣而反复实现链表、栈、树与图，而是因为它们足够好用，使得程序员可以借助这些结构“模板化”地解决各种复杂问题。而在 C++ 中，许多常见结构在标准库即可找到，例如排序算法并不需要为每种输入单独设计，只需遵循数据结构的抽象接口，即可复用于千万场景。学术写作中也能看到类似情况：许多期刊提供 LaTeX 模板，作者只需“填空”即可完成排版。这种模板化不仅减轻了排版负担，也通过统一风格提升了阅读效率——结构不仅服务于内容本身，也使传播、审阅与归档更加便利。

如何构造出结构化的数据也是现代机器学习研究的重点之一。以\uhref{https://arxiv.org/abs/2112.10752}{Stable Diffusion}为代表的\textoverset{Diffusion Model}{扩散模型}可以说是在生成领域的伟大尝试，既然随机生成的数据并无结构，那何不试着无中生有地构造出符合人类认知的图像？它的思路十分巧妙：先获得一些符合人类认识的图像并试图加入噪声，那么只要学习怎么去掉噪声就可以了，就这样无中生有地从噪声中“恢复”出了图像。更晚些的\textoverset{Flow Matching}{\uhref{https://arxiv.org/abs/2210.02747}{流匹配}}可以看作是系统化地完善了原有的扩散框架。从统计学上说，既然随机噪声与结构化图像之间最本质的差别是分布上的差异，那么结构化生成的问题就可以等价地理解为将一个随机的均匀分布迁移到另一个结构化分布的问题，将问题从样本的层面提升到了分布的层面。

不过，结构化并非总是无害。某些场景下，过度追求“模板化”反而可能引发反效果。初中几何问题里各种名字千奇百怪的模型就是“懒惰”催生出的范例：虽然它们确实在套路化的考试中有用，但笔者不甚喜欢这种把几何问题搞得如此神秘，最终变成人人要背一大堆模型的现状，因为它使人陷入套路化命题、套路化解题的怪圈。同时，现在图文模型的内容也常常因为许多细节过于相似而显得“千篇一律”，缺乏个性化的风格。当结构化生成成为唯一的手段，铺遍生活的方方面面，它也可能把苦心经营优质内容的创作者和细腻的人文关怀送入窒息的房间，最终带来语言、审美乃至情感能力的退化。

总而言之，虽然世界多样复杂，但无规律可循的混沌并不是常态，改造世界的英雄主义正是在认识到世界惊人的复杂性真相后依然坚信世界有迹可循、人定胜天\footnote{原句：世界上只有一种英雄主义,看清生活的真相依然热爱生活 ——罗曼·罗兰}。模型能泛化、人类能学习本质上都是因为数据中存在结构性规律。无论是语言、图像还是社会系统，这些结构都源于人类对世界的认知和经验积累。通过学习这些规律，我们可以在新的情境中应用已有的知识，从而实现“举一反三”，在大多时候这很有用，也确实推动着科技与生产力的不断发展。只是在优点大于缺点时，我们也不应该忽视其中的“次要矛盾”，在推崇效率、追求标准答案的同时，应该时常问问，结构之外还有什么可能？

\newpage

\subsection{再论过拟合}

前文已经提到，模型之所以能够泛化，是因为训练数据和测试数据来自同一个分布。换言之，数据并不是杂乱无章的，而是蕴含某种普遍性的规律。只要模型能捕捉到这些规律，就有可能在未见数据上准确判断。但这种拟合能力，既是泛化的基础，也可能成为“过拟合”的根源。

经验告诉我们同样的问题中，模型参数量越大越容易过拟合。神经网络学习的基础是梯度下降算法，因此过拟合还和训练的次数相关，通常不至于例题都没做明白就开始过拟合了。但如果在大参数与过度训练的共同作用下，参数足够大的模型在学会了“主要技能”后庞大的参数空间仍有余地，损失仍有一定的下降空间，模型便开始关注不必要的细枝末节。这些细节对于掌握规律来说毫无必要，可能反而带来误导，看似学得更好，实则学偏了\footnote{注：对人类而言，这种不断重复练习早已掌握的内容，浪费时间却收效甚微，甚至落入思维定势陷阱的做法有一个更加熟悉的名字——内卷。}。能不能有意识地检测出这样的迹象呢？答案是肯定的。

既然想用“合理”的测试评估模型是否在见过的数据以外，还理解了对数据分布中隐含的普适规律，就有了前提：训练和测试数据必须来自同一个分布才不致于教考分离。最稳妥的测试方法是在投入应用环境前，先在数据集内部测试，就像高考前有着数次的模拟考一样，有意识地检测出潜在的问题。不过训练的期间不能“偷看”这一部分测试的题目，否则见过的题目和测试用的题目就不独立了。自然想到将数据集分成两部分，随机挑出一部分留作测试，剩下的用于训练，训练过程中时常检测准确率，这样就可以在训练过程中监控模型的泛化能力，如果效果不错，就可以认为模型确实学到了东西。

为了减少随机误差，更严谨的方法还会多次测试，采用\textoverset{Cross Validation}{交叉验证}的方法来评估模型在不同数据子集上的表现。\textoverset{$k$-fold}{$k$ 折}交叉验证就是最经典的方法之一。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/K-fold_cross_validation_ZH-CN.png}
    \caption{$k$ 折交叉验证示意图}
    \uhref{https://commons.wikimedia.org/w/index.php?curid=82298768}{原图}作者：\uhref{https://commons.wikimedia.org/wiki/User:Gufosowa}{Gufosowa}, 遵循\uhref{https://creativecommons.org/licenses/by-sa/4.0}{CC BY-SA 4.0}，有翻译修改
    \label{fig:kfold-cross-validation}
\end{figure}

以 LeNet-5 网络\footnote{说明：LeNet 采用了卷积结构，不过具体原理先按下不表，下面出现 ResNet 时读者同样暂时无需关心其具体结构，只需要知道它是一个更为复杂、强大的神经网络即可。}处理 MNIST 数据集\footnote{MNIST 数据集：一个经典的手写数字识别数据集，包含 60000 张训练图像和 10000 张测试图像。}为例，由于它已经预先划分好了训练集和测试集，我们便无需再手动选择测试样本。可以在训练过程中定期评估模型在测试集上的表现，观察训练准确率与测试准确率的变化\footnote{注：代码可以在 \texttt{examples/fitting\_mnist.py} 中找到。}\;\footnote{训练的过程：前文叙述过梯度下降的基本原理，这里的一次梯度下降实际上是一\textoverset{Batch}{批次}的数据梯度的平均，以批次为单位训练可以充分利用 GPU 的并行计算优势，计算更快，而且数值上更加稳定。这样当经过 1000 个批次时模型就见过了 $32\times 1000 = 32000$ 条数据。}\;\footnote{定期评估：为了节省时间，每过 5 个批次的数据才检测一次准确率，每次仅随机抽取 100 个样本计算准确率，并在前一次准确率的基础上做了适当的平滑处理。阴影的区域是根据统计方法得到实际准确率的 95\% 置信区间。（读者如果不熟悉概率论与统计只需要认为实际值不太可能在着色的区域之外）。}。例如在下图中二者的变化基本相同，而且准确率快速达到了 95\% 以上，表明模型确实学习到了数据分布中通用的规律。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/lenet5_mnist.png}
    \caption{LeNet-5 在 MNIST 数据集上的训练与测试准确率变化}
    \label{fig:fitting-accuracy}
\end{figure}

然而当我们换一个数据集时，情况就不同了。在 CIFAR-10 数据集\footnote{CIFAR-10 数据集：一个包含 60000 张 $32 \times 32$ 彩色图像的通用图像分类数据集，分为 10 个类别，其中 50000 张用于训练，10000 张用于测试。这也是一个经典数据集，但是其内容复杂的多，直到 2015 年，又一个机器视觉领域奠基性的神经网络 \uhref{https://arxiv.org/abs/1512.03385}{ResNet} 才第一次成功将准确率提升到 90\% 以上。}上测试\footnote{注：代码与前一份类似，可以在 \texttt{examples/fitting\_cifar10.py} 中找到，包含了 LeNet-5，ResNet-10，ResNet-18 三种网络结构，不过运行代码需要使用显卡与较长的运行时间。}，并继续使用 LeNet-5，网络的表现就不尽如人意了。与手写数字识别不同的是，这里的图像包含了更多的颜色和细节，类别之内的多样性更高。如下图可以看到，在训练了 20000 轮之后，即使是在训练集上的准确率也仅仅 60\%，而且显然测试集上的精确率低一些。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/lenet5_cifar10.png}
    \caption{LeNet-5 在 CIFAR-10 数据集上的训练与测试准确率变化}
    \label{fig:lenet5-cifar10-accuracy}
\end{figure}

显然，LeNet-5 的效果并不理想。读者可能会想，这个网络不行，能不能换一个好一点的网络啊？那么就这么试试吧。LeNet-5 毕竟是上个世纪的“老前辈”了，那时毕竟没有如此成熟的深度学习框架和计算资源。相比之下 ResNet 年轻许多，说不定能做得更好呢。于是我们得到了这样的结果：

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/resnet10_cifar10.png}
    \caption{ResNet-10 在 CIFAR-10 数据集上的训练与测试准确率变化}
    \label{fig:resnet10-cifar10-accuracy}
\end{figure}

从绝对的数值看来，确实好了许多。在训练集上的准确率跑到了 98\%，远远好过原本 60\% 的准确率。在测试集上的准确率也来到了 73\%，比 LeNet-5 在及格线边缘反复跳跃的分数要好得多。从这方面看，ResNet-10 确实是一个更好的网络。

但是如果读者的注意力足够敏锐，便会发现一个不对劲的地方：测试集和训练集的准确率比起来未免差距太大。开始训练时二者还基本贴合，但是一段时间之后有了分开的迹象。4000 轮之后，测试集上的分数就已饱和，训练集上的分数却还在不断上升。如果仅仅关注训练集上的结果，我们会形成一种错觉，似乎模型已经学会了所有的内容。实际上测试集上的准确率却并没有跟上来，只是在平均线的附近波动，这是典型的过拟合。

从测试集上我们大约可以看出，训练集几乎全对的成绩有七成的是真正学习到的可泛化知识贡献的，剩下的三成则是学习了太多的细节与噪声所致。所幸这些噪声只是让它在训练集上的分数虚高，而没有在测试集上起到反作用，反向降低分数。

然而即使分数没有下降，这后面 80\% 的宝贵算力和时间也都白白浪费了。我们没有办法确保一个模型总是能够学到十全十美，但是至少我们应该常常检视自己是否在做无用功，并及时止损，将时间投入到更有价值的事物中。当我们检测到训练集的准确率仍在显著上升但是这种训练对测试集没有提高时，训练就应该停止了，这在深度学习中常常被称作\textoverset{Early Stopping}{早停}的训练策略——顾名思义，持续一段时间发现训练对测试结果没有提升就停止训练。

这时读者会好奇，如果再换个更强大的网络呢？它的过拟合会缓解还是加重呢？于是，这是换成 ResNet-18 的结果：

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/resnet18_cifar10.png}
    \caption{ResNet-18 在 CIFAR-10 数据集上的训练与测试准确率变化}
    \label{fig:resnet18-cifar10-accuracy}
\end{figure}

这与 ResNet-10 的结果几乎没有区别，除了因为 ResNet-18 的网络结构更为复杂，参数量也更多，训练的速度更慢一些之外，丝毫没有改变预期内结果和过拟合结果的七三分现状。毕竟多项式拟合的经验已经告诉我们，模型的复杂度越高，越容易出现过拟合。

这样严重的过拟合应当引起警觉，但是既然如前文所言，它们抽样自同一分布，从统计上的分布是相同的，何至于差别如此之大呢？又该如何解决它们带来的影响呢？在解决这个问题之前，不妨先思考人类是怎样排除这样随机因素的影响的。与神经网络不同的是，人类社会早已有了应对策略。于是我想就此简单谈谈统计学中\textoverset{Significance Test}{显著性检验}的意义。

大数定理告诉我们，随着样本量趋向无穷，频率会收敛\footnote{收敛：这里指\textoverset{Convergence in Probability}{依概率收敛}，简单说，就是样本越多，结果越接近真实情况，严谨定义参见概率论教科书或百科。}到真实概率，样本均值会收敛到真实期望。但是世界并不理想——我们没有无穷多的数据，但仅因样本有限，我们就永不可能得到可靠的结论吗？当然不是。统计学告诉我们，即使样本有限，我们也可以得到“足够可靠”的结论。当然，同时统计也会告诉我们一些结论是“不够可靠”的。那么什么是“可靠”呢？答曰：\textoverset{Statistical Significance}{统计显著性}。

我们常常凭直觉推测事情背后的原因，但由于因果关系难以直接验证，仅靠直觉很难说服他人这一推测就是事实\footnote{很难说服他人：至少很难说服论文审稿人。}。直行不通时，需要采用一些迂回的方法：当你排除掉所有不可能的情况后，无论剩下的多么难以置信，那也一定是真相\footnote{注：语出阿瑟·柯南·道尔(Arthur Conan Doyle)所著《福尔摩斯探案集》的《四签名》(\emph{Sherlock Holmes, The Sign of the Four})。}。如果要说明市场对 A 产品和 B 产品的喜好有区别\footnote{注：统计学中这样待证明的假设称为\textoverset{Alternative Hypothesis}{备择假设}$H_1$。}，并不直接比较 A 和 B 的销量，而是反过来先假设市场并无明显偏好\footnote{注：统计学中这样待证明的假设称为\textoverset{Null Hypothesis}{原假设}$H_0$。}，再说明实际数据在这一情况下是否显得异常：检验这一假设下，有多大可能\footnote{注：原假设下比实测值更极端的概率通常称为 $p$ 值。$p$ 值越小，越意味着实际结果在原假设下难以出现，从而为备择假设提供支持。}与实际数据相符或比它更极端。如果数据极端到足以让人怀疑原假设的成立，那么那个希望证明的假设就应该确实成立\footnote{“应该确实成立”：比较严谨的说法是“备择假设 $H_1$ 获得统计支持”。}，正体现了统计推断中一种典型的逆向思维。

且看一个简单的例子：假设你是一个甜品店的店主，你的店里有草莓和巧克力两种风味的冰淇凌，今天卖出了 100 份冰淇凌，其中 58 份是草莓口味，42 份是巧克力口味。由于这是第一天营业，你并不知道顾客的口味偏好，但是这确实会影响你以后进货要进多少。乍一看，草莓口味的销量似乎更好一些，但这是否足以说明顾客更喜欢草莓口味呢？就在你准备下定论时，一个声音出现在你脑中：我是说，有没有一种可能，其实顾客的喜好差不多，只是今天刚好买草莓味的多了一些？你心想，这不应该吧，毕竟 58 和 42 差距还是挺大的。但那道声音又冒出来了：那你就来算算看。

于是你假设每个人都有相同的概率在两种冰淇凌中二选一，且每个人的选择互不影响，在此种情况下设有 $X$ 个人会选择草莓冰淇凌。显然由于概率对半开，$X$ 的期望是 50，也就是说最“不极端”的情况是 $X=50$ 刚好成立。既然认为 58 比较极端，那么你就想知道没有偏好假设的前提下，偏离期望值至少 8 份的概率（即 $\mathrm{P}(|X - 50| \geq 8)$）有多大。使用一点点概率论的知识\footnote{注：熟悉概率论的读者容易看出它服从二项分布 $\mathrm{B}(100, 0.5)$，然后直接将那些“更极端”的情况概率相加，或者使用借助均值和方差近似为正态分布（这种方法可能存在微小误差）都容易计算出结果，此处将不展示计算细节。}，你算出概率 $\mathrm{P}(|X - 50| \geq 8)$ 大约是 13.3\%。相比于 Fisher\footnote{注：Ronald Aylmer Fisher (1890–1962)，英国统计学家，现代统计学的奠基人之一。} 大手一挥写下的推荐标准 5\%\footnote{注：Fisher 建议的标准是 5\%，不过他实际上也指出这一标准应当根据具体情况灵活调整。} 来看并不算小。换句话说，假设顾客的喜好并无差别，那么在 100 份里面比今天偏离中线 8 份或者更多的情况大约每 7 次就会发生一次，并不罕见，也可能只是今天碰巧如此。

到了第二天，你又卖出了 100 份冰淇凌，这次草莓口味的销量是 56 份，巧克力口味的销量是 44 份。虽然从这个结果看，比第一天还更平均了一些。但是你想了想，如果把两天的结果叠加起来看呢？这次你假设两天的 200 份冰淇凌中有 $Y$ 份是草莓口味的，$58+56=114$，所以你计算出 $\mathrm{P}(|Y-100|\ge 14) = 4.0\%$ 的结果。如果假设 5\% 以下的小概率事件没有发生，那么你就可以认为顾客的喜好确实偏向草莓口味\footnote{注：严谨的说法写作在 5\% 的显著性水平下可以拒绝 “$H_0$：用户对两种口味并无偏好”这一命题}。

假设检验作为论证统计显著性的核心方法，已经支持了无数社会科学与医学等领域的研究。它系统性地指导着研究者如何对抗不确定性和随机性，足以防止研究因误读样本中的随机偏差而被误导。同时也以脑中一句“你的假设检验呢？”警示那些希望通过胡乱分析水论文的人。最终如大浪淘沙一般，筛去了数据中的噪声\footnote{筛去噪声：指让我们少犯统计学中的第 I 类错误，即排除了实际上成立的假设。例如某药在实验中实际上并没有用，但是因为数据中的随机因素导致准确率高于对照组（不用药）。如果未经假设检验就很容易排除掉药实际无效的可能，误认为治疗效果与是否用药相关。}，淘洗出真正靠得住的结论。这种对样本中随机性持保留态度的思想，也正是我们在机器学习中所需要追求的。讲到这里，是时候把目光移回神经网络了。面对同样的信息随机性，它们无法有意识地通过假设检验来“证明”某个规律的真实性，那又该如何实现相同的目标：避免过拟合、发掘真正有效的信息呢？

回看前面的历程，原假设总是那个最自然的状态：两件事无直接关系是常态，有关系才值得我们证明，而不是先入为主地假设关系存在，而后才去证明二者无关——这几乎不可能，因为两件事无论看起来再怎么独立，也总是无法排除那些关系微存的可能。这也是无罪推定的原则：我们只能在假设无罪的前提下导出矛盾进而证明有罪，而不能先假设有罪。这通常被称为\textoverset{Occam's Razor}{奥卡姆剃刀}原理，经典的叙述为“如无必要，勿增实体”，它告诉我们应当删繁就简，虽然复杂的假设一定是错的，但应当优先选择简单的解释。即使是相对论时空观这一比伽利略时空观更符合现实的理论，也需要通过大量的实验\footnote{大量的实验：包括 1919 年日食观测验证、1925 年对引力红移的观测等。}被反复证实才能被科学界普遍接受。这并不是说学界不愿意接受新理论，恰恰相反，在这一过程中科学家们保持了高度谨慎的态度，正是这种谨慎不断推动着科学研究的进步。相对论被接受的过程都是如此，那么我们没有理由不在训练神经网络时谨慎一些，让它在拟合的同时保持对复杂假设的怀疑。

同样地，在神经网络中，我们不应轻易接受复杂模型带来的结果，正如统计中不轻易接受复杂假设。为了选择更自然的假设，应该让它尽在拟合的同时可能接近“自然状态”。什么是“自然状态”呢？更小、更简单的网络显然比大的网络更自然，所以应该优先选择小网络。然而前文已经验证过小网络有时无法胜任复杂的任务，即使在训练集上分数都不能跑到较高的水平。在这样的前提下就需要讨论，对于一个固定的网络而言，如何才是更“自然”的状态呢？

从内部结构看来，网络由大量参数构成，网络结构固定时，这种自然性就由权重的简约性体现出来。而从运行结果看来，不难想到不响应那些零星出现、不具代表性的特征才是自然。在忘掉一切特征的情况下，最自然的网络参数已逐渐明晰，它就是——全零的网络。它不会响应任何的数据，不管是偶然的噪声或是有意义的信息。虽然全零状态本身无意义，但向它靠近时模型确实会遗忘掉那些并不普遍的特征，只有足够普遍的特征能被反复强化，与遗忘的力量对抗，留在参数的记忆当中。在全零参数的完全欠拟合与复杂参数的过拟合间寻找平衡正是正则化的意义所在。

\newpage

\subsection{大模型的世界——域外泛化、幻觉与消失的过拟合}