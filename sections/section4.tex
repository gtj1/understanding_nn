\section{泛化性：一个矛盾}

\subsection{网络的大小好像小于训练数据？哪来的泛化性}

先回顾泛化性的概念：泛化性是一种举一反三的能力，我们期待模型在原始的数据集做完题之后再给它新的题目也能做的很好，可现实往往事与愿违。有时候模型连例题都没学会，这种“还没搞懂”的状态就叫做欠拟合；而有时候，模型把训练数据“背得滚瓜烂熟”，反而在新题面前一筹莫展——这就是过拟合。

在许多任务\footnote{许多任务：例如图像处理领域奠基性的文章之一\uhref{http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf}{《基于梯度的学习在文档识别中的应用》}{(\emph{Gradient-Based Learning Applied to Document Recognition})}。论文 11 页图 5 显示，在大小为 60000 的训练集（总像素量大约 4700 万）上错误率降到了不足 1\%，而这一网络大小也仅仅 60000 左右。}中，神经网络的参数数量远小于训练样本的数据量，可它却能比我们想象得更好地泛化，甚至学会了我们没有显式教它的规律\footnote{显式教它：指手动编码特征告诉它什么特征对应什么对象。}。因此在此处我想抛出两个问题：一是泛化到底从何而来？二是我们凭什么相信它能举一反三？又或者说，我们对“举一反三”这件事的理解，本身是否过于天真？在给出解答之前让我举几个例子来帮助我们理解问题，读者也不妨在看的过程中自行思考。

从出生开始，身边大人们的交谈与各种媒体就让我们沉浸在语言环境当中。我们无需先理解“主谓宾”的结构或者词法搭配规则，却完全不影响我们不知不觉地从一张白纸开始，逐渐学会说出完整的句子。同时很显然的是，我们作为母语者并未系统学过语法，但在潜移默化中学习的效果好过拿着辞典和语法书的外国人\footnote{学习效果更好：或许这也可以作为机器学习中人工特征不如自动学习特征的一个旁证。}。

进入学校后，我们在听课之外还要靠练习来掌握各学科内容。中学时代的题目大多围绕考点精心挑选与组织——即便是新题，也多是老知识的排列组合。到了大学，考试当前我们也只是跟着老师划重点，翻一翻历年考题，做几道作业题，考试也就差不多了。我们未必做过所有题，但那些做过的题目，往往足以覆盖考试所需的知识范围。从结果上看，这种学习方式确实达成了目标：通过考试\footnote{达成了目标：从功利角度出发确实如此，但此言并不表明笔者完全认同这种模式。}。

但是课程考试之外呢？除了一些可以称之为“常识”的知识外，我们学过的大部分知识不再适用，需要重新学习。过往是否有好成绩与我们是否会做出一桌好菜几无关系，也并不能确保我们能写出好的论文。面对人生道路上的重要抉择，我们更需要沉思良久来反复斟酌利弊。人类科学的发展历程同样如此，那些有着明确解决方案的问题往往会快速发展成成熟的理论，标志着人类已经学会了“做这类题”。但是已有的经历并不能保证我们能顺利地解决一切新的问题——当一个问题已经解决时，这个问题就成为经验的一部分，不再是新问题了，因此新问题总是在研究的边界上或者远在边界以外。

这些道理非常简单，看起来不言自明。但是就在其中我们可以注意到做出题目至少要求一个人见过相似的问题——毕竟我们不能要求一个理科学生写出一篇好的文艺评论，同样不能要求一个文科学生能对数学分析张口就来。一类问题的背后总是一种普遍的规律，而不是随机的字符组合。正是隐含其中的共通特征让我们有可能在新的题目中用普遍性的方法提取出有用的信息，并按习得的特定方式处理它们。所谓的“灵光乍现”其实也是在某个语境长期的浸染下将一种感觉式的理解转化为一种明确的认知。

用统计的话语来讲，无论是人类的语言又或者是图像和其它媒介，都可以看作采样自各自的概率分布。不同的样本出现的概率并不相同：“今天晚上吃什么”出现的概率显然高于“我认为意大利面就应该拌42号混凝土”；一副人脸的照片出现的概率也远高于一张随机噪声图像，一个特定领域的语言或图像更是其中的一个子集。只要能保证做题和考试的题目来自同一个分布，我们就有理由相信模型能学会从分布中抽取出有用的信息，系统地解决这一个分布内的问题，而不仅仅是那些训练集中的问题。

于是前面的两个问题就有了答案：泛化性来自于数据分布中隐藏的规律。我们之所以相信模型能够举一反三，是因为我们相信：只要数据足够规律，模型就有能力学到其中的结构，而且有能力比我们人工编码的特征做得更好\footnote{比人工编码做的更好：事实上甚至比人类自己做的更好，现代的 AI 求解验证码的速度与准确度均超过了人类，见 \uhref{https://zhuanlan.zhihu.com/p/652027321}{《一觉睡醒，AI 破解验证码的速度比我还快了》}，而当今的验证码\uhref{https://www.zhihu.com/question/519996527/answer/3398147109}{实际上是用来验蠢的}。}。

顺着这个思路继续思考，我们会发现许多有趣的结论。以图像为例：在真实世界中，相邻像素的颜色往往相近，一个物体的颜色也通常只在小范围内波动。而从理论上说，一张 $224 \times 224$ 的 RGB 图像有 $256^{224 \times 224 \times 3}$ 种可能——这是一个天文数字。然而，几乎所有这些可能的图像都是无意义的噪声，真正有意义的图像只占极小一部分\footnote{有意义的数据点只占一部分：这通常称为\textoverset{Manifold Hypothesis}{(低维)流形假设}，假设认为多维的数据点实际上处于一个维度低的多的子空间中。}。

更进一步地，即便在这有限的一部分图像中，也有大量冗余信息。例如在识别动物时，图像整体左右平移几个像素，或略微缩放、旋转，并不会改变其类别。因此，理想情况下，一个模型应当学习的是图像中有用的、稳定的特征，而不是所有微小的细节。

这就好比我们在做回归分析时只关注回归方程，而希望剔除掉每个数据点的随机噪声。幸运的是，当数据量足够大时，噪声的影响往往会被平均掉\footnote{写给熟悉概率论的读者：若噪声独立同分布，样本均值估计的偏差将以 $O(n^{-1/2})$ 的速度减小，可参考大数法则证明。}。神经网络虽然远比线性回归复杂，但在很多实际场景下，我们仍然相信模型会在足够数据下自动“忽略噪声”，聚焦于结构性信息——而这也是它能泛化的又一重要原因。

推荐算法运行的基础也是如此：\uhref{https://www.bilibili.com/video/BV1mUbyzbEqi}{大数据是如何猜测用户喜好的呢}？每个用户可能喜欢某个视频，也可能不喜欢，让每个用户把每个视频都看一遍并打分显然不现实。光是从储存来看，记录上亿的用户观看上亿个视频的结果就已经超出了大多数系统的承受范围。所幸用户的选择并不是独立的，如果两个用户常常看相同的视频，那么他们的喜好也很可能相似，所以：对它使用低秩假设\footnote{\textoverset{Low-Rank Hypothesis}{低秩假设}：指的是可以找到一个低秩的矩阵来近似给定的矩阵，写成一竖一横两个长条形矩阵的乘积。如果读者对线性代数中的秩并不熟悉，只需要理解它表达的含义与前文“有效数据只占极小一部分”相同。}吧。

简单来说，视频有很多，用户也有很多。但视频可以按照内容打上若干标签，用户也可以按照兴趣分成若干类。标签的数量相比视频或者用户数量来说则是极少的，从数学上这可以表示为两个矩阵的乘积，不过从直觉上我想读者应该能理解：通过给用户分类可以得知他们可能喜欢哪些视频，从而相应地推荐给他们。这样一来，即使每个用户并没有看过每个视频的情况下，仍然能给出不错的推荐，新用户或新内容也能在少量互动后迅速归类。早期的谷歌搜索和一些平台的推荐算法确实是基于这样简单原理的，只是现在多半升级换代，已经替换为神经网络了，能更好地匹配用户个性化的兴趣，这相比给人贴上几个固定的标签确实人性化了很多。

事实上，我们人类本身也在做类似的事情。语言就是信息精简最典型的例子。当我说“我吃了一碗牛肉汤粉”，我并不需要告诉你碗的大小、汤粉的粗细、牛肉是煮的还是炖的，但对方依然能迅速理解，并还原出一个清晰的场景\footnote{还原出清晰的场景：这也是图片/视频生成 AI 能从少量的提示词生成占用储存空间更大的视频的基础。}。这种沟通的效率正是基于我们共享的生活经验和语言规则——用极少的文字精准传达出极其丰富的含义。

社会大生产也遵循类似的原则。现代工业之所以能以惊人的效率生产出复杂产品，靠的不是每个工人都具备全局视角和专业知识，而是把一个复杂任务分解成大量重复的小步骤。每位工人只需专注于几个标准化动作，经过极少的培训，也能熟练完成自己的部分。这种“复杂结构由低复杂度模块构成”的机制，本质上也是一种结构性的体现：我们不需要掌握系统的全貌，却能通过掌握其中规律性最强、重复性最高的部分，就能以少量的知识有效地参与到系统运行之中。

结构不止是自然存在于语言、图像、社会系统中的客观规律，它也可以被人为地“设计”和“构造”出来，而这往往源于一种根深蒂固的人类本能：我们总希望用更少的精力解决更多的问题。换句话说，“懒人”推动了世界的进步。

编程中的模板正是如此：在笔者看来，C++ 相比于 C 最伟大的发明是标准库和模板可以让程序员写出可以重复使用的代码，从而减轻人们的心智负担。作为对比，在 C 语言中，我们并非出于兴趣而反复实现链表、栈、树与图，而是因为它们足够好用，使得程序员可以借助这些结构“模板化”地解决各种复杂问题。而在 C++ 中，许多常见结构在标准库即可找到，例如排序算法并不需要为每种输入单独设计，只需遵循数据结构的抽象接口，即可复用于千万场景。学术写作中也能看到类似情况：许多期刊提供 LaTeX 模板，作者只需“填空”即可完成排版。这种模板化不仅减轻了排版负担，也通过统一风格提升了阅读效率——结构不仅服务于内容本身，也使传播、审阅与归档更加便利。

如何构造出结构化的数据也是现代机器学习研究的重点之一。以\uhref{https://arxiv.org/abs/2112.10752}{Stable Diffusion}为代表的\textoverset{Diffusion Model}{扩散模型}可以说是在生成领域的伟大尝试，既然随机生成的数据并无结构，那何不试着无中生有地构造出符合人类认知的图像？它的思路十分巧妙：先获得一些符合人类认识的图像并试图加入噪声，那么只要学习怎么去掉噪声就可以了，就这样无中生有地从噪声中“恢复”出了图像。更晚些的\textoverset{Flow Matching}{\uhref{https://arxiv.org/abs/2210.02747}{流匹配}}可以看作是系统化地完善了原有的扩散框架。从统计学上说，既然随机噪声与结构化图像之间最本质的差别是分布上的差异，那么结构化生成的问题就可以等价地理解为将一个随机的均匀分布迁移到另一个结构化分布的问题，将问题从样本的层面提升到了分布的层面。

不过，结构化并非总是无害。某些场景下，过度追求“模板化”反而可能引发反效果。初中几何问题里各种名字千奇百怪的模型就是“懒惰”催生出的范例：虽然它们确实在套路化的考试中有用，但笔者不甚喜欢这种把几何问题搞得如此神秘，最终变成人人要背一大堆模型的现状，因为它使人陷入套路化命题、套路化解题的怪圈。同时，现在图文模型的内容也常常因为许多细节过于相似而显得“千篇一律”，缺乏个性化的风格。当结构化生成成为唯一的手段，铺遍生活的方方面面，它也可能把苦心经营优质内容的创作者和细腻的人文关怀送入窒息的房间，最终带来语言、审美乃至情感能力的退化。

总而言之，虽然世界多样复杂，但无规律可循的混沌并不是常态，改造世界的英雄主义正是在认识到世界惊人的复杂性真相后依然坚信世界有迹可循、人定胜天\footnote{原句：世界上只有一种英雄主义,看清生活的真相依然热爱生活 ——罗曼·罗兰}。模型能泛化、人类能学习本质上都是因为数据中存在结构性规律。无论是语言、图像还是社会系统，这些结构都源于人类对世界的认知和经验积累。通过学习这些规律，我们可以在新的情境中应用已有的知识，从而实现“举一反三”，在大多时候这很有用，也确实推动着科技与生产力的不断发展。只是在优点大于缺点时，我们也不应该忽视其中的“次要矛盾”，在推崇效率、追求标准答案的同时，应该时常问问，结构之外还有什么可能？

\newpage

\subsection{再论欠拟合与过拟合}

\subsection{训练好像被卡住了——Scaling Law}