#import "../../config.typ": *
#import "section1_plot.typ": *

== 多项式拟合

#figure(
  image("../../../img/polynomial_fitting.png", width: 60%),
  caption: [多项式拟合示意图（图为 3 次拟合） #linebreak() 图源：#link("https://www.geeksforgeeks.org/numpys-polyfit-function-a-comprehensive-guide/", [GeeksforGeeks])]
)

#h2 线性拟合虽然很好，但是如果拿到了明显不线性的一堆数据，那么线性拟合就显得有些力不从心了。不过既然都是拟合，能做一次的那按理来讲也能做多次。#textOverSet("多项式拟合", "Polynomial Fitting")就是这样一种思路，只是预测 $hat(y)$ 从 $k x + b$ 变成了 $a_0 + a_1 x + ... + a_m x^m$#footnote[记号说明：虽然习惯上幂次从大到小排列，但是为了下标和幂次的统一性，所以这里选择从常数项到最高次项排列]，其中 $m$ 是多项式的次数。而均方误差的表达式甚至几乎不用变，仍然是
$ "MSE" = 1/n sum_(i=1)^n (y_i - hat(y))^2 $

只不过展开后是一系列的多项式项，待拟合的参数从两个变成了 $m+1$ 个。但是如果观察一下，这个式子仍然是一个（多变量的）二次函数，所以最小化的方法也是一样的。多项式自有多项式的好，能加的项多了，拟合的灵活性也就大了，误差显然会更小。然而与线性拟合相比，它虽然有#textOverSet("解析解", "Analytical Solution")，但不再像线性拟合一样可以逐项明确说出意义，而是只剩下一堆矩阵运算把这些参数算出来。在这个情况下，相比于记下公式，形成一个整体上的印象显得尤为重要。

上一小节中，我们从图像看到了这种拟合的几何解释，而多项式拟合也是相似的，还是从 $bold(r)$ 的表达式入手
$ bold(r) = bold(y) - (a_0 bold(x)^0 + a_1 bold(x)^1 + ... + a_m bold(x)^m) $

对比之前的表达式，当 $a_0, a_1, ..., a_m$ 变化时，预测得到的结果 $hat(bold(y)) = a_0 bold(x)^0 + a_1 bold(x)^1 + ... + a_m bold(x)^m$ 也会在一个 $m + 1$ 维的空间中变化，正如之前的平面，这个空间也是一个 $m + 1$ 维的子空间。求最小模的 $bold(r)$ 又回到了从点到子空间的垂线问题。虽然不得不承认：想象从一个高维的 $n$ 维空间中向 $m+1$ 维的子空间做垂线确实有些困难，但是这多少离我们的几何直觉更近了一些。

系数的意义不那么明确了，但是误差下来了，这是好事吗？也不一定，灵活性的另一面是潜在的#textOverSet("过拟合", "Overfitting")。前文中做线性拟合的时候有一个重要的假设是测量得到数据带有一定的误差。拟合的直线滤去了大部分的误差，留下了重要的趋势。但是如果灵活性太高，拟合的多项式会过于贴合数据，甚至把误差也拟合进去了。即使在给定的数据上做到了很小的误差，预测新数据的能力却可能会大打折扣。

拿做题打个比方：使用直线拟合明显不线性的数据是方法错了，只能说是没完全学会。但是用接近数据量的参数来拟合数据，留给它的空间都够把结果"背下来"了，捕捉到了数据的细节，却忽略了数据背后的规律，化成了一种只知道背答案的自我感动。在几道例题上能做到滴水不漏，但是一遇到新题就束手无策。

举个例子，在下面这个数据集上试图拟合，我们在二次函数 $y = 0.25 x^2 - x + 1$ 上添加了标准正态分布的噪声，即实际上 $y = 0.25 x^2 - x + 1 + cal(N)(0, 1)$ #footnote[$cal(N)(0, 1)$：表示一个服从#link("https://baike.baidu.com/item/%E6%A0%87%E5%87%86%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83", [标准正态分布])的变量，均值为 0，方差为 1]。

#data_and_noise

#h2 那么现在我们来试试用不同次数的多项式拟合这个数据集。不难看出线性拟合的线与数据点还是相差不少，因为它没能提供可以制造数据"弯曲"形状的项，它没能捕捉到数据更加复杂的趋势，这种现象称为#textOverSet("欠拟合", "Underfitting")。2 次曲线的效果几乎和真实曲线一样，即使提升到 3 次也没有太明显的改变，它们拟合的效果都还算好。

#fitting_3

但是如果继续增加次数呢？先来看看十次的拟合效果。

#fitting_10

你可能会想，虽然是稍微歪了一点，不过这看起来还行吧。但是如果你把 $x$ 的范围稍微扩大一点，你就会发现势头完全不对了。

#fitting_10_large

一旦离开了拟合的区域，十次拟合的曲线就直勾勾地弯向无穷远，这是因为它把噪声也拟合进去了，从而给出了#textOverSet("Generalization", "泛化性")#footnote("泛化性：预测原有数据集以外点的能力")极差的结果。这就是过拟合的危害。因为参数量与样本点数量并没有非常显著的差别（10 个参数，100 个样本点），所以从去噪声的角度看，结果过拟合并不奇怪——过滤掉噪声需要更多的数据。

当然解决办法并不是没有，要解决问题先要找到问题的根源。既然得到的函数行为不符合预期，那么很自然地我们会想问，这个函数的系数怎么样呢？在上面这个具体的例子中，函数的表达式是

$
          hat(y) & = -0.000004129005667 x^10 + 0.000200033877258 x^9 - 0.004061827595427 x^8   \
               & quad + 0.044810202155712 x^7 - 0.291097682876070 x^6 + 1.129425113256322 x^5 \
               & quad - 2.542208192992861 x^4 + 3.091776048493755 x^3 - 1.584353162512058 x^2 \
               & quad - 0.267618886184698 x + 0.362912675589959
$

简单估算一下就会发现，例如 3, 4 次项的系数都在个位数级别，再乘以 $x$ 的 3 次方、4 次方数值就会变得很大。10 次方项的系数看起来只有 $4.1 times 10^-6$，但是乘上 $bold(x)$ 中最大值的 10 次方，也就是 $10^10$ 后，这个数值同样会飙升到上万的级别。一堆上万级别的数加在一起，倒不如说顶着舍入误差#footnote("舍入误差：就像手动计算时保留几位小数一样，计算机计算的并不是“实数”，而是具有一定精度的浮点数，同样也有误差。例如对于 32-bit 的浮点数，只能精确到 7 个十进制位，这意味着从万位向后数到第 7 位，从百分位就可能已经不准确，在此之后的数位就不太可靠了。")还能够回归到原来的数据集上已经是奇迹了。也就只有 MSE 可以限制一下它在数据集内的行为，出了预定义的范围，这个高次函数大概就放飞自我了。

不过如果一定要用高次函数，补救的办法也不是没有。既然这些系数导致了很大的数值，那限制一下这些数值就好了，这就是
#textOverSet("正则化", "Regularization")
的思路。我们在待优化的函数上加上一个
#textOverSet("惩罚项", "Penalty Term")
，同样地使用平方求和的形式，只不过这次是对系数进行惩罚，为了让系数尽量小，即尽量贴近于 $0$，自然想到把它们乘以权重后的平方也加起来，优化的目标
#footnote[Loss：损失，与前文单纯使用 MSE 时相同，我们希望让它尽可能小，它的每一项包含了我们对拟合结果的一个美好“祝愿”，MSE 项希望它误差减小，正则化项希望它系数正常。]
变成了


$
  "Loss" = "MSE" + underbrace(sum_(i=1)^10 (mu_i a_i)^2, "正则化项")
$

可调参数 $mu_i$ 表明我们希望在多大程度上抑制每个系数，在这个例子中不同次项的权重可能并不相同，不过在后续拟合的许多例子中这个会使用统一的权重，即所有的 $mu_i$ 均为相同的定值 $mu$，式子从而变为了一个常数 $lambda = mu^2$ 倍的平方和。为每一项设置分立的系数是为了解决一个致命的问题：不同系数对最终结果的影响可能不同。例如在 $x=10$ 这一点上，$x^10$ 项的系数对结果的影响远远大于 $x$ 项的系数，即使是 $4.1times 10^-6$ 这样微小的 $10$ 次项系数也会导致非常大的数值。平方后这一系数变得十分微小，原本用于约束系数大小的正则化项对它的影响更是微乎其微。

不过即使我们使用了相同的系数 $lambda$，也仍然有一些技巧可以帮我们把高次项压下去。我们发现一路下来导致问题的根源都是 $x$ 的高次项即使在系数很小时也会导致数值爆炸。但是如果把 $x$ 放到 $[-1, 1]$ 的闭区间内呢？这样即使是 $x^10$ 项，$x$ 的 10 次方也不会超过 $1$，系数再怎么大，至少在小范围内也不会导致数值爆炸，这样一来正则化项才能发挥其约束作用。


操作上只需要把 $[0, 10]$ 范围内的 $x$ 线性地映射到 $[-1, 1]$ 范围内。这很简单，令 $z = 0.2 x - 1$ 再对 $y$ 用 $z$ 的多项式拟合，这种操作称作
#textOverSet("Normalization", "归一化")
#footnote[归一化：调整数据到某个给定的范围内，使数据在不同场景下更加可比、更加数值稳定。前文计算误差时取平均实际上也是一种归一化。]。


事实上只需要一个很小的 $lambda$ 就可以在一定程度上抑制高次项的系数，这里我们取 $lambda = 0.01$，优化的目标变为了

$
  "Loss" = "MSE" + 0.01 sum_(i=1)^10 a_i^2
$

这时拟合出来的图像是这样的：

#fitting_10_normalized

虽然图中拟合曲线与真值在数据集外确实仍然有显著的差距，但经过自变量归一化和参数正则化项的加入，拟合的曲线至少把数据的大体趋势成功地延伸到了数据集的一个邻域内，不至于像原本的那样惨不忍睹。

不过在实际应用中，几乎不会用到 5 次以上的多项式拟合。仍然是因为容易过拟合：就以 $[-1, 1]$ 上的函数为例，$x^5$ 与 $x^7$ 的图像几乎是一样的，它们最大的差值仅为 0.12，这意味着如果允许的高次项太多，一点点微小的噪声就能让轻易地把五次项的系数“分给”七次项，或者反之。这导致拟合的数值稳定性很差，因此显然不太可靠。从这种影响的角度看，高次多项式拟合本身就有
#textOverSet("Singularity", "奇异性")
，解并不稳定（这种对微小噪声敏感的问题常称为
#textOverSet("Ill-posed Problem", "病态问题")
）。因此比正则化或者归一化更重要的是，我们应该意识到高次函数并不是万能的。当你觉得需要用到很高次的函数才能成功拟合时，不如先想想，多项式的假设真的合适吗？

我们注意到一个重要的事实：虽然拟合的参数在变化，但是拟合前仍然需要人为地设定多项式的次数，正则项（如果有的话）权重也需要人为设定。这些
#textOverSet("Prior", "先验")
的
#footnote("先验：在观测到数据之前，我们已经了解了一些数据特征。")
参数通常称为
#textOverSet("Hyperparameter", "超参数")
。如何用模型拟合固然是重要的问题，但是模型的结构，包括如何选取适当的超参数也有学问。因为它们通常不是直接从数据中学习的，而需要人为设定。
#link("https://www.zhihu.com/question/625846838/answer/3251736042", "有道是")
“学而不思则欠拟合，思而不学则过拟合”。参数太少就会像那直线拟合曲线一样，必然导致拟合的精度不足。参数太多则会受到太多的噪声干扰，像是拿高次函数拟合低次函数一样因为一点噪声导致函数行为夸张，在预定义的数据集外两眼一抹黑。只有在一定程度上了解问题的本质，才能选出合适的拟合模型。

\begin{tcolorbox}[myrecommendbox, title=推荐阅读, breakable=false]
    \begin{itemize}
        \item 如果你想看更多关于多项式拟合的实战，可以阅读：\\
              \textit{多项式拟合的介绍与例子 - 姓甚名谁的文章 - 知乎}\\
              \url{https://zhuanlan.zhihu.com/p/366870301}
        \item 如果你曾经想过拿问卷调查来做拟合，可以看看：\\
              \textit{理科生觉得哪些知识不知道是文科生的遗憾？ - 一只小猫咪的回答 - 知乎}\\
              \url{https://www.zhihu.com/question/270455074/answer/2374983755}
        \item 这个比喻很好，同一问题下的其它回答也很有趣：\\
              \textit{人的大脑会不会出现“过拟合”病? - 莲梅莉usamimeri的回答 - 知乎}\\
              \url{https://www.zhihu.com/question/625846838/answer/3250463511}
    \end{itemize}
\end{tcolorbox}

#recommend("推荐阅读",
[
  如果你想看更多关于多项式拟合的实战，可以阅读：\
  #h2 `多项式拟合的介绍与例子 - 姓甚名谁的文章 - 知乎`\
  #h2 #link("https://zhuanlan.zhihu.com/p/366870301")\
  如果你曾经想过拿问卷调查来做拟合，可以看看：\
  #h2 `理科生觉得哪些知识不知道是文科生的遗憾？ - 一只小猫咪的回答 - 知乎`\
  #h2 #link("https://www.zhihu.com/question/270455074/answer/2374983755")\
  这个比喻很好，同一问题下的其它回答也很有趣：\
  #h2 `人的大脑会不会出现“过拟合”病? - 莲梅莉usamimeri的回答 - 知乎`\
  #h2 #link("https://www.zhihu.com/question/625846838/answer/3250463511")\
]
)