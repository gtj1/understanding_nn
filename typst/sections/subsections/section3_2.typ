#import "../../config.typ": *
#import "./section3_plot.typ": *

== "激活函数与非线性"

#h2 将 $y = w x + b$ 作为一次函数的类比应该足以说明它是很简单的一类函数。但是正如一次函数的复合 $y = w_2(w_1 x + b_1) + b_2 = w_2 w_1 x + (w_2 b_1 + b_2)$ 仍然是一次函数一样，如果仅仅沉浸在矩阵运算中，我们便永远无法表达那些复杂的函数。举个最简单的例子，我们甚至无法表示输入的绝对值 $y = |x|$。因此我们需要在模型的结构中加点"非线性"，让它不仅仅局限于简单的加减乘除，专业的说法称之为#textOverSet("Activation Function", "激活函数")。激活函数直接作用在每个特征上，而且函数本身通常是固定的#footnote[通常是固定的：在一些模型，例如使用可变样条函数的#link("https://arxiv.org/pdf/2404.19756")[KAN]中，激活函数也是可学习的，而且各个元素上的效果可能不同，但是可变的激活函数总体来说并不常见。]，且总体通常呈现递增的趋势。

所谓逐元素作用，也就是说，与矩阵对特征进行组合不同，激活函数对各个分量的操作是独立的。其输入是一个向量，输出也是一个同样维数的向量。如果选定了激活函数 $f: RR -> RR$，输入为 $x = [x_1, x_2, dots.c, x_n]$，则输出为 $y = [f(x_1), f(x_2), dots.c, f(x_n)]$。

现在使用最多的激活函数是#textOverSet("Rectified Linear Unit", "线性整流函数") (ReLU)，虽然相对于其它激活函数，诸如 Sigmoid、tanh 等等，"ReLU" 其实算是晚辈，但是在关于激活函数的讨论中，#link("https://proceedings.mlr.press/v15/glorot1a/glorot1a.pdf")[有研究]表明它的效果更好，而后#link("https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf")[AlexNet] 的成功更让它成为了主流的激活函数。虽然失去了早期其它激活函数的仿生背景，但它好用，而且非常简单。它的定义是：

$ "ReLU"(x) = max{0, x} = cases(
  x "," x >= 0,
  0 "," x < 0
) $

图像是这样的：

TODO: 这里需要一个图，展示ReLU函数的图像

举一个例子就可以看出逐元素作用的含义。例如有输入向量 $x = [1, -2, 3]$，那么它经过 ReLU 激活函数的输出为 $y = [1, 0, 3]$。正的部分被保留了，而负的部分被置为 0。正如电路中的半波#textOverSet("Rectifier", "整流器")一样，把负值截断了。

而它的导数也非常简单：

$ diff/(d x) "ReLU"(x) = cases(
  1 "," x > 0,
  0 "," x < 0
) $

读者或许会关心，那 0 这一点不可导要怎么办？其实关系不大，因为一个小数几乎不可能#footnote[几乎不可能：在最常用的 32 位浮点数中，一个数恰好取到 0 的概率大概在 $10^(-9)$ 量级。虽然在 FP8 或者 FP16 量化中恰好取到 0 的概率更大，然而实践中这单个不可导点几乎不会对训练产生影响。]在训练中恰好落在 0 上。即使有，也可以任意地选择一个值，例如 0 或者 1#footnote[0 处的导数：PyTorch 通常选择 0]。有了这样的激活函数，函数的表达能力大大就增强了。以目标 $|x|$ 为例，假设有输入 $x$，只需两个 ReLU 函数值的和就可以表示它：

$ |x| = "ReLU"(x) + "ReLU"(-x) = max{0, x} + max{0, -x} $

初看可能会觉得这样的表达方式有点多此一举，像是为了 $|x|$ 这盘醋专门包的饺子。但是别急，让我们把它拆解成神经网络的结构，更加结构化地看待。

最初的输入是 $x$，它先经过一个线性的函数得到 $[x, -x]$，再经过 ReLU 函数得到中间的向量 $x^((1)) = ("max"{0, x}, "max"{0, -x})$，而这使用一个线性函数就可以得到 $y = |x|$。

写成矩阵的形式就有

$ w_1 = mat(1; -1), b_1 = mat(0; 0), w_2 = mat(1, 1), b_2 = 0 $

遂可以写成 $y = w_2 "ReLU"(w_1 x + b_1) + b_2$。我认为，把这件事作为一个 toy case#footnote[toy case：玩具案例，指的是一个简单的例子，用于说明某个概念或方法。]想明白多少可以帮助理解神经网络。把矩阵的每个权重都画出来就是这样了：

TODO: 这里需要一个图，展示神经网络表示|x|的结构

这看起来很简单，读者可能想问：还能不能再给力一点，看看更复杂的情况呢？当然可以。不过在看之前先抛出两个思考题：
1. 试着用线性函数和 ReLU 函数表示 $y = max{x_1, x_2}$，并画出它的神经网络结构图。
2. 线性函数和 ReLU 的组合*不能*表示什么函数呢？

在思考这个问题时，读者可以先回顾 ReLU 的性质：它的作用是将负数截断为 0，而正数保持不变。那么，能否通过适当的线性变换和 ReLU 来分辨两个数的大小呢？实际上我们可以很容易地发现

$ max{x_1, x_2} = x_1 + "ReLU"(x_2 - x_1) $

但是这个答案并不够好，如果直接把它画成神经网络结构图，就会发现它的结构看起来像是这样：

TODO: 这里需要一个图，展示神经网络表示max{x_1, x_2}的一种方法

变量 $x_1$ 没有经过统一的隐藏层，而是跳过中间，直接连接到了输出层。显然就不能用一致的 $"ReLU"(w x + b)$ 的形式来表示了，而是要单独开一个通道来处理。而我们使用神经网络的目的本来就是用一致的方式来处理所有的输入，所以这样的表示方式并不优雅#footnote[并不优雅：与之对比，在深层神经网络网络中通常会引入看起来有些像这里的#textOverSet("Shortcut Connection", "跳连接")结构，由此引出#textOverSet("Residual Network", "残差网络")#footnote[残差网络：是指在深度神经网络中，通过引入跳过中间层的连接，使得网络能够更好地学习到输入和输出之间的残差，从而使得网络能够更深地进行训练。]的概念。它看起来有些像这里的跳过中间层的结构，但那里是系统性地引入这样的连接，而不是这样对某个分量单独处理。]。

不过使用一点小小的技巧，可以把 $x_1$ 本身写成 $x_1 = "ReLU"(x_1) - "ReLU"(-x_1)$，这样一来就可以把它写成带有三个中间变量的一个网络结构了。把

$ max{x_1, x_2} = "ReLU"(x_1) - "ReLU"(-x_1) + "ReLU"(x_2 - x_1) $

这一式子中的三个分量提出来，便可以得到

$ x_1^((1)) &= "ReLU"(x_1 + 0x_2) \
  x_2^((1)) &= "ReLU"(-x_1 + 0x_2) \
  x_3^((1)) &= "ReLU"(-x_1 + x_2) \
  y &= x_1^((1)) - x_2^((1)) + x_3^((1)) $

偏置 $b$ 仍然为 $0$，读者可以自行试着写出对应的权重矩阵 $w$，按照新的写法重新绘制，这时结构图就会变成这样：

TODO: 这里需要一个图，展示神经网络表示max{x_1, x_2}的另一种方法

虽然中间的神经元多了一些，但是它的结构看起来就统一而且整齐得多了。或许有人会有疑问，这里连的线变多了，不是把事情复杂化了吗？实际上并没有，恰恰相反，把它整齐地写出来才有利于算法的数值优化。

一个有趣的事实是，如果把 True 和 False 分别视作 1 和 0，那么最多两层的网络就可以表示任意的逻辑函数。例如

$ x_1 "and" x_2 &= "ReLU"(x_1 + x_2 - 1) \
  x_1 "or" x_2 &= "ReLU"(x_1) + "ReLU"(x_2 - x_1) \
  x_1 "xor" x_2 &= "ReLU"(x_1 - x_2) + "ReLU"(x_2 - x_1) $

这至少表明逻辑可以在一定程度上编码进神经网络中，用一些可调的权重来模拟逻辑门#footnote[用权重模拟逻辑门：这里仅说明它可以，不过这么做太奢侈了，很浪费储存和计算资源。]，因此从这一特例来看，求特征的交集、并集的操作确实可以自然地以权重的方式编码到网络的运算中。

推而广之，不难发现 ReLU 本质上完成的是将函数分段的操作。调整权重就可以做到在不同的区域选择不同的段，从而给出不同的表达式。虽然它在每一根区域内仍然是线性的，但却可以通过一些点上的弯折来实现非线性，表达能力比单纯的线性函数大大提高。这样的函数在数学上称为#textOverSet("Piecewise Linear Function", "分段线性函数")，如我们所见，ReLU 函数就提供了一种通用的方式来实现分段线性函数，从而将关于"分类"的信息编码到网络中。

那么它不能表示什么函数呢？由于其分段线性的特性，不难证明它无法完全精准地表示光滑的曲线，例如 $y = x^2$。而且可以证明，对于任何一个分段线性函数 $f(x)$，都可以找到一个常数 $c$ 使对于 $norm(x)$ 足够大的时候，$f(x) <= c norm(x)$。从而增长速度有限，无法表示指数函数或者高次的多项式函数。

这确实体现出了它的局限性，但这必然是它的弱点吗？并不一定。一方面，虽然它本身无法*精准地表示*光滑的函数，但是只要给定一个自变量的区间，在这样的函数堆叠多层之后总是可以调整参数，做到*良好地近似*给定的函数。事实上，只需四段就可以在区间 $[-1, 1]$ 上用如下的分段线性函数来相当好地近似 $x^2$ 了，例如下面的分段线性函数 $f(x)$：

$ f(x) = 2"ReLU"(x-1) + 2"ReLU"(x) + 2"ReLU"(-x) + 2"ReLU"(-x-1) - 0.04 $

图像是这样的：

TODO: 这里需要一个图，展示分段线性函数近似光滑函数

另一方面，虽然它的输出会被输入大小的一个常数倍所控制，但在很大程度上，这也避免了在第一章中多项式拟合的数值爆炸问题。此外，这提醒我们应当将模型的输入输出控制在一个范围之内。遵循这些原则，ReLU 网络的表达能力已经足够强大，能解决大多数实际问题。尽管仍有一些细节需要注意，但这并不影响我们对其整体能力的理解。

另外再提一嘴其它的激活函数。Sigmoid 函数#footnote[Sigmoid 函数：Sigmoid 来源于拉丁语，得名于其类似小写字母 sigma 的 S 形状。]是一个 S 型函数，定义为

$ "Sigmoid"(x) = 1/(1 + e^(-x)) $

输出随输入变化的图像是这样的，可见它把输入压缩到了 $[0, 1]$ 的范围内：

TODO: 这里需要一个图，展示Sigmoid函数的图像

tanh 函数是双曲正切函数，其定义为

$ "tanh"(x) = (e^x - e^(-x))/(e^x + e^(-x)) $

它的图像和 Sigmoid 函数很类似，只是经过了一个伸缩和平移，输出范围是 $[-1, 1]$：

TODO: 这里需要一个图，展示tanh函数的图像

早期的研究中，它们出现在许多生物学的研究中，可以描述生物神经元的激活或者极化程度，于是人工神经网络出于仿生的考虑也使用了它们。然而它们在两端很小的导数也为优化带来了许多麻烦，导致了#textOverSet("Vanishing Gradient", "梯度消失")#footnote[梯度消失：是指在深度神经网络中，由于输出随输入的变化过于小，导致信息无法有效地从输出传回输入，从而使得网络难以优化学习的现象。关于梯度的进一步介绍会在后文给出，此处可以简单理解为信息回传受阻。]的问题，后来逐渐被 ReLU 函数取代，仅在特定层要将输出限制在给定范围内时才使用。虽然近期有#link("https://arxiv.org/pdf/2503.10622")[研究]指出现在的优化器有能力克服这个问题，即使使用 tanh 仍然可以正常地优化，不过这也仅是一个理论上的结果，实际应用中通常认为它们仍然不如 ReLU 函数好用。从此也能看见人工智能的发展并非一帆风顺，仿生不是唯一的出路，人工的神经网络的发展和对其规律的认识必然要走过曲折的探索，才能形成一套独特而成熟的方法论。

不过 ReLU 在 $x < 0$ 的区域也存在斜率为 $0$ 导致梯度消失的问题，为此人们还提出了一些变体，例如 Leaky ReLU 函数，它在 $x < 0$ 的区域也有一个小的斜率，定义为

$ "Leaky ReLU"(x) = cases(
  x "," x >= 0,
  alpha x "," x < 0
) $

上式中 $alpha$ 是一个小的常数，通常取 $0.01$，它同样简单易于计算。还有一些较为复杂的变体，包括#textOverSet("Gaussian Error Linear Unit", "高斯误差线性单元") (GELU)，#textOverSet("Exponential Linear Unit", "指数线性单元") (ELU) 等，都在一定程度上克服了 ReLU 导数为 $0$ 导致信息传播不畅的问题。不过这些都属于工程上的细节问题，读者可以在需要的时候再去了解。

由此我们更加具体化地认识到了神经网络的工作原理：它的基本单元由线性函数与激活函数交替组成。每一层都可以看作是对输入进行线性组合，然后通过激活函数进行非线性变换以实现更复杂的表达能力。这让网络以一种统一的方式来处理输入数据，并有能力通过调整参数拟合复杂的输出。

#recommend(
  "相关阅读",
  [
    文中为了简单起见，只是简单介绍了 ReLU 激活函数，关于更多激活函数的定义与性质可以看这篇笔记：\
    #h2 `深度学习随笔——激活函数(Sigmoid、Tanh、ReLU、Leaky ReLU、PReLU、RReLU、ELU、SELU、Maxout、Softmax、Swish、Softplus) - Lu1zero9的文章 - 知乎`\
    #h2 #link("https://zhuanlan.zhihu.com/p/585276457")\
    形成图形化的直觉许多时候相当重要，这篇文章就给出了一个图形解释：\
    #h2 `形象的解释神经网络激活函数的作用是什么？ - 忆臻的文章 - 知乎`\
    #h2 #link("https://zhuanlan.zhihu.com/p/25279356")\
  ]
)
