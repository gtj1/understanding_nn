#import "../../config.typ": *
#import "./section3_plot.typ": *

// 从函数拟合的角度引入神经网络
== 神经网络：一个大的函数

#v1 #h2 相比于#textOverSet("Neural Network", "神经网络")如何实现其功能，读者或许更想问的是：为什么要用神经网络？现有的神经网络为什么用了这些方法？对于这一类问题，一个现实的回答是：机器学习是高度以实用为导向的，实验显示这样做效果更好。在现实中，我们往往要解决各种各样的问题，人类开发者以手写每一行代码创造了各种各样的程序，自动化地解决了许多问题。但很多问题难以在有限的时间内找到确定性的解决方案，例如识别图片中的物体、识别语音、自然语言处理等等。它们有一个共同点：输入的信息量巨大、关系复杂，难以用确定的规则来描述。手动规定像素范围来判断物体类型，或用固定的规则来解析自然语言显然并不现实。因此人们自然要问有没有更加自动化、灵活、智能的方法来一劳永逸地解决这些问题。人工智能的概念就此提出，人们希望让机器自己学习知识来解决问题。

虽然目前人类仍然很难说摸到了#textOverSet("Artificial General Intelligence", "通用人工智能")#footnote[通用人工智能：指能像人类一样解决各种通用的问题的人工智能。]的边界，但人工智能已然在许多问题上取得了巨大成就，走出了 20 世纪末 21 世纪初被大众认为是"伪科学"的寒冬。经过#link("https://arxiv.org/pdf/1512.03385")[深度残差网络]在图像识别的重大突破、#link("https://www.davidsilver.uk/wp-content/uploads/2020/03/unformatted_final_mastering_go.pdf")[AlphaGo]学会下围棋、#link("https://arxiv.org/pdf/1706.03762")[Transformer]在翻译比赛取得优异成绩并引来一波生成式模型的热潮等等，人工智能就这样走向了时代的焦点。但是如果要问：为什么它这么成功？最直接的回答仍是：It works.

除了一些基础的训练方法外，其它的结构构成、参数调整等等往往都是人们有一个想法，于是就这样展开了实验。部分实验成功了，就说明这个想法是对的，从而延伸出新的调节思路。如此循环往复，形成了现在的人工智能领域。因此就模型结构而言并没有非常完备的理论，有的只能说是经验法则。

不过我想可以对解决的方法做一个简单的分类。按照参数的数量，从参数复杂到参数简单可以画出一条轴。按照模型获取经验的方式，从模型完全编码了先验经验，到通过一些例子得到经验，再到持续在与环境的互动中获取经验，可以画出另一条轴。在这里我也试图并不严谨地画出了这样一个表格。

#align(center)[#table(
  columns: (auto, auto, auto, auto, auto),
  align: center,
  [*监督方式 \\ 参数量*], [*超大参数量*], [*大参数量*], [*小参数量*], [*经典模型*],
  [*持续互动*], [PPO, A3C], [DQN], [Q-Learning], [经典控制],
  [*输入/输出对*], [ResNet, Transformer], [浅层CNN], [浅层MLP], [SVM],
  [*无监督*], [GAN, SimCLR], [--------], [K-Means, KNN], [PCA, t-SNE]
)]

读者看到的第一反应大抵是感到看不懂。不过我也并非想让读者先学完再来看这个表格，而是希望读者看到：解决问题的方法虽然多样，但仍可根据若干指标大致分类。表中的术语有的是模型结构，有的是算法，有的是思想，有的是算法，有的是思想，而右侧的一列甚至根本就不是机器学习，对机器学习有基本了解的读者或许会认为它们可比性存疑。诚然，模型之间并没有一个实际上的绝对界限，表中划分的位置也仅是凭借我的经验评价一个模型大多数时候处于什么位置，而非绝对的准则，但我认为这样的划分是有意义的，用一种更为建设性的话来说：意义就是在混乱的世界中建构起规律，用于解决问题。

大参数量的一侧——神经网络的领域，正是本书的主题。作为神经网络的引入，有必要从更高的角度来理解以神经网络为基础的模型目标是什么。小节标题已经足以表达内容核心：先不论内部结构如何，所谓的神经网络，无非也是一个函数。所谓函数，就必然要考虑到输入和输出，或者更准确地说，我们关心的就是怎么用计算机程序对给定的输入，得到我们想要的输出。无论是连续的数据，还是按照 0 或者 1 编码为向量的标签，输入和输出都可以变为向量。因此许多问题都可以归结为一个更加狭义的、数值拟合意义上的函数拟合问题。一个#textOverSet("Encoder", "编码器")将原始输入变为向量这种易于处理的形式。而对于函数的原始输出，可以通过一个#textOverSet("Decoder", "解码器")将数值构成的向量变为我们想要的输出。

而再向前看，在第一章中我们已经初步了解了以线性回归为代表的一类函数拟合问题。虽然这一问题从结构上相对简单，但是从这一情境中可以抽象出函数拟合的理念：有一些输入和输出的对应关系，我们要设计一个带参数的拟合模型，调整参数，让模型的输出尽可能接近我们预期的输出，接近程度则通过一个损失函数来衡量。

因此我会把模型抽象成五个要素：#textOverSet("Input", "输入")、#textOverSet("Output", "输出")、#textOverSet("Architecture", "模型结构")、#textOverSet("Loss Function", "损失函数")和#textOverSet("Optimizer", "优化算法")。输入、模型架构和具体参数决定了输出如何计算，按照损失函数计算得到的损失指导模型调整具体参数，优化算法则决定了参数如何调整。当然这样的划分只是我自己的理解，而非理解神经网络的唯一方式。这里我不打算在概念之间玩文字游戏，把机器学习中的概念倒来倒去，变成一篇又臭又长，令人看完莫名其妙、不知所云、又对实践毫无益处的文章。因此我认为画一个图串起来是最直观的方式。


TODO: 这里需要一个图，展示了神经网络的五个要素及其关系
#nn_elements

从输入到输出再到损失的过程通常称为#textOverSet("Forward Propagation", "正向传播")，而从损失到参数的更新过程则称为#textOverSet("Backward Propagation", "反向传播")。而这中间的模型结构常常由矩阵运算与一些#textOverSet("Activation Function", "激活函数")构成的层组成。几乎可以说众多的神经网络中，只有这种传播的方式和网络的基本组成元素是相同的，如何从这些基本元素构建出好的模型则像是搭积木一样，各有各的搭法。

在这里我想简单讲讲使用矩阵运算的原因。在第一章中我们已经简单地学习了矩阵运算的基本知识，它本质上是正比例函数在向量空间中的推广，只是 $y = k x$ 中的斜率变成了一个个从输入 $x_j$ 连接到输出 $y_i$ 的权重 $w_(i j)$。从行看过去，它反映了输出的每个分量（或称为特征）是如何由输入的每个分量线性组合而成的。而从列看过去，它表明了输入的每个分量是如何影响输出的。就像一次函数有一个常数项一样，矩阵运算也有一个偏置项 $b$，运算的总体结构是 $y = w x + b$。从代数上看，它运算简单#footnote[简单：仅由简单的四则运算组成，现代 GPU 也常常提供高效的矩阵运算加速。]，而从分析上看，它的输出变化光滑，容易求导#footnote[容易求导：记住这一点，这对后续反向传播等算法的实现至关重要。如果在离散的空间中操作，例如使用阶跃函数或者逻辑门，便无法借助导数来进行参数更新。]。

#recommend(
  "相关阅读",
  [\
这篇文章讲述了神经网络的起源：\
#h2 `如何简单形象又有趣地讲解神经网络是什么？ - 佳人李大花的回答 - 知乎`\
#h2 #link("https://www.zhihu.com/question/22553761/answer/3359939138")\
读者或许会好奇所谓的万能逼近定理需要是如何能逼近给定函数的，这篇回答的解释不错：\
#h2 `神经网络的万能逼近定理已经发展到什么地步了？ - 牛油果博士的回答 - 知乎`\
#h2 #link("https://www.zhihu.com/question/347654789/answer/1534866932")\
这一问题下有关于神经网络"涌现"出新的现象的讨论，对其机理感兴趣的读者也可以想想背后的原因：\
#h2 `如果神经网络规模足够大，会产生智能吗？ - 知乎`\
#h2 #link("https://www.zhihu.com/question/408690594")\
  ]
)