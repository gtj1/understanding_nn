#import "../../config.typ": *
#import "section1_plot.typ": *

== 高维的线性拟合

#figure(
  image("../../../img/high_dimension_fitting.png", 
  // width: 60%
  ),
  caption: [高维线性拟合示例]
)

#h2 第一节我们介绍了“简单线性回归”，即只有一个自变量的线性回归。但是在实际问题中，自变量往往不止一个，这时一元的线性回归就需要改成
#textOverSet("多元线性回归", "Multiple Linear Regression")。不过按照我的习惯，文中仍然称为“拟合”。

现实世界中的数据往往是多维的，就以估计体重为例，不难发现年龄和身高就是两个可能相关的变量。如果我们想用一个模型来描述这种相关性的话，最简单的就是线性模型了，与之前的 $hat(y)=k x+b$ 类似，自然想到用这样的函数
#footnote[记号说明：这里使用字母 $w$ 表示#textOverSet("权重", "weight")，$b$ 表示#textOverSet("偏置", "bias")，即常数项，$d$ 表示的是空间的#textOverSet("维度", "dimension")。]去拟合数据：

$
  hat(y) = w_1 x_1 + w_2 x_2 + dots + w_d x_d + b
$

同样地，优化的目标仍然是最小化均方误差，即对 $n$ 个数据点令
$
    "MSE" = 1/n sum_(i=1)^n (y_i - hat(y_i))^2
$

通过最小化误差得到 $w = [w_1, w_2, dots, w_d]$ 和 $b$ 的值。这个过程与一元线性回归的过程是类似的，只不过自变量是一维时，可以在平面上直接画出拟合的直线，二维时可以在空间中画出平面，但是当维数增加到三维及以上时，拟合所用的线性函数就变为#textOverSet("超平面", "Hyperplane")了，我们无法直观地看到这个超平面，但是可以猜测，它的原理差不多。

话不多说，先看看效果。这里以美国人类学家 Richard McElreath 搜集到的一个年龄、身高与体重#link("https://github.com/rmcelreath/rethinking/blob/master/data/Howell1.csv", "数据集")为例，它的分布与拟合出来的平面是这样的：

#figure(
  image("../../../img/fitted_plane.png",
   width: 80%
  ),
)

通过拟合，我们可以得到一个超平面，它大致描述了数据的分布。这个超平面的方程是 $0.04676645038926784 dot "age" + 0.47766688346191755 dot "height" - 31.805656676953056 = hat("weight")$，它比单纯使用身高或者年龄的拟合效果都要好一些。由此还可以量化地看到，年龄与身高都会影响体重，但是年龄是弱相关，而身高是强相关，这也符合我们的日常经验。

不过正如我们之前一直在做的一样，让我们看看更为直观的几何视角。仍然用 $bold(x^0)$ 表示全 $1$ 的向量，使用 $bold(upright(x))_(:1)$ 表示所有样本的第一个
#textOverSet("特征", "Feature")（分量），$bold(upright(x))_(:2)$ 表示所有样本的第二个特征，以此类推
#footnote[记号说明：冒号表示取所有行，这是为了与 Python 中 Numpy, Torch 等库的列切片语法 $a[:, j]$ 对齐。]。那么多元线性拟合时的残差向量变为了
#footnote[记号说明：在公式中我特意将常数项放到了最前面，这是为了让它和多项式拟合的形式保持一致。]
$
  bold(upright(r)) = bold(upright(y)) - hat(bold(upright(y))) = bold(upright(y)) - (b bold(upright(x))^0 + w_1 bold(upright(x_(:1))) + w_2 bold(upright(x_(:2))) + dots + w_d bold(upright(x_(:d))))
$

如果回顾一下我们在多项式拟合一节的内容，就会发现这和多项式时的残差向量

$ bold(upright(r)) = bold(upright(y)) - (a_0 bold(upright(x))^0 + a_1 bold(upright(x))^1 + dots + a_m bold(upright(x))^m) $

有着惊人的相似之处。细心的读者可能已经发现，如果令这些分量 $bold(upright(x))_(:1), bold(upright(x))_(:2), \ldots, bold(upright(x))_(:n)$ 分别为 $bold(upright(x))$ 的幂次组成的向量 $bold(upright(x))^1, bold(upright(x))^2, dots, bold(upright(x))^d$，那么我们得到的完完全全就是多项式拟合。这也就意味着，多项式拟合实际上可以视为多元线性拟合的一种特殊情况。

事已至此，我们似乎已经许多次遇到了这样一种情况：从一面看过去，是代数上，一组样本点上的线性拟合。但是从另一面看过去，确是在几何上找到高维空间的超平面中最接近给定点的向量。这里其实有不少精妙的数学原理
#footnote[写给数学基础好的读者：这本质上体现了代数与几何的#textOverSet("对偶性", "Duality")。]
，但是考虑到这里的主题是机器学习，我将只带读者简要地复习（或者学习）一下线性代数，更为系统性地从几种略有差距的视角
#footnote[
  几种视角：
  #textOverSet("整体", "Overall")解读、
  #textOverSet("按行", "Row-wise")解读、
  #textOverSet("按列", "Column-wise")解读、
  #textOverSet("按元素", "Element-wise")解读。
]
体会矩阵的本质。


在绘图讲解前，我首先要感谢
#link("https://github.com/kenjihiranabe/The-Art-of-Linear-Algebra", "《线性代数的艺术》")
（#emph[The Art of Linear Algebra]）

这篇笔记，我第一次读到便感到文中的插图绘制非常精妙。它的思路是顺着 Gilbert Strang 教授书籍
#link("https://math.mit.edu/~gs/everyone/", "《写给所有人的线性代数》") 的思路，使用图形化的方式来解释线性代数的概念。认为可以看成是一本矩阵图鉴，对理解矩阵运算有着极大的帮助。

#link("https://www.bilibili.com/video/BV1ys411472E", "3Blue1Brown 的线性代数系列")
也是优质线性代数学习资源。这个制作精良的合集仅用不到两个小时的视频就清晰地从几何的角度讲明白了线性代数的基础知识，也是我入门线性代数的第一课。

矩阵有很多种#textOverSet("解读", "Interpretation")，不过我觉得大致可以按照是否把行看作一个整体以及是否把列看作一个整体来分为四类。

#matrix_interpretations

矩阵究竟是什么，我们的大学教了很多年，也没有完全搞清楚。从数学的角度看，可能#link("https://linear.axler.net/LADR4e.pdf")[Linear Algebra Done Right]的思路比较好，搞了个向量空间起手，全程以#textOverSet("映射", "Map")的逻辑贯穿。但是在国内，大部分教材一上来前两章就是讲行列式的计算，教学内容逐渐搞僵化了。

既然是服务于机器学习，许多内容#footnote[许多内容：线性方程组的求解、行列式、#textOverSet("基变换", "Change of Basis")、#textOverSet("特征分解", "Eigen Decomposition")、#textOverSet("二次型", "Quadratic Form")。]我们一概砍掉，只留下最为基础的内容。第一种视角就是作为 $bb(R)^n -> bb(R)^m$ 的线性映射。使用矩阵的第一个重要目的就是把一个线性映射"打包"成一个符号，毕竟只有这样才能方便地书写、推导和计算。从工科的视角看来，一个"向量"无非是一个数组，而一个"线性映射"实际上就是吃进去一个数组，吐出来另一个数组的机器#footnote[此注释写给编程基础较好的同学：这里的机器指的就是编程中的#textOverSet("函数", "Function")。]。矩阵作为一个二维数组，忠实地记录了这个机器的所有参数。它的运算规则是这样的：

$
mat(
  a_(11), a_(12), dots, a_(1n);
  a_(21), a_(22), dots, a_(2n);
  dots.v, dots.v, dots.down, dots.v;
  a_(m 1), a_(m 2), dots, a_(m n)
)
mat(
  x_1;
  x_2;
  dots.v;
  x_n
)
=
mat(
  a_(11)x_1 + a_(12)x_2 + dots + a_(1n)x_n;
  a_(21)x_1 + a_(22)x_2 + dots + a_(2n)x_n;
  dots.v;
  a_(m 1)x_1 + a_(m 2)x_2 + dots + a_(m n)x_n
)
$

从输出的表达式中，我们自然引出了行的视角。如果把矩阵看作若干行：

#matrix_row_view

我们会发现输出 $y$ 的每一个分量 $y_i$ 都是输入 $x$ 与行 $a_i$ 的点积#footnote[严格地说 $a$ 是行向量，$x$ 是列向量，这中间在数学上有一些差别，是矩阵乘法而不是点积。但是在计算机存储中，因为都是一维数组，从实用的角度并不需要纠结于此，这种#textOverSet("记号混用", "Abuse of Notation")就见怪不怪了。]。例如

$
y_1 = a_1 dot x = a_(11)x_1 + a_(12)x_2 + dots + a_(1n)x_n
$

诚然，每一行都是一个#textOverSet("齐次的", "Homogeneous")#footnote[齐次：指不带偏置(常数)项。]线性函数，它的形式也只能是这种 $x$ 的#textOverSet("加权和", "Weighted Sum")，但是相信一定会有读者好奇：点积衡量了两个向量的相似程度，那么这里做点积的几何意义是什么呢？答曰：探测输入的特征。

我们可以把矩阵的每一行看作一个#textOverSet("特征检测器", "Feature Detector")，它的方向表明了待检测的特征方向，与 $x$ 的点积则说明了这个输入在这个特征上的响应强度(通常称为#textOverSet("特征响应", "Feature Response"))。当 $x$ 与特征的方向相近时响应的值为正，而当 $x$ 与特征的方向相反时响应的值会为负，当 $x$ 与特征几乎无关时响应的值会接近于零。这是点积的几何特性，至少从理论上为特征提取画出了一条路径。

这时如果考虑怎么样的输入可以获得接近预期的输出呢？根据几何解释，其实就是试图找到一个输入向量，让它尽可能通过这些特征检测器，使得每一个特征检测器的响应值与预期的响应(输出的对应分量)尽可能接近，并考虑使用均方误差来"惩罚"不接近的程度，我们一开始的代数解释便是如此。

但是如果改改输出的写法，把矩阵切成若干列，我们又有了一个不同的视角，不过这里我们用 $a_(:j)$ 表示它的列：

#matrix_column_view

这样看来，矩阵的乘法也可以写成

$
A x &= mat(a_(:1), a_(:2), dots, a_(:n)) mat(x_1; x_2; dots.v; x_n) \
&= x_1 a_(:1) + x_2 a_(:2) + dots + x_n a_(:n)
$

也就是说，输出写成了输入的线性组合。这时我们在输出的空间 $bb(R)^m$ 操作，而输入的空间 $bb(R)^n$ 仅仅是作为权重的载体，给出了这些列向量应该以怎么样的比例组合。

这时我们要怎么考虑用输出反推合适的输入这个问题呢？随着输入的变化，输出会变为列向量的不同组合方式，正如之前所说的，我们需要在这些列向量线性组合形成的超平面上找点，让它和预期的输出尽可能接近，这就是我们在前文提到的最小二乘的几何视角。

最后一种角度则带有更为浓重的#textOverSet("解构", "Distruction")#footnote[解构：哲学术语，通常指的是对一个结构或概念进行拆解、分析。]色彩：把矩阵看成一个数表，作为一个填了数字的 $m times n$ 矩形：

#matrix_element_view

一般来说，我们的教材都是这么引入的，但是就像我刚才提到的一样，这种理解有着一股解构的色彩。如果没有解构后重新的#textOverSet("建构", "Construction")#footnote[建构：哲学术语，通常指的是对一个结构或概念进行重建、整合。]，这种理解很容易让人迷失在行列式、特征值、特征向量等等复杂计算的汪洋大海中，从而忘记了矩阵的本质。那么这种解释有什么意义呢？我认为它的作用就在于 $a_(i j)$ 体现了第 $j$ 个输入对第 $i$ 个输出的权重。

#matrix_address_line_view

矩阵乘法可以看成这样：输入的每个分量沿着列地址线#footnote[地址线：计算机内存中的概念，虽然逻辑上内存是连续的，但是实际上内存寻址时有很多层，最底层时被选择的内存芯片是通过行和列寻址的，物理上由两条地址线输入#textOverSet("行/列地址选通信号", "Row/Column Address Strobe")。]输入到每一列的所有块，由每个元素乘上对应的权重后，将结果"上传"到对应的行地址线上，最后在行地址线上的所有块累积起来得到输出的每个分量。

这个图还有另外一个很常见的呈现形式，把它看成一个无偏置的#textOverSet("线性层", "Linear Layer")#footnote[线性层：在后面章节的机器学习中会成为一个基本模块，本质上就是从输入 $x$ 得到输出 $y=A x + b$ 的过程，这里的无偏置即 $b=0$。]:

#linear_layer_view

每条从 $x$ 到 $y$ 的连线都代表有一个从 $x$ 到 $y$ 的权重，我们给 $x_j$ 到 $y_i$ 的连线赋予权重 $a_(i j)$，它就表明了输入 $x_j$ 是如何影响输出 $y_i$ 的。

至此我们已经从四个有差别但是又有联系的视角理解了矩阵的行为，不过我觉得我对于不同的解释还有一点观察。当降维时，特征提取(行的视角)体现的更明显，而当升维时，特征组合(列的视角)更为重要。降维伴随着对信息的压缩和精简，通过去掉不重要的部分，更接近事物的本质。升维不仅仅是增加维度，更是通过新的空间来赋予数据提供更多的可变性。在这个过程中，每一列代表了一个基向量，整个矩阵的列向量按比例组合出高维的结果。

花了一些篇幅来复习线性代数，是时候回到多元线性拟合的问题上了。不过这次可以使用矩阵的语言来描述这个问题了。假设我们有 $n$ 组 $d$ 维的 $x$ 的取值，它们组成了一个 $n times d$ 的矩阵 $bold(x)$#footnote[记号说明：使用大写字母表示矩阵的比较多，但是这里为了美观和符号的一致性，我们仍然采用黑体小写字母表示所有数据点的集合。$bold(x)_i$ 表示第 $i$ 个数据点，$bold(x)_(:j)$ 表示所有数据点的第 $j$ 个分量，$x_(i j)$ 表示第 $i$ 个数据点的第 $j$ 个分量，因为是标量，所以采取小写。而在方程 $x dot w$ 中，$x,w$ 并非数据点的集合，而是变量，故虽然为向量，但是采用斜体。]。我们的目标是找到一个 $d$ 维的 $w$，使得数据集上 $x dot w$ 尽可能接近 $y$，现在 $hat(y)$ 的表达式用点积的语言可以简洁地写为 $hat(y) = x dot w + b$ 这种形式。

但是通过一点点技巧可以让问题更简洁，在拟合函数中，我们可以把 $b$ 合并到 $w$ 中，也就是

$
hat(y) &= x dot w + b \
&= x_1 w_1 + x_2 w_2 + dots + x_d w_d + b\
&= mat(x_1, x_2, dots, x_d, 1) mat(w_1; w_2; dots.v; w_d; b)
$

对于样本，把所有的行并起来就变成了不需要额外添加偏置的 $hat(bold(y)) = tilde(bold(x)) tilde(w)$。如果要说这个 $tilde(bold(x))$ 是什么，它就是把 $bold(x)$ 的每一行拼上一个 $1$：

#multivariate_fitting_matrix_view

但是如果改成用列的视角来看待矩阵 $tilde(bold(x))$，我们会发现问题变成了用 $bold(x)_(:1), bold(x)_(:2), dots, bold(x)_(:d)$ 与 $bold(x)^0$ 的线性组合来贴近 $bold(y)$：

#multivariate_fitting_column_view

诶？这不是函数拟合问题吗？如果把 $bold(y)$ 和 $bold(x)_(:j)$ 看作是关于 $i$ 的函数（这里函数定义在 ${1, 2, dots, n}$ 上，换言之，记 $bold(y)(1) = y_1, bold(y)(2) = y_2, dots, bold(y)(n) = y_n$，同理，设 $bold(x)_(:j)(i) = x_(i j)$。那么我们的问题就是用 $d$ 个函数 $bold(x)_(:1)(i) tilde bold(x)_(:d)(i)$ 与一个常值函数 $bold(x)^0(i) equiv 1$ 来拟合一个给定的函数 $bold(y)(i)$。

去掉常数项（或者把常数项也当成一个 $bold(x)_(:d+1)$ ）并把自变量 $i$ 当成一元函数拟合问题中的 $x$ 就可以发现：这与使用给定函数 $f_1(x) tilde f_d(x)$ 的线性组合，在若干数据点上拟合给定函数 $f(x)$ 这一问题没有任何差别。由此可见，用给定的函数集来拟合一个函数本质上与多元线性拟合有着完全相同的数学结构。

至此，我们至少已经初步理解了线性拟合。在这一章的最后，让我们做个总结。

- 最开始我们引入了一元的线性拟合，学会了最小二乘法与最小化损失以优化参数的基本思想，也学会了如何给数据加权。
- 接下来来到了多项式拟合，虽然是曲线，但是如果把 $x$ 的方幂看作线性独立的分量，这仍然可以看作是一种线性拟合，在这里我们领略到了过拟合的危害，也理解了为什么要使用正则化与归一化方法。
- 在过拟合中，我们得到的更重要的启示是要意识到高次并不意味着万能，合适的才是最好。如果参数足以存下所有的数据，那么大概率就会过拟合。
- 最后我们引入了多元线性拟合，通过矩阵的不同解读，我们理解了特征提取与特征重组的逻辑。
- 从矩阵的行、列解读中，我们意识到多元线性拟合与函数的线性拟合本质上是一样的，这里有一种精妙的对应关系。

#recommend(
"推荐阅读",
[
The Art of Linear Algebra 这篇文章非常好，但是如果你上不去 GitHub，进这个知乎回答看也行：\
#h2 `如何快速理解线性代数？ - 丿小奇迹丨的回答 - 知乎`\
#h2 #link("https://www.zhihu.com/question/30726396/answer/3124578647")\
这篇文章推荐给数理统计与线性代数都学的较好的读者：\
#h2 `回归分析|笔记整理（6）——多元线性回归（上） - 学弱猹的文章 - 知乎`\
#h2 #link("https://zhuanlan.zhihu.com/p/48541799")\
]
)