#import "../../config.typ": *

== 最简单的规律——简单线性回归

#figure(
  image("../../../img/linear_regression.png", width: 60%),
  caption: [线性回归示意图 #linebreak() 图源：#link("https://en.wikipedia.org/wiki/Linear_regression", [Wikipedia])]
)

#h2
虽然#textOverSet("线性回归", "Linear Regression")的名字叫做"#textOverSet("回归", "Regression")"，但是事实上我更喜欢叫做#textOverSet("线性拟合", "Linear Fitting")。它的目的是找到一条直线尽可能"贴近"数据点。在这一基础上，我们可以发现数据之间的规律，从而做出一些预测。不过这里有几个问题：

- 为什么要用直线？为什么不用曲线？
- 为什么要用直线拟合数据点？这有什么用？
- "贴近"数据点的标准是什么？为什么要选择这个标准？

#h2

我认为用直线的原因无非两点：一是直线 $y = k x + b$ 简单且意义明确，又能处理不少的问题。几何上直线作为基本对象，尺子就能画出；代数上只需要加减乘除，一次函数我们也很早就学过了。而它的思想一路贯穿到了微积分的导数并延申到了线性代数。二是许多曲线的回归可以转为线性回归（见后文）。例如指数型的 $y = k upright(e)^(alpha x)$ 取对数变为 $z = alpha x + ln k$，又如分式型的 $y = (alpha x + beta)^(-1)$ 取倒数转化为 $z = alpha x + beta$，从而归结为线性拟合。因此带着线性拟合经验再去考虑曲线会更轻松。

至于其意义：一是找到数据的规律，二是做出预测。拟合的系数可以用于测算数据之间的关系，斜率 $k$ 表明输出对输入的敏感程度。一个经典例子是广告投放的#textOverSet("边际效益", "Marginal Benefit")#footnote[边际效益：经济学概念，每增加单位 投入，产出会增加多少 单位]，在一定范围内拟合收益与投入的关系，可以估算当前的边际效益，从而决定是否继续投放。而物理上，比值定义法定义的各种物理量，如电阻、电容等，最常用的测算方式都是线性拟合。例如测量电源输出的若干组 电压和电流数据，并拟合出直线，斜率的绝对值是电源的内阻，同时截距顺带给出了电源的电动势，这样测得的数据就可以用于预测电源的输出情况。对我们所处的世界有定量的认识是科学的基础。可测量的数据和数学模型来描述、解释和预测自然现象是科学的基本方法，也是拟合的根本目的。

既然有了基本思路，那么如何选择“贴近的”标准呢？直接去度量一堆散点和直线的接近程度多少有点霰弹枪#footnote[霰弹枪：一种枪，射出的子弹像雨点一样散开]打移动靶的感觉，但是我们总是可以计算子弹达到了几 环。换言之，两个相差的部分才是关键的，#textOverSet("残差", "Residual")的概念由此产生。取出每个点实际值和拟合值的差，就得到了这样一个列表#footnote[记号说明：对于变量，无论是一维变量还是多维变量，一律采用斜体。对于具体的数据点，视是否为向量决定使用黑体还是斜体。例如$r = y - hat(y)$表示的是方程，所以全部采用斜体。但是具体数据的残差计算，例如$bold(upright(r = y - hat(y)))$，是对数据点的向量运算，所以采用正体。]（其中根据拟合函数$hat(y_i) = k x_i + b$计算出预测值）：

$ bold(upright(r)) = [r_1, r_2, ..., r_n] = [y_1 - hat(y)_1, y_2 - hat(y)_2, ..., y_n - hat(y)_n] $ 

度量数据点与直线间偏差这一问题就转为了度量残差与 0 的偏差。还记得勾股定理吗？直角坐标系内一点到 0 的距离是坐标平方和的平方根，只不过这里残差列表是个$n$维的向量，度量它偏离原点的成都就是向量的#textOverSet("模", "Norm")。这个模越小，说明拟合的效果越好。这样我们就自然地引入了度量拟合效果的量化标准，不过实际应用中出于方便（特别是计算上的方便），通常省去开根号的一步，直接采用残差的平方和，此外还会除以样本点数得到“平均”的残差平方。习惯上称之为#textOverSet("均方误差", "Mean Squared Error")（MSE）：

$ "MSE" = 1 / n |bold(upright(r))|^2 = 1 / n sum^n_(i = 1) r_i^2 $ 

在踏出下一步之前，我想这里有一点点思考的空间。例如：

- 为什么要用平方和而不是直接相加呢？

这是因为直接相加会有正负相互抵消的可能，度量出的偏差为 0  even 为负实在是不合理，因此至少要保证每一项都是正数。但是这又引出下一个问题。

- 为什么不用绝对值呢？绝对值也是正的啊。

从正态分布的角度看，选用平方和自有它的#link("https://www.zhihu.com/question/20447622/answer/25186207", [道理]) 。但是即使读者并不熟悉这些统计的知识背景，也可以从另一个角度理解：平方和的确是一个更好的度量方式，因为它对大的偏差更加敏感。例如一个残差为 2 的点和一个残差为 4 的点，直接绝对值相加的话是 6 ,在这里残差为 4 的点贡献了 $4 \/ 6 approx 66.7 %$ 的偏差。而它们的平方和是 $2 ^ 2 + 4 ^ 2 = 20$ ,残差为 4 的点贡献了$4 ^ 2 \/ 2 0 = 80 %$,更加凸显了 4 的偏差，反映了我们更“关注”这一大偏差的想法，更符合通常对“偏差”的直观认识。

- 为什么要除以样本点数$n$呢？

一方面是为了跨数据集比较。数据集的大小通常有区别，就像买东西的重量不同。这正如不能光看价格不看质量就评价 5 元 2 斤的苹果贵还是 3 元 1 斤的苹果贵，因此需要一个“单位”来衡量。另一方面，看完下一个问题你就会明白其中的精妙之处。

- 这里直接把所有的残差平方加了起来，但如果有的点重要一些怎么办？

先说明一下，这样的需求并非空想。有时测量条件决定了不同点的可靠性并不相同。以一个精度 1 % 的表为例，测量得到 1.00, 2.00, 3.00 时它们本身允许的误差分别是 0.01, 0.02, 0.03，而非相同。也就是说我们会觉得 1.00 的测量值从残差的大小上#footnote[残差的大小：严谨地说称作#textOverSet("绝对误差", "Absolute Error")]更为可靠，这时似乎应该衡量一下点的“重要性”。如果你想说一个点很重要怎么办？直观上来讲你可能会想把它重复几遍。例如，如果你很关心$r_1$, 你可能会想，这还不简单吗？在误差列表中把$r_1$重复 3 遍就好，就像这样：
$
  "Refined" bold(upright(r)) = [r_1, r_1, r_1, r_2, r_3, dots, r_n]
$

这时再计算均方误差呢，变成了$n+2$个点，一种我们设想的“#textOverSet("改善的", "Refined")”均方误差公式就变成了这样：

$ "Refined MSE" = 1 / (n + 2) (2r^2_1 + sum^(n+2)_(i=1) r_i^2) $ 

只不过这样的方式无疑有点“笨重”。再仔细想想呢？如果把$1\/(n+2)$乘到每一项上，就像这样：

$
  "Refined MSE" = 3/(n+2) r_1^2 + 1/(n+2) r_2^2 + ... + 1/(n+2) r_n^2
$

再对照者上面的列表看一看，$3\/(n+2)$不正好表明在大小为$n+2$的列表中$r_1$出现了3次吗？频次就这样和#textOverSet("权重", "Weight")（系数）联系起来了。我们也没必要守着重复 3 次或者 5 次这种固定的规则——至少自然可没有限制重要性之间的比例正好是整数。这样一来只需要一个权重列表就可以了。权重乘在残差平方前，这就引出了#textOverSet("加权误差", "Weighted Error")，大权重表示更重要。略微改写一下公式得到：

$
  "Weighted MSE" = sum^n_(i=1) w_i r_i^2
$

这里为了方便起见，假设了权重的和为 1，即 $sum_(i=1)^n w_i = 1$，如果不为 1，可以先计算误差再除以权重的和。由此可以根据实际情况调整不同点的重要性，也可以看出，之前的均方误差不过是因为在 $n$ 个数中每个残差变量都出现了 1 次，所以权重都设为了 $1/n$。在重要性可变时，加权均方误差无疑提供了一种更加“通用”的#textOverSet("度量方式", "Measurement Metrics")。

使用的工具已经准备好了，目标也已经明确了，那么可以开始拟合了。当然，为了简单起见，这里还是只考虑无权重的情况。我们要做的是找到一组#textOverSet("最优的", "Optimal")#textOverSet("参数值", "Parameter") $hat(k), hat(b)$ 使得均方误差最小，从这一点可以窥见贯穿整个机器学习的核心思想——#textOverSet("最小化", "Minimize")#textOverSet("损失", "Loss")（误差）。形式上，公式会这么写：

$
  hat(k), hat(b) = arg min_(k, b) "MSE" = arg min_(k, b) 1 / n sum^n_(i=1) (y_i - k x_i - b)^2
$

#h2
但是它并没有那么神秘：$arg$ 是 argument 的缩写#footnote[Argument: 自变量，数学优化中函数的输入变量。然而 $arg min$ 中的 $arg$ 仅仅表明在优化算法看来 $k, b$ 是可变的、待优化的自变量。但是从拟合模型外看过去，它们是固定的参变量，通常意义上仍称作 Parameter。这里 Argument 与 Parameter 的区别一定程度上体现了视角的转换。]，$min$ 则是 minimize 的缩写。上面的式子完全可以读作“#textOverSet("找到参数值"+ $hat(k), hat(b)$+"使得均方误差最小", "Find the parameter values "+ $k, b$ + " that minimize the MSE")”。虽然项很多，但这本质上只是一个二次函数，所以无论是配方法、对 $k, b$ 分别求导还是使用矩阵方法，都可以很容易地求解。不过我很喜欢另一个较少被人提及的视角——从线性代数和几何的角度来看待这个问题。我们回头看看残差的表达式：

$
  bold(r) &= [r_1, r_2, dots, r_n] \
&= [y_1-(k x_1+b), y_2-(k x_2+b), dots, y_n-(k x_n+b)] \
&= [y_1, y_2, dots, y_n] - (k[x_1, x_2, dots, x_n] + b[1, 1, dots, 1])
$

我们暂时用一个这样的记号，记拟合所用的函数在这些数据点上的取值

$
  &bold(x)^0 = [1, 1, dots, 1] \
  &bold(x)^1 = [x_1, x_2, dots, x_n] 
$

并记输出 $bold(y) = [y_1, y_2, dots, y_n]$，那么残差就可以写成 $bold(r) = bold(y) - (k bold(x)^1 + b bold(x)^0)$。这样一来，我们的目标是找到 $k, b$ 使得 $bold(r)$ 的模最小。写到这里，从代数上看可能依然不够直观，让我们换个角度看看。


TODO: 图2:从集合的角度看残差

从几何上，$k bold(x)^1 + b bold(x)^0$ 落在 $bold(x)^0$ 与 $bold(x)^1$ 确定的平面上，求 $bold(r) = bold(y) - (k bold(x)^1 + b bold(x)^0)$ 的最小值实际上就是从点向平面做垂线并求垂线长。平面上的点恰好表示了那些可以精准拟合的数据，而偏离平面的部分则暗示了无论怎么用直线拟合都会有误差。不得不说从几何上看确实清晰很多，事实上也有人从几何角度给出了#link("https://www.bilibili.com/video/BV15zPBevERL", "推导")，不过掠过这些细节，仅保留一个直观的印象也无大碍。本节的几篇推荐阅读中都用不同的方法解答了如何最小化误差，有详细的推导，因此这里不再赘述。但是我认为如果读者有一些基础的统计知识而且想记住线性回归推导出的结果，那么结论值得一提，不过跳过也无妨。计算出来的结论是这样的：

首先要计算的是样本中心点，对 $b$ 的导数项为 0 推出最优的直线必然经过样本中心点 $(dash(x), dash(y))$，其中

$ dash(x) = 1/n sum^n_(i=1) x_i, dash(y) = 1/n sum^n_(i=1) y_i $

即#textOverSet("Mean", "均值")。

看斜率之前先看看#textOverSet("方差", "Variance")和#textOverSet("协方差", "Covariance")，方差#footnote("此注释写给学过数理统计的读者：此处并非"+textOverSet("Sample Variance","样本方差")+"，样本方差除以的是"+$n-1$)的表达式是

$
  "Var" = 1/n sum^n_(i=1) (x_i - dash(x))^2
$

是不是感觉很熟悉？这不就是自变量相对均值的 MSE 吗？而协方差的表达式是

$
  "Cov" = 1/n sum^n_(i=1) (x_i - dash(x))(y_i - dash(y))
$

它把方差中的平方项换成了 $x$ 和 $y$ 的#textOverSet("交叉项", "Cross Term")，并由此体现出了#textOverSet("相关关系", "Correlation")。接下来计算的是斜率 $k$，它的表达式是

$
  hat(k) = (sum^n_(i=1) (x_i - dash(x))(y_i - dash(y))) / (sum^n_(i=1) (x_i - dash(x))^2)
$

虽然分子分母都是求和式，看起来有些复杂，但是总结起来其实就是协方差除以自变量的方差，即 $k = "Cov"(bold(x), bold(y)) / "Var"(bold(x))$，如果把协方差看作一种乘法#footnote("此注释写给熟悉线性代数的同学：在"+link("https://en.wikipedia.org/wiki/Inner_product_space\#Random_variables", "向量空间内积") + "的意义上这几乎正确")，那么 $k = (bold(x) dot bold(y)) / (bold(x) dot bold(x))$ 看起来确实挺像那么回事的。




这样一来，通过点-斜率式方程就可以得到最优的直线，那么直线拟合就告一段落了。

#recommend(
"推荐阅读",
[
如果你想了解"回归"与"#textOverSet("最小二乘", "Least Squares")"的含义：\
#h2 `用人话讲明白线性回归Linear Regression - 化简可得的文章 - 知乎`\
#h2 #link("https://zhuanlan.zhihu.com/p/72513104")\
如果你想阅读从求导法到线性代数方法的详尽公式推理：\
#h2 `非常详细的线性回归原理讲解 - 小白Horace的文章 - 知乎`\
#h2 #link("https://zhuanlan.zhihu.com/p/488128941")\
如果你想详细了解了线性回归中的术语、求解过程与几何诠释：\
#h2 `机器学习| 算法笔记-线性回归（Linear Regression） - iamwhatiwant的文章 - 知乎`\
#h2 #link("https://zhuanlan.zhihu.com/p/139445419")\
]
)
