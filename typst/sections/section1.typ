#import "../config.typ": *


= 从函数拟合开始

== 最简单的规律——简单线性回归

#figure(
  image("../../img/linear_regression.png", width: 60%),
  caption: [线性回归示意图 #linebreak() 图源：#link("https://en.wikipedia.org/wiki/Linear_regression", [Wikipedia])]
)

#h2
虽然#textOverSet("线性回归", "Linear Regression")的名字叫做"#textOverSet("回归", "Regression")"，但是事实上我更喜欢叫做#textOverSet("线性拟合", "Linear Fitting")。它的目的是找到一条直线尽可能"贴近"数据点。在这一基础上，我们可以发现数据之间的规律，从而做出一些预测。不过这里有几个问题：

- 为什么要用直线？为什么不用曲线？
- 为什么要用直线拟合数据点？这有什么用？
- "贴近"数据点的标准是什么？为什么要选择这个标准？

#h2

我认为用直线的原因无非两点：一是直线 $y = k x + b$ 简单且意义明确，又能处理不少的问题。几何上直线作为基本对象，尺子就能画出；代数上只需要加减乘除，一次函数我们也很早就学过了。而它的思想一路贯穿到了微积分的导数并延申到了线性代数。二是许多曲线的回归可以转为线性回归（见后文）。例如指数型的 $y = k upright(e)^(alpha x)$ 取对数变为 $z = alpha x + ln k$，又如分式型的 $y = (alpha x + beta)^(-1)$ 取倒数转化为 $z = alpha x + beta$，从而归结为线性拟合。因此带着线性拟合经验再去考虑曲线会更轻松。

至于其意义：一是找到数据的规律，二是做出预测。拟合的系数可以用于测算数据之间的关系，斜率 $k$ 表明输出对输入的敏感程度。一个经典例子是广告投放的#textOverSet("边际效益", "Marginal Benefit")#footnote[边际效益：经济学概念，每增加单位 投入，产出会增加多少 单位]，在一定范围内拟合收益与投入的关系，可以估算当前的边际效益，从而决定是否继续投放。而物理上，比值定义法定义的各种物理量，如电阻、电容等，最常用的测算方式都是线性拟合。例如测量电源输出的若干组 电压和电流数据，并拟合出直线，斜率的绝对值是电源的内阻，同时截距顺带给出了电源的电动势，这样测得的数据就可以用于预测电源的输出情况。对我们所处的世界有定量的认识是科学的基础。可测量的数据和数学模型来描述、解释和预测自然现象是科学的基本方法，也是拟合的根本目的。

既然有了基本思路，那么如何选择“贴近的”标准呢？直接去度量一堆散点和直线的接近程度多少有点霰弹枪#footnote[霰弹枪：一种枪，射出的子弹像雨点一样散开]打移动靶的感觉，但是我们总是可以计算子弹达到了几 环。换言之，两个相差的部分才是关键的，#textOverSet("残差", "Residual")的概念由此产生。取出每个点实际值和拟合值的差，就得到了这样一个列表#footnote[记号说明：对于变量，无论是一维变量还是多维变量，一律采用斜体。对于具体的数据点，视是否为向量决定使用黑体还是斜体。例如$r = y - hat(y)$表示的是方程，所以全部采用斜体。但是具体数据的残差计算，例如$bold(upright(r = y - hat(y)))$，是对数据点的向量运算，所以采用正体。]（其中根据拟合函数$hat(y_i) = k x_i + b$计算出预测值）：

$ bold(upright(r)) = [r_1, r_2, ..., r_n] = [y_1 - hat(y)_1, y_2 - hat(y)_2, ..., y_n - hat(y)_n] $ 

度量数据点与直线间偏差这一问题就转为了度量残差与 0 的偏差。还记得勾股定理吗？直角坐标系内一点到 0 的距离是坐标平方和的平方根，只不过这里残差列表是个$n$维的向量，度量它偏离原点的成都就是向量的#textOverSet("模", "Norm")。这个模越小，说明拟合的效果越好。这样我们就自然地引入了度量拟合效果的量化标准，不过实际应用中出于方便（特别是计算上的方便），通常省去开根号的一步，直接采用残差的平方和，此外还会除以样本点数得到“平均”的残差平方。习惯上称之为#textOverSet("均方误差", "Mean Squared Error")（MSE）：

$ "MSE" = 1 / n |bold(upright(r))|^2 = 1 / n sum^n_(i = 1) r_i^2 $ 

在踏出下一步之前，我想这里有一点点思考的空间。例如：

- 为什么要用平方和而不是直接相加呢？

这是因为直接相加会有正负相互抵消的可能，度量出的偏差为 0  even 为负实在是不合理，因此至少要保证每一项都是正数。但是这又引出下一个问题。

- 为什么不用绝对值呢？绝对值也是正的啊。

从正态分布的角度看，选用平方和自有它的#link("https://www.zhihu.com/question/20447622/answer/25186207", [道理]) 。但是即使读者并不熟悉这些统计的知识背景，也可以从另一个角度理解：平方和的确是一个更好的度量方式，因为它对大的偏差更加敏感。例如一个残差为 2 的点和一个残差为 4 的点，直接绝对值相加的话是 6 ,在这里残差为 4 的点贡献了 $4 \/ 6 approx 66.7 %$ 的偏差。而它们的平方和是 $2 ^ 2 + 4 ^ 2 = 20$ ,残差为 4 的点贡献了$4 ^ 2 \/ 2 0 = 80 %$,更加凸显了 4 的偏差，反映了我们更“关注”这一大偏差的想法，更符合通常对“偏差”的直观认识。

- 为什么要除以样本点数$n$呢？

一方面是为了跨数据集比较。数据集的大小通常有区别，就像买东西的重量不同。这正如不能光看价格不看质量就评价 5 元 2 斤的苹果贵还是 3 元 1 斤的苹果贵，因此需要一个“单位”来衡量。另一方面，看完下一个问题你就会明白其中的精妙之处。

- 这里直接把所有的残差平方加了起来，但如果有的点重要一些怎么办？

先说明一下，这样的需求并非空想。有时测量条件决定了不同点的可靠性并不相同。以一个精度 1 % 的表为例，测量得到 1.00, 2.00, 3.00 时它们本身允许的误差分别是 0.01, 0.02, 0.03，而非相同。也就是说我们会觉得 1.00 的测量值从残差的大小上#footnote[残差的大小：严谨地说称作#textOverSet("绝对误差", "Absolute Error")]更为可靠，这时似乎应该衡量一下点的“重要性”。如果你想说一个点很重要怎么办？直观上来讲你可能会想把它重复几遍。例如，如果你很关心$r_1$, 你可能会想，这还不简单吗？在误差列表中把$r_1$重复 3 遍就好，就像这样：
$
  "Refined" bold(upright(r)) = [r_1, r_1, r_1, r_2, r_3, dots, r_n]
$

这时再计算均方误差呢，变成了$n+2$个点，一种我们设想的“#textOverSet("改善的", "Refined")”均方误差公式就变成了这样：

$ "Refined MSE" = 1 / (n + 2) (2r^2_1 + sum^(n+2)_(i=1) r_i^2) $ 

只不过这样的方式无疑有点“笨重”。再仔细想想呢？如果把$1\/(n+2)$乘到每一项上，就像这样：

$
  "Refined MSE" = 3/(n+2) r_1^2 + 1/(n+2) r_2^2 + ... + 1/(n+2) r_n^2
$

再对照者上面的列表看一看，$3\/(n+2)$不正好表明在大小为$n+2$的列表中$r_1$出现了3次吗？频次就这样和#textOverSet("权重", "Weight")（系数）联系起来了。我们也没必要守着重复 3 次或者 5 次这种固定的规则——至少自然可没有限制重要性之间的比例正好是整数。这样一来只需要一个权重列表就可以了。权重乘在残差平方前，这就引出了#textOverSet("加权误差", "Weighted Error")，大权重表示更重要。略微改写一下公式得到：

$
  "Weighted MSE" = sum^n_(i=1) w_i r_i^2
$

#recommend(
"推荐阅读",
[
如果你想了解"回归"与"#textOverSet("最小二乘", "Least Squares")"的含义：\
#h2 `用人话讲明白线性回归Linear Regression - 化简可得的文章 - 知乎`\
#h2 #link("https://zhuanlan.zhihu.com/p/72513104")\
如果你想阅读从求导法到线性代数方法的详尽公式推理：\
#h2 `非常详细的线性回归原理讲解 - 小白Horace的文章 - 知乎`\
#h2 #link("https://zhuanlan.zhihu.com/p/488128941")\
如果你想详细了解了线性回归中的术语、求解过程与几何诠释：\
#h2 `机器学习| 算法笔记-线性回归（Linear Regression） - iamwhatiwant的文章 - 知乎`\
#h2 #link("https://zhuanlan.zhihu.com/p/139445419")\
]
)

#pagebreak()

== 多项式拟合

#figure(
  image("../../img/polynomial_fitting.png", width: 60%),
  caption: [多项式拟合示意图（图为 3 次拟合） #linebreak() 图源：#link("https://www.geeksforgeeks.org/numpys-polyfit-function-a-comprehensive-guide/", [GeeksforGeeks])]
)

#h2 线性拟合虽然很好，但是如果拿到了明显不线性的一堆数据，那么线性拟合就显得有些力不从心了。不过既然都是拟合，能做一次的那按理来讲也能做多次。#textOverSet("多项式拟合", "Polynomial Fitting")就是这样一种思路，只是预测 $hat(y)$ 从 $k x + b$ 变成了 $a_0 + a_1 x + ... + a_m x^m$#footnote[记号说明：虽然习惯上幂次从大到小排列，但是为了下标和幂次的统一性，所以这里选择从常数项到最高次项排列]，其中 $m$ 是多项式的次数。而均方误差的表达式甚至几乎不用变，仍然是
$ "MSE" = 1/n sum_(i=1)^n (y_i - hat(y))^2 $

只不过展开后是一系列的多项式项，待拟合的参数从两个变成了 $m+1$ 个。但是如果观察一下，这个式子仍然是一个（多变量的）二次函数，所以最小化的方法也是一样的。多项式自有多项式的好，能加的项多了，拟合的灵活性也就大了，误差显然会更小。然而与线性拟合相比，它虽然有#textOverSet("解析解", "Analytical Solution")，但不再像线性拟合一样可以逐项明确说出意义，而是只剩下一堆矩阵运算把这些参数算出来。因此相比于记下公式，形成一个整体上的印象显得尤为重要。

上一小节中，我们从图像看到了这种拟合的几何解释，而多项式拟合也是相似的，还是从 $bold(r)$ 的表达式入手
$ bold(r) = bold(y) - (a_0 bold(x)^0 + a_1 bold(x)^1 + ... + a_m bold(x)^m) $

对比之前的表达式，当 $a_0, a_1, ..., a_m$ 变化时，预测得到的结果 $hat(bold(y)) = a_0 bold(x)^0 + a_1 bold(x)^1 + ... + a_m bold(x)^m$ 也会在一个 $m + 1$ 维的空间中变化，正如之前的平面，这个空间也是一个 $m + 1$ 维的子空间。求最小模的 $bold(r)$ 又回到了从点到子空间的垂线问题。虽然不得不承认：想象从一个高维的 $n$ 维空间中向 $m+1$ 维的子空间做垂线确实有些困难，但是这多少离我们的几何直觉更近了一些。

系数的意义不那么明确了，但是误差下来了，这是好事吗？也不一定，灵活性的另一面是潜在的#textOverSet("过拟合", "Overfitting")。前文中做线性拟合的时候有一个重要的假设是测量得到数据带有一定的误差。拟合的直线滤去了大部分的误差，留下了重要的趋势。但是如果灵活性太高，拟合的多项式会过于贴合数据，甚至把误差也拟合进去了。即使在给定的数据上做到了很小的误差，预测新数据的能力却可能会大打折扣。

拿做题打个比方：使用直线拟合明显不线性的数据是方法错了，只能说是没完全学会。但是用接近数据量的参数来拟合数据，留给它的空间都够把结果"背下来"了，捕捉到了数据的细节，却忽略了数据背后的规律，化成了一种只知道背答案的自我感动。在几道例题上能做到滴水不漏，但是一遇到新题就束手无策。

举个例子，在下面这个数据集上试图拟合，我们在二次函数 $y = 0.25 x^2 - x + 1$ 上添加了标准正态分布的噪声，即实际上 $y = 0.25 x^2 - x + 1 + cal(N)(0, 1)$ #footnote[$cal(N)(0, 1)$：表示一个服从#link("https://baike.baidu.com/item/%E6%A0%87%E5%87%86%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83", [标准正态分布])的变量，均值为 0，方差为 1]。

#figure(
  image("../../img/polynomial_fitting.png", width: 60%),
  caption: [多项式拟合示意图（图为 3 次拟合） #linebreak() 图源：#link("https://www.geeksforgeeks.org/numpys-polyfit-function-a-comprehensive-guide/", [GeeksforGeeks])]
)

#h2 那么现在我们来试试用不同次数的多项式拟合这个数据集。不难看出线性拟合的线与数据点还是相差不少，因为它没能提供可以制造数据"弯曲"形状的项，它没能捕捉到数据更加复杂的趋势，这种现象称为#textOverSet("欠拟合", "Underfitting")。2 次曲线的效果几乎和真实曲线一样，即使提升到 3 次也没有太明显的改变，它们拟合的效果都还算好。